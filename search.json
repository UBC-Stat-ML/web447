[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Caution\n\n\n\nPage under construction: information on this page may change.\n\n\n\n\n\n\n\n\n\n\n\nDate\nTopics\nReadings\n\n\n\n\nTue, Jan 9\nOverview. Probabilistic inference on discrete spaces.\nBDA 1.1-1.10\n\n\nTh, Jan 11\n\n\n\n\nTue, Jan 16\nA tour of Bayesian inference (on discrete spaces).\nBDA 2.1-2.5\n\n\nTh, Jan 18\n\n\n\n\nTue, Jan 23\nUniversal probabilistic inference via importance sampling.\n\n\n\nTh, Jan 25\n\n\n\n\nTue, Jan 30\nDecision theory.\nBDA 9.1-9.5\n\n\nTh, Feb 1\n\n\n\n\nTue, Feb 6\nBayesian regression, GLMs, and beyond.\n\n\n\nTh, Feb 8\n\n\n\n\nTue, Feb 13\nHierarchical models.\nBDA 5.1-5.7\n\n\nTh, Feb 15\n\n\n\n\nTue, Feb 20\nReading week.\n\n\n\nTh, Feb 22\nReading week.\n\n\n\nTue, Feb 27\nQuiz 1.\n\n\n\nTh, Feb 29\nTBA\n\n\n\nTue, Mar 5\nMCMC user guide (via Stan).\n\n\n\nTh, Mar 7\n\n\n\n\nFri, Mar 8\nProject proposal due.\n\n\n\nTue, Mar 12\nBayesian workflow.\nBDA 6.1-6.5\n\n\nTh, Mar 14\n\n\n\n\nTue, Mar 19\nModelling techniques (selected from prior design, mixtures, imputation, complex data collection).\n\n\n\nTh, Mar 21\n\n\n\n\nTue, March 26\nQuiz 2.\n\n\n\nTh, Mar 28\nTBA\n\n\n\nTue, Apr 2\nMCMC developer guide.\n\n\n\nTh, Apr 4\n\n\n\n\nTue, Apr 9\nVariational inference.\n\n\n\nTh, Apr 11\nLast lecture.\n\n\n\nFri, Apr 19\nFinal project due."
  },
  {
    "objectID": "w00_intro/topic02_what.html",
    "href": "w00_intro/topic02_what.html",
    "title": "What?",
    "section": "",
    "text": "MAP estimators (maximum a posteriori)\nposterior means\nBayes rule\nmodels where some unknown quantities are treated as random\nnone of the above\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nAll these popular answers are misleading and/or very incomplete:\n\nMAP estimators (maximum a posteriori)\n\nMAP is seldom used by expert Bayesians (mode is misleading in high dimensions)\n\nposterior means\n\nthe posterior mean is often undefined (e.g. Bayesian analysis over combinatorial objects such as graphs)\n\nBayes rule\n\nBayes rule is intractable in most practical situations (we use MCMC/variational methods)\n\nmodels where some unknown quantities are treated as random\n\ntrue for Bayesian models, but also for many non-Bayesian models, e.g., random effect models\n\n\nSo… what is Bayesian Analysis?\n\n\nBayesian Analysis: statistical discipline centered around the use of Bayes estimators\nBayes estimators: for data \\(Y\\), unobserved \\(X\\), loss \\(L\\), and possible actions \\(A\\), the Bayes estimator is defined as:\n\\[\\operatorname{arg\\,min}\\{ \\mathbb{E}[L(a, X) | Y] : a \\in A \\}\\]\nNote: you are not expected to understand this equation at this point!\n\n\n\nThe primary objective of this course is to understand Bayes estimators:\n\nWhy they are so powerful.\nTheir limitations (model misspecification, computational challenges).\nImportant special cases (posterior means, credible intervals, MAP).\nHow to do it in practice\n\nhow to build models\nhow to approximate conditional expectations."
  },
  {
    "objectID": "w00_intro/topic02_what.html#poll-what-characterizes-bayesian-analysis",
    "href": "w00_intro/topic02_what.html#poll-what-characterizes-bayesian-analysis",
    "title": "What?",
    "section": "",
    "text": "MAP estimators (maximum a posteriori)\nposterior means\nBayes rule\nmodels where some unknown quantities are treated as random\nnone of the above\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nAll these popular answers are misleading and/or very incomplete:\n\nMAP estimators (maximum a posteriori)\n\nMAP is seldom used by expert Bayesians (mode is misleading in high dimensions)\n\nposterior means\n\nthe posterior mean is often undefined (e.g. Bayesian analysis over combinatorial objects such as graphs)\n\nBayes rule\n\nBayes rule is intractable in most practical situations (we use MCMC/variational methods)\n\nmodels where some unknown quantities are treated as random\n\ntrue for Bayesian models, but also for many non-Bayesian models, e.g., random effect models\n\n\nSo… what is Bayesian Analysis?\n\n\nBayesian Analysis: statistical discipline centered around the use of Bayes estimators\nBayes estimators: for data \\(Y\\), unobserved \\(X\\), loss \\(L\\), and possible actions \\(A\\), the Bayes estimator is defined as:\n\\[\\operatorname{arg\\,min}\\{ \\mathbb{E}[L(a, X) | Y] : a \\in A \\}\\]\nNote: you are not expected to understand this equation at this point!\n\n\n\nThe primary objective of this course is to understand Bayes estimators:\n\nWhy they are so powerful.\nTheir limitations (model misspecification, computational challenges).\nImportant special cases (posterior means, credible intervals, MAP).\nHow to do it in practice\n\nhow to build models\nhow to approximate conditional expectations."
  },
  {
    "objectID": "w00_intro/topic03_high_level.html",
    "href": "w00_intro/topic03_high_level.html",
    "title": "High-level picture",
    "section": "",
    "text": "Construct a probability model including\n\nrandom variables for what we will measure/observe\nrandom variables for the unknown quantities\n\nthose we are interested in (“parameters”, “predictions”)\nothers that just help us formulate the problem (“nuisance”, “random effects”).\n\n\nCompute the posterior distribution conditionally on the actual data at hand\nUse the posterior distribution to:\n\nmake prediction (point estimate)\nestimate uncertainty (credible intervals)\nmake a decision (more on this later)"
  },
  {
    "objectID": "w00_intro/topic03_high_level.html#bayesian-recipe-high-level-picture",
    "href": "w00_intro/topic03_high_level.html#bayesian-recipe-high-level-picture",
    "title": "High-level picture",
    "section": "",
    "text": "Construct a probability model including\n\nrandom variables for what we will measure/observe\nrandom variables for the unknown quantities\n\nthose we are interested in (“parameters”, “predictions”)\nothers that just help us formulate the problem (“nuisance”, “random effects”).\n\n\nCompute the posterior distribution conditionally on the actual data at hand\nUse the posterior distribution to:\n\nmake prediction (point estimate)\nestimate uncertainty (credible intervals)\nmake a decision (more on this later)"
  },
  {
    "objectID": "w00_intro/topic03_high_level.html#plan",
    "href": "w00_intro/topic03_high_level.html#plan",
    "title": "High-level picture",
    "section": "Plan",
    "text": "Plan\n\nFirst week: probability essentials (foundations for steps 1 and 2 of the Bayesian Recipe)\nSecond week: steps 1, 2, 3 for one specific discrete probability models\nThird week: step 1, 2, 3 for arbitrary models"
  },
  {
    "objectID": "w00_intro/topic03_high_level.html#first-step-of-the-recipe-constructing-a-probability-model",
    "href": "w00_intro/topic03_high_level.html#first-step-of-the-recipe-constructing-a-probability-model",
    "title": "High-level picture",
    "section": "First step of the Recipe: “constructing a probability model”",
    "text": "First step of the Recipe: “constructing a probability model”\n\nWhat is a model?\nWhat is a probability model?\nExample (week 2): building a probability model for the rocket launch problem."
  },
  {
    "objectID": "w00_intro/topic03_high_level.html#what-is-a-model",
    "href": "w00_intro/topic03_high_level.html#what-is-a-model",
    "title": "High-level picture",
    "section": "What is a model?",
    "text": "What is a model?\n(Scientific) model: A simplification of reality amenable to mathematical investigation.\n\\[\\text{Reality} \\xrightarrow{\\text{Art + Scientific method}} \\text{Model} \\xrightarrow{\\text{Mathematics}} \\text{Prediction}\\]\n\nIn this course “mathematics” will be Bayesian analysis/probability theory.\nBayesian analysis/probability theory assume a model as starting point.\n\nTo create a first model is a bit of an art. It comes with data analysis experience.\nThen after we start with an initial model we can improve it by checking predictions against reality."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus: STAT447C Bayesian Statistics",
    "section": "",
    "text": "Bayesian inference is a flexible and powerful approach to modeling reality, making optimal predictions from data, and quantifying uncertainty in a coherent manner. Thanks to their versatility, Bayesian methods are now widely used in virtually all fields of science, engineering, and beyond.\nIn STAT 447C, you will:\n\ndesign probabilistic models to approach real-world inferential problems;\nperform inference using Bayesian modelling languages;\ncritically assess, debug, and iteratively improve Bayesian workflows;\ndevelop and analyze custom posterior approximation machinery."
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Syllabus: STAT447C Bayesian Statistics",
    "section": "",
    "text": "Bayesian inference is a flexible and powerful approach to modeling reality, making optimal predictions from data, and quantifying uncertainty in a coherent manner. Thanks to their versatility, Bayesian methods are now widely used in virtually all fields of science, engineering, and beyond.\nIn STAT 447C, you will:\n\ndesign probabilistic models to approach real-world inferential problems;\nperform inference using Bayesian modelling languages;\ncritically assess, debug, and iteratively improve Bayesian workflows;\ndevelop and analyze custom posterior approximation machinery."
  },
  {
    "objectID": "syllabus.html#lecture-time-and-place",
    "href": "syllabus.html#lecture-time-and-place",
    "title": "Syllabus: STAT447C Bayesian Statistics",
    "section": "Lecture time and place",
    "text": "Lecture time and place\nLecture dates: January 9, 2024 to April 11, 2024. Detailed schedule\nTuesday and Thursday, 9:30-11:00. FNH Building, Room 40."
  },
  {
    "objectID": "syllabus.html#teaching-team",
    "href": "syllabus.html#teaching-team",
    "title": "Syllabus: STAT447C Bayesian Statistics",
    "section": "Teaching team",
    "text": "Teaching team\n\nAlexandre Bouchard-Côté (Instructor)\nMiguel Biron-Lattes (TA)\nAli Mehrabian (TA)"
  },
  {
    "objectID": "syllabus.html#prerequisite",
    "href": "syllabus.html#prerequisite",
    "title": "Syllabus: STAT447C Bayesian Statistics",
    "section": "Prerequisite",
    "text": "Prerequisite\n\nProbability: STAT 302, MATH 302 or equivalent. I will do a review of the relevant concepts, but Bayesian statistics is entirely built on top of probability theory so prior exposure to probability is the key prerequisite for this course.\nBasic background in linear algebra (e.g. matrix multiplication, eigenvectors) and calculus (see STAT 302’s prerequisites for example)\nComputing: we will use R in the homework and during lectures. If you know another programming language but not R, you can still take this course but be prepared to spend a bit of extra time to get familiar with the R syntax. We will have special office hours sessions at the beginning of the term to help you doing that.\n\nCome talk to me at the end of the first lecture if you are unsure about your preparation for this course."
  },
  {
    "objectID": "syllabus.html#software",
    "href": "syllabus.html#software",
    "title": "Syllabus: STAT447C Bayesian Statistics",
    "section": "Software",
    "text": "Software\nAll software used is free and open source. Some key tools we will use:\n\nR\nRStudio\nRStan\n\nWe assume you have a laptop on which you can install these tools, if not, you may be able to borrow one from UBC library."
  },
  {
    "objectID": "syllabus.html#textbook",
    "href": "syllabus.html#textbook",
    "title": "Syllabus: STAT447C Bayesian Statistics",
    "section": "Textbook",
    "text": "Textbook\nNotes will be provided and complemented with readings from the following freely available textbook:\n\nBayesian Data Analysis, Third Rdition. Andrew Gelman, John Carlin, Hal Stern, David Dunson, Aki Vehtari, and Donald Rubin. PDF freely available.\n\nAdditional readings and case studies will be drawn from other textbooks that are either freely available or available within UBC VPN:\n\nBayesian essentials with R, Second Edition. Jean-Michel Marin and Christian Robert. PDF available via UBC library. Solution to exercises.\nBayes Rules! Alicia A. Johnson, Miles Q. Ott, Mine Dogucu. HTML freely available\nDoing Bayesian data analysis: a tutorial with R, JAGS, and Stan, Second Edition. John K. Kruschke. PDF freely available.\nProbability and Bayesian modeling. Jim Albert and Jingchen Hu. PDF/HTML/EPUB freely available.\nStatistical Rethinking, Second Edition. Richard McElreath. HTML available via UBC library."
  },
  {
    "objectID": "syllabus.html#assessments",
    "href": "syllabus.html#assessments",
    "title": "Syllabus: STAT447C Bayesian Statistics",
    "section": "Assessments",
    "text": "Assessments\nClick on each item for details.\n\nParticipation: 15%\n\nWeekly reading assignment: each week ask and answer one question about the readings or lectures on Piazza.\nIn-class iClicker questions: only participations points (unless your score is indistinguishable from random). Setup iClicker Cloud on Canvas.\n\nHomework: 15%\n\nWeekly.\nReleased and submitted on Canvas.\n\nQuizzes (2 x 20%): 40%\n\nIn-class.\nDates: Tuesday February 27, Tuesday March 26.\n\nFinal project: 30%\n\nFor the reading assignment and homework, we will drop the lowest week. For the iClicker, we will automatically skip up to two missed lectures. Keep these for sick days/unforeseen circumstances. No need to ask for permission/provide doctor’s note, this will be done automatically for everyone."
  },
  {
    "objectID": "syllabus.html#office-hours",
    "href": "syllabus.html#office-hours",
    "title": "Syllabus: STAT447C Bayesian Statistics",
    "section": "Office hours",
    "text": "Office hours\n\nInstructor office hour: Thursdays, 3:30-4:30, ESB 3125\nTA office hour: Fridays, 11:00-12:00, ESB 3125\n\nAvailable by appointment if you are unable to attend drop-in hours."
  },
  {
    "objectID": "syllabus.html#course-communication",
    "href": "syllabus.html#course-communication",
    "title": "Syllabus: STAT447C Bayesian Statistics",
    "section": "Course communication",
    "text": "Course communication\n\nAnnouncements\nCourse announcements will be posted on Canvas.\n\n\nQuestions\nUse Piazza for questions about the material, logistics, etc. Use public posts as much as possible so that other students can learn from the discussion.\nUse private piazza questions if, and only if the question is about a personal matter."
  },
  {
    "objectID": "exercises/ex02.html",
    "href": "exercises/ex02.html",
    "title": "Exercise 2: Bayes estimator",
    "section": "",
    "text": "Build a probability model for a concrete example.\nIntroduce the concept of Bayes estimators.\nReview sequential Bayes update."
  },
  {
    "objectID": "exercises/ex02.html#goals",
    "href": "exercises/ex02.html#goals",
    "title": "Exercise 2: Bayes estimator",
    "section": "",
    "text": "Build a probability model for a concrete example.\nIntroduce the concept of Bayes estimators.\nReview sequential Bayes update."
  },
  {
    "objectID": "exercises/ex02.html#setup",
    "href": "exercises/ex02.html#setup",
    "title": "Exercise 2: Bayes estimator",
    "section": "Setup",
    "text": "Setup\nThis exercise is centered around the following scenario:\n\nYou are consulting for a satellite operator\nThey are about to send a $100M satellite on a Delta 7925H rocket\nData: as of Jan 2024, Delta 7925H rockets have been launched 3 times, with 0 failed launches \n\nNote: Delta 7925H is not reusable, so each rocket is “copy- built” from the same blueprint\n\nShould you recommend buying a $2M insurance policy?"
  },
  {
    "objectID": "exercises/ex02.html#q.1-define-a-probabilistic-model",
    "href": "exercises/ex02.html#q.1-define-a-probabilistic-model",
    "title": "Exercise 2: Bayes estimator",
    "section": "Q.1: define a probabilistic model",
    "text": "Q.1: define a probabilistic model\n\nWhat are the unknown quantities in this scenario? And what is the data?\n\nIn order to perform inference on the unknown quantities, we must specify how they relate to the data; i.e., we need a probabilistic model. Assume that every Delta 7925H rocket has the same probability \\(p\\) of failing. For simplicity, let us assume that \\(p\\) is allowed to take values on an evenly space grid \\[\np \\in \\left\\{\\frac{k}{K}: k\\in \\{0,\\dots,K\\}\\right\\}\n\\] for some fixed \\(K\\in\\mathbb{N}\\). Furthermore, we have access to a collection of numbers \\(\\rho_k\\in[0,1]\\) such that \\[\n\\forall k\\in\\{0,\\dots,K\\}:\\ \\mathbb{P}\\left(p=\\frac{k}{K}\\right) = \\rho_k.\n\\]\nLet \\(Z_i\\) denote a binary variable indicating a failed launch. We assume that, conditionally on \\(p\\), these binary variables are independent of each other.\n\nWrite the joint distribution of this model (use the \\(\\sim\\) notation)."
  },
  {
    "objectID": "exercises/ex02.html#q.2-probability-calculations",
    "href": "exercises/ex02.html#q.2-probability-calculations",
    "title": "Exercise 2: Bayes estimator",
    "section": "Q.2: probability calculations",
    "text": "Q.2: probability calculations\n\nGive an expression for the prior predictive probability of failure for the next launch \\[\n\\mathbb{P}(Z_4=1)\n\\]\nGive an expression for the posterior probability \\[\n\\mathbb{P}\\left(p=\\frac{k}{K}\\middle| Z_1=Z_2=Z_3=0 \\right)\n\\]\nWrite a function called posterior_distribution that takes a vector \\(\\rho\\in\\mathbb{R}^{K+1}\\) and returns the posterior distribution derived in the previous step.\nPlot the result of applying posterior_distribution to the prior \\[\n\\rho_k \\propto \\frac{k}{K}\\left(1-\\frac{k}{K}\\right).\n\\tag{1}\\] Add \\(\\rho\\) to the plot and compare the two distributions.\nReport the mean, median, and mode of \\(p\\) under the posterior distribution of the previous step."
  },
  {
    "objectID": "exercises/ex02.html#q.3-bayes-action",
    "href": "exercises/ex02.html#q.3-bayes-action",
    "title": "Exercise 2: Bayes estimator",
    "section": "Q.3: Bayes action",
    "text": "Q.3: Bayes action\nLet \\(a\\in\\{0,1\\}\\) be a binary variable denoting the decision of buying the insurance (\\(a=1\\)) or not (\\(a=0\\)).\n\nDefine a loss function \\((a,Z_4)\\mapsto L(a,Z_4)\\) that summarizes the cost of having taken decision \\(a\\in\\{0,1\\}\\) when \\(Z_4\\in\\{0,1\\}\\) occurs.\nDerive an expression for the expected loss under the posterior predictive distribution \\[\n\\mathcal{L}(a) := \\mathbb{E}[L(a,Z_4)|Z_{1:3}=0].\n\\]\nGive an expression for the posterior predictive probability of failure \\[\n\\mathbb{P}\\left(Z_4=1 \\middle| Z_{1:3}=0 \\right)\n\\]\nWrite a function posterior_failure_prob that takes a vector \\(\\rho\\in\\mathbb{R}^{K+1}\\) and returns the posterior predictive probability of failure. Your function should call posterior_distribution.\nWrite a function expected_loss that computes \\(\\mathcal{L}(a)\\) for any value of \\(a\\). Based on this, formulate a recommendation to the owner of the satellite."
  },
  {
    "objectID": "exercises/ex02.html#q.4-sequential-updating",
    "href": "exercises/ex02.html#q.4-sequential-updating",
    "title": "Exercise 2: Bayes estimator",
    "section": "Q.4: sequential updating",
    "text": "Q.4: sequential updating\nLet us suppose that we go back in time and are able to recreate the inferences that a statistician would have made as the successive launches happened.\n\nDerive an expression for \\(\\mathbb{P}(p|Z_1=0)\\).\nUse the previous expression as a prior distribution TODO"
  },
  {
    "objectID": "drafts/ex03.html",
    "href": "drafts/ex03.html",
    "title": "Exercise 3: Universal probabilistic inference via importance sampling",
    "section": "",
    "text": "Caution\n\n\n\nPage under construction: information on this page may change."
  },
  {
    "objectID": "drafts/ex03.html#goals",
    "href": "drafts/ex03.html#goals",
    "title": "Exercise 3: Universal probabilistic inference via importance sampling",
    "section": "Goals",
    "text": "Goals\n\nUnderstanding and implementing importance sampling\nIntroduction to probabilistic programming"
  },
  {
    "objectID": "w01_discrete_inference/topic11_monte_carlo.html",
    "href": "w01_discrete_inference/topic11_monte_carlo.html",
    "title": "Monte Carlo",
    "section": "",
    "text": "Simple Monte Carlo method\nTheoretical guarantee from the Law of Large Numbers\n\n\n\n\nSimple Monte Carlo is the foundation for more complex Monte Carlo methods used by Bayesian practioners (e.g. Importance Sampling and Markov chain Monte Carlo (MCMC))"
  },
  {
    "objectID": "w01_discrete_inference/topic11_monte_carlo.html#outline",
    "href": "w01_discrete_inference/topic11_monte_carlo.html#outline",
    "title": "Monte Carlo",
    "section": "",
    "text": "Simple Monte Carlo method\nTheoretical guarantee from the Law of Large Numbers\n\n\n\n\nSimple Monte Carlo is the foundation for more complex Monte Carlo methods used by Bayesian practioners (e.g. Importance Sampling and Markov chain Monte Carlo (MCMC))"
  },
  {
    "objectID": "w01_discrete_inference/topic11_monte_carlo.html#approximation-of-expectations-using-forward-simulation",
    "href": "w01_discrete_inference/topic11_monte_carlo.html#approximation-of-expectations-using-forward-simulation",
    "title": "Monte Carlo",
    "section": "Approximation of expectations using forward simulation",
    "text": "Approximation of expectations using forward simulation\n\nAs before, we want to compute an expectation \\(\\mathbb{E}[g(X, Y_1, \\dots, Y_4)]\\)\nBut imagine a very large tree, but where most branches have very low probability and only few have large probability\nIn this case, instead of computing the exact expectation by iterating over each of the leaves as before, we will approximate expectations using forward simulation (a method know as simple Monte Carlo)\nThis is done as follows:\n\nCall your forward simulator \\(M\\) times\nDenote the output at iteration \\(m \\in \\{1, 2, \\dots M\\}\\) by: \\[(X^{(m)}, Y_1^{(m)}, \\dots, Y_4^{(m)}) \\sim p_{X, Y_{1:4}}(\\cdot)\\]\nCompute \\(g\\) on each, call each of the \\(M\\) outputs \\(g^{(m)}\\) \\[g^{(m)}= g(X^{(m)}, Y_1^{(m)}, \\dots, Y_4^{(m)}).\\]\nReturn the average \\[\\hat G_M = \\frac{1}{M} \\sum_{m=1}^M g^{(m)}.\\]\n\n\nIntuitively, the output \\(\\hat G_M\\) has the nice property: \\[\\hat G_M \\approx \\mathbb{E}[g(X, Y_1, \\dots, Y_4)]. \\tag{1}\\]"
  },
  {
    "objectID": "w01_discrete_inference/topic11_monte_carlo.html#motivation-from-the-law-of-large-numbers",
    "href": "w01_discrete_inference/topic11_monte_carlo.html#motivation-from-the-law-of-large-numbers",
    "title": "Monte Carlo",
    "section": "Motivation from the Law of Large Numbers",
    "text": "Motivation from the Law of Large Numbers\nQuestion: How can we make \\(\\approx\\) more formal in Equation 1?\nProposition (Law of Large Numbers, LLN): if \\(Z_1, Z_2, \\dots\\) are i.i.d. random variables with \\(\\mathbb{E}|Z_i| &lt; \\infty\\), then1 \\[ \\frac{1}{M} \\sum_{m=1}^M Z_m \\to \\mathbb{E}[Z_i].\\]\nPicking \\(Z_m = g^{(m)}\\) we arrive to the following formalization of \\(\\approx\\): for any approximation error tolerence, we can find a number of iterations \\(M\\) large enough such that we will be within that error tolerence with high probability."
  },
  {
    "objectID": "w01_discrete_inference/topic11_monte_carlo.html#footnotes",
    "href": "w01_discrete_inference/topic11_monte_carlo.html#footnotes",
    "title": "Monte Carlo",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHere”\\(\\to\\)” denotes a suitable notion of convergence of random variables In STAT 302 you may have seen LLN with “convergence in probability”, but this can be strengthen to “convergence almost sure.” The difference between these notions of convergence will not matter in this course.↩︎"
  },
  {
    "objectID": "w01_discrete_inference/topic10_expectations.html",
    "href": "w01_discrete_inference/topic10_expectations.html",
    "title": "Expectations",
    "section": "",
    "text": "Expectation for discrete random models\nLaw of the Unconscious Statistician\n\n\n\n\nExpectation si the main tool to translate a posterior distribution into the various outputs of Bayesian inference (point estimate, credible intervals, prediction, action)."
  },
  {
    "objectID": "w01_discrete_inference/topic10_expectations.html#outline",
    "href": "w01_discrete_inference/topic10_expectations.html#outline",
    "title": "Expectations",
    "section": "",
    "text": "Expectation for discrete random models\nLaw of the Unconscious Statistician\n\n\n\n\nExpectation si the main tool to translate a posterior distribution into the various outputs of Bayesian inference (point estimate, credible intervals, prediction, action)."
  },
  {
    "objectID": "w01_discrete_inference/topic10_expectations.html#expectation-of-a-single-random-variable",
    "href": "w01_discrete_inference/topic10_expectations.html#expectation-of-a-single-random-variable",
    "title": "Expectations",
    "section": "Expectation of a single random variable",
    "text": "Expectation of a single random variable\nRecall: \\[\\mathbb{E}[X] = \\sum_x x p_X(x),\\] where the sum is over the point masses of \\(X\\), i.e. \\(\\{x : p_X(x) &gt; 0\\}\\).\nTest yourself: compute \\(\\mathbb{E}[X]\\) if \\(X \\sim {\\mathrm{Bern}}(p)\\)."
  },
  {
    "objectID": "w01_discrete_inference/topic10_expectations.html#law-of-the-unconscious-statistician",
    "href": "w01_discrete_inference/topic10_expectations.html#law-of-the-unconscious-statistician",
    "title": "Expectations",
    "section": "Law of the Unconscious Statistician",
    "text": "Law of the Unconscious Statistician\nProposition: if \\(g\\) is some function, \\[\\mathbb{E}[g(X)] = \\sum_x g(x) p_X(x).\\]\nTest yourself: compute \\(\\mathbb{E}[X^2]\\) if \\(X \\sim {\\mathrm{Bern}}(p)\\), and hence \\(\\operatorname{Var}[X] = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2\\)."
  },
  {
    "objectID": "w01_discrete_inference/topic10_expectations.html#expectation-of-a-function-of-several-random-variables",
    "href": "w01_discrete_inference/topic10_expectations.html#expectation-of-a-function-of-several-random-variables",
    "title": "Expectations",
    "section": "Expectation of a function of several random variables",
    "text": "Expectation of a function of several random variables\nLet us go back to our running example:\n\nImagine a bag with 3 coins each with a different probability parameter \\(p\\)\nCoin \\(i\\in \\{0, 1, 2\\}\\) has bias \\(i/2\\)—in other words:\n\nFirst coin: bias is \\(0/2 = 0\\) (i.e. both sides are “tails”, \\(p = 0\\))\nSecond coin: bias is \\(1/2 = 0.5\\) (i.e. standard coin, \\(p = 1/2\\))\nThird coin: bias is \\(2/2 = 1\\) (i.e. both sides are “heads”, \\(p = 1\\))\n\n\n\n\n\n\nConsider the following two steps sampling process\n\nStep 1: pick one of the three coins, but do not look at it!\nStep 2: flip the coin 4 times\n\nMathematically, this probability model can be written as follows: \\[\n\\begin{align*}\nX &\\sim {\\mathrm{Unif}}\\{0, 1, 2\\} \\\\\nY_i | X &\\sim {\\mathrm{Bern}}(X/2)\n\\end{align*}\n\\tag{1}\\]\n\nExercise: computing \\(\\mathbb{E}[X (Y_1+1)]\\)\nForm: \\(\\mathbb{E}[g(X, Y_1, \\dots, Y_4)]\\) for \\(g(x, y_1, \\dots, y_4) = x(y_1+1)\\).\nGeneric strategy:\n\nstart the joint PMF of all the random variables in the model (even if they do not occur in \\(g\\))\ncompute the expectation using \\[\\mathbb{E}[g(X, Y_1, \\dots, Y_4)] = \\sum_x \\sum_{y_1} \\sum_{y_2} \\dots \\sum_{y_4} g(x, y_1, \\dots, y_4)  p(x, y_1, y_2, y_3, y_4).\\]\nEach sum runs over the point mass of its PMF as before, e.g. \\(x \\in \\{0, 1, 2\\}\\).\nRecall: \\(p(x, y_1, y_2, y_3, y_4)\\) can be computed using the chain rule.\n\nRecall the decision tree, how to visualize the above equation?\n\n\n\n\n\nflowchart TD\nS__and__X_0 -- 1.0 --&gt; S__and__X_0__and__Y1_false[\"Y1=false\"]\nS__and__X_2__and__Y1_true -- 1.0 --&gt; S__and__X_2__and__Y1_true__and__Y2_true[\"Y2=true\"]\nS -- 0.33 --&gt; S__and__X_0[\"X=0\"]\nS__and__X_1__and__Y1_true__and__Y2_true__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_true__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_true__and__Y2_false__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_true__and__Y4_false[\"Y4=false\"]\nS__and__X_0__and__Y1_false__and__Y2_false__and__Y3_false -- 1.0 --&gt; S__and__X_0__and__Y1_false__and__Y2_false__and__Y3_false__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_false__and__Y2_false__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_false__and__Y4_true[\"Y4=true\"]\nS -- 0.33 --&gt; S__and__X_1[\"X=1\"]\nS__and__X_2__and__Y1_true__and__Y2_true__and__Y3_true -- 1.0 --&gt; S__and__X_2__and__Y1_true__and__Y2_true__and__Y3_true__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_true__and__Y2_false__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_false__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_false__and__Y2_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_false[\"Y3=false\"]\nS__and__X_1__and__Y1_true__and__Y2_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_false[\"Y3=false\"]\nS__and__X_1__and__Y1_false__and__Y2_false__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_false__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_true__and__Y2_true__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_false__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_true__and__Y2_false__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_true__and__Y4_true[\"Y4=true\"]\nS__and__X_1 -- 0.5 --&gt; S__and__X_1__and__Y1_false[\"Y1=false\"]\nS__and__X_1__and__Y1_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false[\"Y2=false\"]\nS__and__X_1__and__Y1_false__and__Y2_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_true[\"Y3=true\"]\nS__and__X_1__and__Y1_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true[\"Y2=true\"]\nS__and__X_0__and__Y1_false -- 1.0 --&gt; S__and__X_0__and__Y1_false__and__Y2_false[\"Y2=false\"]\nS__and__X_1__and__Y1_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false[\"Y2=false\"]\nS__and__X_1__and__Y1_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true[\"Y2=true\"]\nS__and__X_2 -- 1.0 --&gt; S__and__X_2__and__Y1_true[\"Y1=true\"]\nS__and__X_1__and__Y1_false__and__Y2_true__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_true__and__Y4_false[\"Y4=false\"]\nS__and__X_1 -- 0.5 --&gt; S__and__X_1__and__Y1_true[\"Y1=true\"]\nS -- 0.33 --&gt; S__and__X_2[\"X=2\"]\nS__and__X_1__and__Y1_true__and__Y2_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_false[\"Y3=false\"]\nS__and__X_2__and__Y1_true__and__Y2_true -- 1.0 --&gt; S__and__X_2__and__Y1_true__and__Y2_true__and__Y3_true[\"Y3=true\"]\nS__and__X_1__and__Y1_false__and__Y2_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_true[\"Y3=true\"]\nS__and__X_1__and__Y1_false__and__Y2_true__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_false__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_true__and__Y2_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_true[\"Y3=true\"]\nS__and__X_1__and__Y1_true__and__Y2_true__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_true__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_false__and__Y2_true__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_false__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_true__and__Y2_true__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_false__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_false__and__Y2_false__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_true__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_true__and__Y2_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_true[\"Y3=true\"]\nS__and__X_1__and__Y1_true__and__Y2_false__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_false__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_false__and__Y2_false__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_true__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_false__and__Y2_true__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_true__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_false__and__Y2_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_false[\"Y3=false\"]\nS__and__X_0__and__Y1_false__and__Y2_false -- 1.0 --&gt; S__and__X_0__and__Y1_false__and__Y2_false__and__Y3_false[\"Y3=false\"]"
  },
  {
    "objectID": "w01_discrete_inference/topic01_outcomes.html",
    "href": "w01_discrete_inference/topic01_outcomes.html",
    "title": "Sample space, outcomes, events",
    "section": "",
    "text": "Review of basic probability theory concepts: outcome, event, sample space\nIntuition from the Bayesian perspective\n\n\n\n\n\nOne definition of Bayesian inference: applying probability theory to statistical inference problems\n\nTherefore, it is critical to understand probability to learn Bayesian inference\nThis week, we will help you “reload in memory” some of the most important bits of probability theory used in this course"
  },
  {
    "objectID": "w01_discrete_inference/topic01_outcomes.html#outline",
    "href": "w01_discrete_inference/topic01_outcomes.html#outline",
    "title": "Sample space, outcomes, events",
    "section": "",
    "text": "Review of basic probability theory concepts: outcome, event, sample space\nIntuition from the Bayesian perspective\n\n\n\n\n\nOne definition of Bayesian inference: applying probability theory to statistical inference problems\n\nTherefore, it is critical to understand probability to learn Bayesian inference\nThis week, we will help you “reload in memory” some of the most important bits of probability theory used in this course"
  },
  {
    "objectID": "w01_discrete_inference/topic01_outcomes.html#definitions",
    "href": "w01_discrete_inference/topic01_outcomes.html#definitions",
    "title": "Sample space, outcomes, events",
    "section": "Definitions",
    "text": "Definitions\n\nSample space, denoted \\(S\\), a set.\n\nExample: \\(S = \\{1, 2, 3, 4\\}\\) (see Figure).\n\nEach element \\(s\\) of \\(S\\) is called an outcome, \\(s \\in S\\).\n\nExample: each of the 4 points.\n\nA set of outcomes \\(E \\subset S\\) is called an event.\n\nExample: \\(E = \\{s \\in S : s \\text{ is odd}\\}\\) (red in the Figure)."
  },
  {
    "objectID": "w01_discrete_inference/topic01_outcomes.html#intuition-bayesian-view",
    "href": "w01_discrete_inference/topic01_outcomes.html#intuition-bayesian-view",
    "title": "Sample space, outcomes, events",
    "section": "Intuition: Bayesian view",
    "text": "Intuition: Bayesian view\n\nIn Bayesian statistics, an outcome will describe the state of the world.\nWe do not know which outcome is the true state of the world.\nWe observe partial information on the state of the world/outcome.\nWe rule out the outcomes that are not consistent with the observation…\n…but there will be several outcomes left!\n\nWe will deal with this situation using probability theory."
  },
  {
    "objectID": "w01_discrete_inference/topic01_outcomes.html#intuition-randomized-algorithms",
    "href": "w01_discrete_inference/topic01_outcomes.html#intuition-randomized-algorithms",
    "title": "Sample space, outcomes, events",
    "section": "Intuition: randomized algorithms",
    "text": "Intuition: randomized algorithms\n\nAn algorithm is “randomized” if it has access to virtual dices/coins.\nIn practice this is done using pseudorandom number generators.\nIn this context an outcome is a random seed, i.e. the initialization of the pseudorandom number generator."
  },
  {
    "objectID": "w01_discrete_inference/topic03_random_variables.html",
    "href": "w01_discrete_inference/topic03_random_variables.html",
    "title": "Random variables",
    "section": "",
    "text": "Random variable as mathematical objects.\nNotation convention for observation/latent\n\n\n\n\nRandom variables are used as building blocks for two key uses in Bayesian stats: modelling “knowns” (observations) and “unknowns” (latent variables/parameters/prediction)."
  },
  {
    "objectID": "w01_discrete_inference/topic03_random_variables.html#outline",
    "href": "w01_discrete_inference/topic03_random_variables.html#outline",
    "title": "Random variables",
    "section": "",
    "text": "Random variable as mathematical objects.\nNotation convention for observation/latent\n\n\n\n\nRandom variables are used as building blocks for two key uses in Bayesian stats: modelling “knowns” (observations) and “unknowns” (latent variables/parameters/prediction)."
  },
  {
    "objectID": "w01_discrete_inference/topic03_random_variables.html#definition",
    "href": "w01_discrete_inference/topic03_random_variables.html#definition",
    "title": "Random variables",
    "section": "Definition",
    "text": "Definition\nA (real) random variable is a function from a sample space \\(S\\) to the reals, \\(X : S \\to \\mathbb{R}\\).\nExample:\n\nContinuing the example with \\(S = \\{1, 2, 3, 4\\}\\).\nConsider \\(X(s) = 1\\) if \\(s\\) is odd, and \\(X(s) = 0\\) otherwise."
  },
  {
    "objectID": "w01_discrete_inference/topic03_random_variables.html#probabilists-notation",
    "href": "w01_discrete_inference/topic03_random_variables.html#probabilists-notation",
    "title": "Random variables",
    "section": "Probabilist’s notation",
    "text": "Probabilist’s notation\n\nLet \\(X\\) denote a random variable.\nThe notation \\((X = 1)\\) or \\((X \\in E)\\) is invalid in set theory.\nTherefore, probabilists “gave it a meaning” as follows:\n\n\\[(X = 1) = \\{s : X(s) = 1\\}.\\]\nExample: Consider \\(X(s) = 1\\) if \\(s\\) is odd, and \\(X(s) = 0\\) otherwise. Then \\((X = 1)\\) corresponds to the red circle."
  },
  {
    "objectID": "w01_discrete_inference/topic03_random_variables.html#conventions-probability-vs-bayesian",
    "href": "w01_discrete_inference/topic03_random_variables.html#conventions-probability-vs-bayesian",
    "title": "Random variables",
    "section": "Conventions: probability vs Bayesian",
    "text": "Conventions: probability vs Bayesian\nProbability convention:\n\nRandom variables are denoted with capitals in probability theory\nThe same letter in small cap is used for a dummy variable holding the output of the random variable.\n\nNote: “A dummy variable holding the output of the random variable” is called a realization.\nExample: \\(X\\) for the random variable and \\(x\\) for its realization.\n\nWe will start off using this convention in the first few weeks.\n\nBayesian statistics convention:\n\nOften the capitalization convention is not used in the Bayesian statistics literature.\nHence we will eventually drop the probability theory capitalization convention."
  },
  {
    "objectID": "w01_discrete_inference/topic03_random_variables.html#more-conventions",
    "href": "w01_discrete_inference/topic03_random_variables.html#more-conventions",
    "title": "Random variables",
    "section": "More conventions",
    "text": "More conventions\n\n\\(X\\): unobserved random variable\n\\(Y\\): observed random variable\n\nMore precisely:\n\n\\(Y\\) is the “mechanism of observation”..\nwhereas the actual observation is a realization \\(y\\) of \\(Y\\)."
  },
  {
    "objectID": "w01_discrete_inference/topic03_random_variables.html#extension",
    "href": "w01_discrete_inference/topic03_random_variables.html#extension",
    "title": "Random variables",
    "section": "Extension",
    "text": "Extension\nA random vector is a function from a sample space to \\(\\mathbb{R}^n\\).\nExample in Bayesian statistics: the vector \\((X, Y)\\) containing both the unobserved and observed quantities."
  },
  {
    "objectID": "w01_discrete_inference/topic06_forward_sampling.html",
    "href": "w01_discrete_inference/topic06_forward_sampling.html",
    "title": "Forward sampling",
    "section": "",
    "text": "Notion of forward sampling (also known as forward simulation)\nHow to do it in practice\n\nUseful functions\nGraphical models\n\n\n\n\n\n\nSampling is the main way Bayesian inference is performed nowadays.\nWe introduce here the simplest flavour of sampling, forward sampling.\nBayesian inference mostly uses a more complicated type of sampling called posterior sampling which we will cover later.\nBut forward sampling is still helpful to help debug Bayesian inference software as we will see soon."
  },
  {
    "objectID": "w01_discrete_inference/topic06_forward_sampling.html#outline",
    "href": "w01_discrete_inference/topic06_forward_sampling.html#outline",
    "title": "Forward sampling",
    "section": "",
    "text": "Notion of forward sampling (also known as forward simulation)\nHow to do it in practice\n\nUseful functions\nGraphical models\n\n\n\n\n\n\nSampling is the main way Bayesian inference is performed nowadays.\nWe introduce here the simplest flavour of sampling, forward sampling.\nBayesian inference mostly uses a more complicated type of sampling called posterior sampling which we will cover later.\nBut forward sampling is still helpful to help debug Bayesian inference software as we will see soon."
  },
  {
    "objectID": "w01_discrete_inference/topic06_forward_sampling.html#forward-sampling-as-depth-first-traversal",
    "href": "w01_discrete_inference/topic06_forward_sampling.html#forward-sampling-as-depth-first-traversal",
    "title": "Forward sampling",
    "section": "Forward sampling as depth-first traversal",
    "text": "Forward sampling as depth-first traversal\nRecall our recurring bag sampling example, with its corresponding decision tree:\n\n\n\n\n\nflowchart TD\nS__and__X_0 -- 1.0 --&gt; S__and__X_0__and__Y1_false[\"Y1=false\"]\nS__and__X_2__and__Y1_true -- 1.0 --&gt; S__and__X_2__and__Y1_true__and__Y2_true[\"Y2=true\"]\nS -- 0.33 --&gt; S__and__X_0[\"X=0\"]\nS__and__X_1__and__Y1_true__and__Y2_true__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_true__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_true__and__Y2_false__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_true__and__Y4_false[\"Y4=false\"]\nS__and__X_0__and__Y1_false__and__Y2_false__and__Y3_false -- 1.0 --&gt; S__and__X_0__and__Y1_false__and__Y2_false__and__Y3_false__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_false__and__Y2_false__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_false__and__Y4_true[\"Y4=true\"]\nS -- 0.33 --&gt; S__and__X_1[\"X=1\"]\nS__and__X_2__and__Y1_true__and__Y2_true__and__Y3_true -- 1.0 --&gt; S__and__X_2__and__Y1_true__and__Y2_true__and__Y3_true__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_true__and__Y2_false__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_false__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_false__and__Y2_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_false[\"Y3=false\"]\nS__and__X_1__and__Y1_true__and__Y2_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_false[\"Y3=false\"]\nS__and__X_1__and__Y1_false__and__Y2_false__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_false__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_true__and__Y2_true__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_false__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_true__and__Y2_false__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_true__and__Y4_true[\"Y4=true\"]\nS__and__X_1 -- 0.5 --&gt; S__and__X_1__and__Y1_false[\"Y1=false\"]\nS__and__X_1__and__Y1_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false[\"Y2=false\"]\nS__and__X_1__and__Y1_false__and__Y2_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_true[\"Y3=true\"]\nS__and__X_1__and__Y1_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true[\"Y2=true\"]\nS__and__X_0__and__Y1_false -- 1.0 --&gt; S__and__X_0__and__Y1_false__and__Y2_false[\"Y2=false\"]\nS__and__X_1__and__Y1_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false[\"Y2=false\"]\nS__and__X_1__and__Y1_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true[\"Y2=true\"]\nS__and__X_2 -- 1.0 --&gt; S__and__X_2__and__Y1_true[\"Y1=true\"]\nS__and__X_1__and__Y1_false__and__Y2_true__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_true__and__Y4_false[\"Y4=false\"]\nS__and__X_1 -- 0.5 --&gt; S__and__X_1__and__Y1_true[\"Y1=true\"]\nS -- 0.33 --&gt; S__and__X_2[\"X=2\"]\nS__and__X_1__and__Y1_true__and__Y2_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_false[\"Y3=false\"]\nS__and__X_2__and__Y1_true__and__Y2_true -- 1.0 --&gt; S__and__X_2__and__Y1_true__and__Y2_true__and__Y3_true[\"Y3=true\"]\nS__and__X_1__and__Y1_false__and__Y2_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_true[\"Y3=true\"]\nS__and__X_1__and__Y1_false__and__Y2_true__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_false__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_true__and__Y2_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_true[\"Y3=true\"]\nS__and__X_1__and__Y1_true__and__Y2_true__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_true__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_false__and__Y2_true__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_false__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_true__and__Y2_true__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_false__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_false__and__Y2_false__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_true__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_true__and__Y2_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_true[\"Y3=true\"]\nS__and__X_1__and__Y1_true__and__Y2_false__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_false__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_false__and__Y2_false__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_true__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_false__and__Y2_true__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_true__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_false__and__Y2_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_false[\"Y3=false\"]\nS__and__X_0__and__Y1_false__and__Y2_false -- 1.0 --&gt; S__and__X_0__and__Y1_false__and__Y2_false__and__Y3_false[\"Y3=false\"]\n\n\n\n\n\n\n\nForward simulation is a type of tree traversal. I.e. moving from node to node in the tree.\nForward simulation is a recursive process initialized at the root of the decision tree (labelled \\(S\\)).\n\nWhen we are a node \\(v\\) in the tree, we pick one of \\(v\\)’s children at random.\n\nMore precisely, we use methods discussed in the previous section on simulation from PMFs\n\nWe recurse until we reach a leaf.\n\nFrom this leaf we obtain an outcome and hence a realization for all random variables, both “observed” and “unobserved.”"
  },
  {
    "objectID": "w01_discrete_inference/topic06_forward_sampling.html#forward-sampling-as-specifying-a-model",
    "href": "w01_discrete_inference/topic06_forward_sampling.html#forward-sampling-as-specifying-a-model",
    "title": "Forward sampling",
    "section": "Forward sampling as specifying a model",
    "text": "Forward sampling as specifying a model\nWe have encountered that notation earlier:\n\\[\n\\begin{align*}\nX &\\sim {\\mathrm{Unif}}\\{0, 1, 2\\} \\\\\nY_i | X &\\sim {\\mathrm{Bern}}(X/2).\n\\end{align*}\n\\]\n\nThis notation is a recipe providing all the information required to perform forward sampling.\n\nSpecifically, the PMF to use at each recursion step.\nIn continuous models, it will be the same idea except that we will have a probability density instead of a PMF."
  },
  {
    "objectID": "w01_discrete_inference/topic09_bayes.html",
    "href": "w01_discrete_inference/topic09_bayes.html",
    "title": "Bayes rule",
    "section": "",
    "text": "Bayes rule for discrete models\nVisual intuition\n\n\n\n\nFirst example of computing a posterior distribution, a key concept in Bayesian statistics."
  },
  {
    "objectID": "w01_discrete_inference/topic09_bayes.html#outline",
    "href": "w01_discrete_inference/topic09_bayes.html#outline",
    "title": "Bayes rule",
    "section": "",
    "text": "Bayes rule for discrete models\nVisual intuition\n\n\n\n\nFirst example of computing a posterior distribution, a key concept in Bayesian statistics."
  },
  {
    "objectID": "w01_discrete_inference/topic09_bayes.html#running-example",
    "href": "w01_discrete_inference/topic09_bayes.html#running-example",
    "title": "Bayes rule",
    "section": "Running example",
    "text": "Running example\n\nImagine a bag with 3 coins each with a different probability parameter \\(p\\)\nCoin \\(i\\in \\{0, 1, 2\\}\\) has bias \\(i/2\\)—in other words:\n\nFirst coin: bias is \\(0/2 = 0\\) (i.e. both sides are “tails”, \\(p = 0\\))\nSecond coin: bias is \\(1/2 = 0.5\\) (i.e. standard coin, \\(p = 1/2\\))\nThird coin: bias is \\(2/2 = 1\\) (i.e. both sides are “heads”, \\(p = 1\\))\n\n\n\n\n\n\nConsider the following two steps sampling process\n\nStep 1: pick one of the three coins, but do not look at it!\nStep 2: flip the coin 4 times\n\nMathematically, this probability model can be written as follows: \\[\n\\begin{align*}\nX &\\sim {\\mathrm{Unif}}\\{0, 1, 2\\} \\\\\nY_i | X &\\sim {\\mathrm{Bern}}(X/2)\n\\end{align*}\n\\tag{1}\\]\n\nConsider the question in the first exercise:\nSuppose now that you observe the outcome of the 4 coin flips, but not the type of coin that was picked. Say you observe: “heads”, “heads”, “heads”, “heads” = [0, 0, 0, 0]. Given that observation, what is the probability that you picked the standard coin (i.e., the one with \\(p = 1/2\\))?"
  },
  {
    "objectID": "w01_discrete_inference/topic09_bayes.html#strategy",
    "href": "w01_discrete_inference/topic09_bayes.html#strategy",
    "title": "Bayes rule",
    "section": "Strategy",
    "text": "Strategy\nDenote the observation by \\(y_{1:4} = (0, 0, 0, 0)\\).\n\nAttack the more general problem \\(\\pi(x) = \\mathbb{P}(X = x | Y_{1:4} = y_{1:4})\\).\nBy definition of conditioning: \\[\\pi(x) = \\frac{\\mathbb{P}(X = x, Y_{1:4} = y_{1:4})}{\\mathbb{P}(Y_{1:4} = y_{1:4})}.\\] Let us call the numerator \\[\\gamma(x) = \\mathbb{P}(X = x, Y_{1:4} = y_{1:4}),\\] and the denominator, \\[Z = \\mathbb{P}(Y_{1:4} = y_{1:4}).\\]\nStart by computing \\(\\gamma(x)\\) for all \\(x\\). (using chain rule)\nNote \\(Z = \\gamma(0) + \\gamma(1) + \\gamma(2)\\) (why?)."
  },
  {
    "objectID": "w01_discrete_inference/topic08_chain.html",
    "href": "w01_discrete_inference/topic08_chain.html",
    "title": "Chain rule",
    "section": "",
    "text": "Mathematical statement\nVisual intuition on a decision tree\nSpecial names for the pieces of chain rule (joint and conditional PMFs)\nConditional independence\n\n\n\n\nThe chain rule seem innocent but is used heavily in Bayesian statistics. It is also the building block for Bayes rule."
  },
  {
    "objectID": "w01_discrete_inference/topic08_chain.html#outline",
    "href": "w01_discrete_inference/topic08_chain.html#outline",
    "title": "Chain rule",
    "section": "",
    "text": "Mathematical statement\nVisual intuition on a decision tree\nSpecial names for the pieces of chain rule (joint and conditional PMFs)\nConditional independence\n\n\n\n\nThe chain rule seem innocent but is used heavily in Bayesian statistics. It is also the building block for Bayes rule."
  },
  {
    "objectID": "w01_discrete_inference/topic08_chain.html#proposition",
    "href": "w01_discrete_inference/topic08_chain.html#proposition",
    "title": "Chain rule",
    "section": "Proposition",
    "text": "Proposition\nIf \\(E_1\\) and \\(E_2\\) are any events, \\[\\mathbb{P}(E_1, E_2) = \\mathbb{P}(E_1) \\mathbb{P}(E_2 | E_1).\\]\nThis is true in any order, i.e. we also have \\(\\mathbb{P}(E_1, E_2) = \\mathbb{P}(E_2) \\mathbb{P}(E_1 | E_2)\\)."
  },
  {
    "objectID": "w01_discrete_inference/topic08_chain.html#generalization",
    "href": "w01_discrete_inference/topic08_chain.html#generalization",
    "title": "Chain rule",
    "section": "Generalization",
    "text": "Generalization\nFor any events \\(E_1, E_2, E_3 \\dots\\),\n\\[\\mathbb{P}(E_1, E_2, E_3 \\dots) = \\mathbb{P}(E_1) \\mathbb{P}(E_2 | E_1) \\mathbb{P}(E_3 | E_1, E_2) \\dots.\\]"
  },
  {
    "objectID": "w01_discrete_inference/topic08_chain.html#visual-intuition",
    "href": "w01_discrete_inference/topic08_chain.html#visual-intuition",
    "title": "Chain rule",
    "section": "Visual intuition",
    "text": "Visual intuition\nChain rule: the probability of a node is the product of the edge labels on the path to the root.\n\n\n\n\n\nflowchart TD\nS__and__X_0 -- 1.0 --&gt; S__and__X_0__and__Y1_false[\"Y1=false\"]\nS__and__X_2__and__Y1_true -- 1.0 --&gt; S__and__X_2__and__Y1_true__and__Y2_true[\"Y2=true\"]\nS -- 0.33 --&gt; S__and__X_0[\"X=0\"]\nS__and__X_1__and__Y1_true__and__Y2_true__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_true__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_true__and__Y2_false__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_true__and__Y4_false[\"Y4=false\"]\nS__and__X_0__and__Y1_false__and__Y2_false__and__Y3_false -- 1.0 --&gt; S__and__X_0__and__Y1_false__and__Y2_false__and__Y3_false__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_false__and__Y2_false__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_false__and__Y4_true[\"Y4=true\"]\nS -- 0.33 --&gt; S__and__X_1[\"X=1\"]\nS__and__X_2__and__Y1_true__and__Y2_true__and__Y3_true -- 1.0 --&gt; S__and__X_2__and__Y1_true__and__Y2_true__and__Y3_true__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_true__and__Y2_false__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_false__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_false__and__Y2_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_false[\"Y3=false\"]\nS__and__X_1__and__Y1_true__and__Y2_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_false[\"Y3=false\"]\nS__and__X_1__and__Y1_false__and__Y2_false__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_false__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_true__and__Y2_true__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_false__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_true__and__Y2_false__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_true__and__Y4_true[\"Y4=true\"]\nS__and__X_1 -- 0.5 --&gt; S__and__X_1__and__Y1_false[\"Y1=false\"]\nS__and__X_1__and__Y1_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false[\"Y2=false\"]\nS__and__X_1__and__Y1_false__and__Y2_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_true[\"Y3=true\"]\nS__and__X_1__and__Y1_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true[\"Y2=true\"]\nS__and__X_0__and__Y1_false -- 1.0 --&gt; S__and__X_0__and__Y1_false__and__Y2_false[\"Y2=false\"]\nS__and__X_1__and__Y1_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false[\"Y2=false\"]\nS__and__X_1__and__Y1_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true[\"Y2=true\"]\nS__and__X_2 -- 1.0 --&gt; S__and__X_2__and__Y1_true[\"Y1=true\"]\nS__and__X_1__and__Y1_false__and__Y2_true__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_true__and__Y4_false[\"Y4=false\"]\nS__and__X_1 -- 0.5 --&gt; S__and__X_1__and__Y1_true[\"Y1=true\"]\nS -- 0.33 --&gt; S__and__X_2[\"X=2\"]\nS__and__X_1__and__Y1_true__and__Y2_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_false[\"Y3=false\"]\nS__and__X_2__and__Y1_true__and__Y2_true -- 1.0 --&gt; S__and__X_2__and__Y1_true__and__Y2_true__and__Y3_true[\"Y3=true\"]\nS__and__X_1__and__Y1_false__and__Y2_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_true[\"Y3=true\"]\nS__and__X_1__and__Y1_false__and__Y2_true__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_false__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_true__and__Y2_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_true[\"Y3=true\"]\nS__and__X_1__and__Y1_true__and__Y2_true__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_true__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_false__and__Y2_true__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_false__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_true__and__Y2_true__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_false__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_false__and__Y2_false__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_true__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_true__and__Y2_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_true[\"Y3=true\"]\nS__and__X_1__and__Y1_true__and__Y2_false__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_false__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_false__and__Y2_false__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_true__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_false__and__Y2_true__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_true__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_false__and__Y2_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_false[\"Y3=false\"]\nS__and__X_0__and__Y1_false__and__Y2_false -- 1.0 --&gt; S__and__X_0__and__Y1_false__and__Y2_false__and__Y3_false[\"Y3=false\"]\nstyle S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_true fill:#f9f,stroke:#333,stroke-width:4px\n\n\n\n\n\n\nExample: what is the probability of the node in red?"
  },
  {
    "objectID": "w01_discrete_inference/topic08_chain.html#poll-probability-of-a-node",
    "href": "w01_discrete_inference/topic08_chain.html#poll-probability-of-a-node",
    "title": "Chain rule",
    "section": "Poll: probability of a node",
    "text": "Poll: probability of a node\n\n1/2\n1/4\n1/8\n1/24\nNone of the above\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nMultiplying the four edge leading to the node we get: \\(1/2 \\cdot 1/2 \\cdot 1/2 \\cdot 1/3 = 1/24\\).\n\n\n\nReview: what is the event corresponding to that node?"
  },
  {
    "objectID": "w01_discrete_inference/topic08_chain.html#poll-event-for-a-node",
    "href": "w01_discrete_inference/topic08_chain.html#poll-event-for-a-node",
    "title": "Chain rule",
    "section": "Poll: event for a node",
    "text": "Poll: event for a node\n\n\\((Y_3 = 1)\\)\n\\((Y_3 = 1, Y_2 = 1)\\)\n\\((Y_3 = 1, Y_2 = 1)\\)\n\\((Y_3 = 1, Y_2 = 1, Y_1 = 1)\\)\n\\((Y_3 = 1, Y_2 = 1, Y_1 = 1, X = 1)\\)\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nRecall that the event is the intersection of all node labels to the root, hence the event is \\((Y_3 = 1, Y_2 = 1, Y_1 = 1, X = 1)\\).\nThe calculation we did visually in the previous clicker question is mathematically: \\[\\mathbb{P}(Y_3 = 1, Y_2 = 1, Y_1 = 1, X = 1) = \\mathbb{P}(X = 1) \\mathbb{P}(Y_1 | X = 1) \\mathbb{P}(Y_2 | X = 1, Y_1 = 1) \\mathbb{P}(Y_3 = 1 | X = 1, Y_1 = 1, Y_2 = 1).\\]"
  },
  {
    "objectID": "w01_discrete_inference/topic08_chain.html#joint-pmf",
    "href": "w01_discrete_inference/topic08_chain.html#joint-pmf",
    "title": "Chain rule",
    "section": "Joint PMF",
    "text": "Joint PMF\nWe will often encounter expression of the form of a conjunction (intersection/and) of several variables. A handy notation for that is the joint PMF\nFor example, here is the joint PMF of \\((X, Y_1, Y_2, Y_3)\\):\n\\[p(x, y_1, y_2, y_3) = \\mathbb{P}(X = x, Y_1 = y_1, Y_2 = y_2, Y_3 = y_3).\\]\nSometimes we put the random variables in question as subscript, for example \\(p_{X, Y_1}(x, y)\\) for the joint PMF of \\(X\\) and \\(Y_1\\)."
  },
  {
    "objectID": "w01_discrete_inference/topic08_chain.html#conditional-pmf",
    "href": "w01_discrete_inference/topic08_chain.html#conditional-pmf",
    "title": "Chain rule",
    "section": "Conditional PMF",
    "text": "Conditional PMF\nSimilarly, here is an example of a conditional PMF: \\[p_{Y_1|X}(y | x) = \\mathbb{P}(Y_1 = y | X = x).\\]"
  },
  {
    "objectID": "w01_discrete_inference/topic08_chain.html#conditional-independence",
    "href": "w01_discrete_inference/topic08_chain.html#conditional-independence",
    "title": "Chain rule",
    "section": "Conditional independence",
    "text": "Conditional independence\nThe model was specified as:\n\\[\n\\begin{align*}\nX &\\sim {\\mathrm{Unif}}\\{0, 1, 2\\} \\\\\nY_i | X &\\sim {\\mathrm{Bern}}(X/2)\n\\end{align*}\n\\] i.e. with \\(\\mathbb{P}(X = x)\\) and \\(\\mathbb{P}(Y_i = y | X = x)\\) for all \\(x\\) and \\(y\\).\nQuestion: how did we go from \\(\\mathbb{P}(Y_2 | X = 1, Y_1 = 1)\\) (in our chain rule computation) to \\(\\mathbb{P}(Y_2 | X = 1)\\) (model specification)?\nDefinition: \\(V\\) and \\(W\\) are conditionally independence given \\(Z\\) if \\[\\mathbb{P}(V = v, W = w | Z = z) = \\mathbb{P}(V = v | Z = z) \\mathbb{P}(W = w | Z = z).\\]\nExercise: show the above definition is equivalent to:\n\\[\\mathbb{P}(V = v | W = w, Z = z) = \\mathbb{P}(V = v | Z = z).\\]"
  },
  {
    "objectID": "w01_discrete_inference/topic07_conditional.html",
    "href": "w01_discrete_inference/topic07_conditional.html",
    "title": "Conditioning",
    "section": "",
    "text": "Intuition on conditioning\nA conditional probability is a probability.\n\n\n\n\n\nConditioning is the workhorse of Bayesian inference!\n\nUsed to define models (as when we assigned probabilities to edges of a decision tree)\nAnd soon, to gain information on latent variables given observations."
  },
  {
    "objectID": "w01_discrete_inference/topic07_conditional.html#outline",
    "href": "w01_discrete_inference/topic07_conditional.html#outline",
    "title": "Conditioning",
    "section": "",
    "text": "Intuition on conditioning\nA conditional probability is a probability.\n\n\n\n\n\nConditioning is the workhorse of Bayesian inference!\n\nUsed to define models (as when we assigned probabilities to edges of a decision tree)\nAnd soon, to gain information on latent variables given observations."
  },
  {
    "objectID": "w01_discrete_inference/topic07_conditional.html#conditioning-as-belief-update",
    "href": "w01_discrete_inference/topic07_conditional.html#conditioning-as-belief-update",
    "title": "Conditioning",
    "section": "Conditioning as belief update",
    "text": "Conditioning as belief update\nKey concept: Bayesian methods use probabilities to encode beliefs.\n\nWe will explore this perspective in much more details next week."
  },
  {
    "objectID": "w01_discrete_inference/topic07_conditional.html#a-conditional-probability-is-a-probability",
    "href": "w01_discrete_inference/topic07_conditional.html#a-conditional-probability-is-a-probability",
    "title": "Conditioning",
    "section": "A conditional probability is a probability",
    "text": "A conditional probability is a probability\nThe “updated belief” interpretation highlights the fact that we want the result of the conditioning procedure, \\(\\mathbb{P}(\\cdot | E)\\) to be a probability when viewed as a function of the first argument for any fixed \\(E\\)."
  },
  {
    "objectID": "w01_discrete_inference/topic07_conditional.html#intuition-behind-conditioning",
    "href": "w01_discrete_inference/topic07_conditional.html#intuition-behind-conditioning",
    "title": "Conditioning",
    "section": "Intuition behind conditioning",
    "text": "Intuition behind conditioning\n\n\n\n\n\n\n\nFor a query even \\(A\\), what should be the updated probability?\nWe want to remove from \\(A\\) all the outcomes that are not compatible with the new information \\(E\\). How?\n\nTake the intersection: \\(A \\cap E\\)\nWe also want: \\(\\mathbb{P}(S | E) = 1\\) (last section)\nHow? Renormalize: \\[\\mathbb{P}(A | E) = \\frac{\\mathbb{P}(A \\cap E)}{\\mathbb{P}(E)}\\]\nIntersection can also be denoted using a comma, for example \\(\\mathbb{P}(A \\cap E) = \\mathbb{P}(A, E)\\)"
  },
  {
    "objectID": "w01_discrete_inference/topic04_pmfs.html",
    "href": "w01_discrete_inference/topic04_pmfs.html",
    "title": "Probability mass functions",
    "section": "",
    "text": "Probability Mass Function (PMF):\n\ndenoted by \\(p\\) (not to be confused by \\(\\mathbb{P}\\)),\ndefined by: \\[p(x) = \\mathbb{P}(X = x).\\]\nIf there are several random variables, we use a subscript to disambiguate the PMFS, e.g.,\n\n\\(p_X\\) for the PMF of the latent random variable,\n\\(p_Y\\) for the PMF of the observed random variable."
  },
  {
    "objectID": "w01_discrete_inference/topic04_pmfs.html#definition",
    "href": "w01_discrete_inference/topic04_pmfs.html#definition",
    "title": "Probability mass functions",
    "section": "",
    "text": "Probability Mass Function (PMF):\n\ndenoted by \\(p\\) (not to be confused by \\(\\mathbb{P}\\)),\ndefined by: \\[p(x) = \\mathbb{P}(X = x).\\]\nIf there are several random variables, we use a subscript to disambiguate the PMFS, e.g.,\n\n\\(p_X\\) for the PMF of the latent random variable,\n\\(p_Y\\) for the PMF of the observed random variable."
  },
  {
    "objectID": "w01_discrete_inference/topic04_pmfs.html#sec-simulate-pmf",
    "href": "w01_discrete_inference/topic04_pmfs.html#sec-simulate-pmf",
    "title": "Probability mass functions",
    "section": "Simulation/sampling from a PMF",
    "text": "Simulation/sampling from a PMF\n\nIn R\n\nrequire(extraDistr)\n\nLoading required package: extraDistr\n\n# 10 coin flips:\nrbern(10, prob=0.5)\n\n [1] 0 1 1 1 0 1 0 1 0 1\n\n# two dice rolls\nrdunif(2, min=1, max=6)\n\n[1] 1 3\n\n\n\n\nHow does it work?\n\nThink of the green lines in the uniform PMF as “sticks” with “labels.”\n\nThe “labels” are the different realization, e.g. 1, 2, 3, …, 6.\n\nPlace the six sticks in the interval \\([0, 1]\\) so that they do not overlap.\nSample a uniform real number in \\([0, 1]\\)\n\nin R: runif(1)\n\nThe uniform falls in exactly one of the sticks.\nReturn the label of that stick.\n\n\n\nMathematical explanation\nThe above “stick-based” algorithm can be implemented using the cumulative distribution function and a generalization of its inverse known as the quantile function.\nCumulative distribution function (CDF): \\(F(x) = \\mathbb{P}(X \\le x).\\)\nQuantile function: \\(F^{-1}(u) = \\inf\\{x \\in \\mathbb{R}: u \\le F(x)\\}\\).\nThen the “stick-based” algorithm can be written as:\n\n\\(U \\sim {\\mathrm{Unif}}[0, 1]\\)\nReturn \\(F^{-1}(U)\\)."
  },
  {
    "objectID": "w01_discrete_inference/topic02_axioms.html",
    "href": "w01_discrete_inference/topic02_axioms.html",
    "title": "Axioms of probability",
    "section": "",
    "text": "Definition: \\(E_1, E_2, \\dots\\) is a partition of \\(E\\) if:\n\nthe \\(E_i\\)’s are disjoint, i.e., \\[E_i \\cap E_j = \\emptyset \\text{ when } i\\neq j,\\]\nand their union is \\(E\\), i.e., \\(\\cup_i E_i = E\\)."
  },
  {
    "objectID": "w01_discrete_inference/topic02_axioms.html#partitions",
    "href": "w01_discrete_inference/topic02_axioms.html#partitions",
    "title": "Axioms of probability",
    "section": "",
    "text": "Definition: \\(E_1, E_2, \\dots\\) is a partition of \\(E\\) if:\n\nthe \\(E_i\\)’s are disjoint, i.e., \\[E_i \\cap E_j = \\emptyset \\text{ when } i\\neq j,\\]\nand their union is \\(E\\), i.e., \\(\\cup_i E_i = E\\)."
  },
  {
    "objectID": "w01_discrete_inference/topic02_axioms.html#axioms-of-probability",
    "href": "w01_discrete_inference/topic02_axioms.html#axioms-of-probability",
    "title": "Axioms of probability",
    "section": "Axioms of probability",
    "text": "Axioms of probability\n\nA probability is a function \\(\\mathbb{P}\\) that satisfy the following constraints:\n\n\\(\\mathbb{P}\\) should take events as input and return a number between zero and one: \\[\\mathbb{P}(E) \\in [0, 1].\\]\nIf \\(E_1, E_2, \\dots\\) is a partition of \\(E\\), then \\[\\mathbb{P}(E) = \\sum_i \\mathbb{P}(E_i).\\]\n\\(\\mathbb{P}(S) = 1\\)\n\nThanks to the constraints, even if I only specify a few known probabilities I can recover many other ones mathematically/computationally."
  },
  {
    "objectID": "w01_discrete_inference/topic05_decision_diagrams.html",
    "href": "w01_discrete_inference/topic05_decision_diagrams.html",
    "title": "Decision trees",
    "section": "",
    "text": "Decision trees\nReview of more probability theory concepts, contextualized in decision trees: outcome, event, sample space, partitions, conditional probability\n\n\n\n\nWe will use decision trees to provide visualization for a bunch of complex concepts such as forward simulation, posterior inference, importance sampling, probabilistic programming, etc."
  },
  {
    "objectID": "w01_discrete_inference/topic05_decision_diagrams.html#outline",
    "href": "w01_discrete_inference/topic05_decision_diagrams.html#outline",
    "title": "Decision trees",
    "section": "",
    "text": "Decision trees\nReview of more probability theory concepts, contextualized in decision trees: outcome, event, sample space, partitions, conditional probability\n\n\n\n\nWe will use decision trees to provide visualization for a bunch of complex concepts such as forward simulation, posterior inference, importance sampling, probabilistic programming, etc."
  },
  {
    "objectID": "w01_discrete_inference/topic05_decision_diagrams.html#running-example",
    "href": "w01_discrete_inference/topic05_decision_diagrams.html#running-example",
    "title": "Decision trees",
    "section": "Running example",
    "text": "Running example\n\nImagine a bag with 3 coins each with a different probability parameter \\(p\\)\nCoin \\(i\\in \\{0, 1, 2\\}\\) has bias \\(i/2\\)—in other words:\n\nFirst coin: bias is \\(0/2 = 0\\) (i.e. both sides are “tails”, \\(p = 0\\))\nSecond coin: bias is \\(1/2 = 0.5\\) (i.e. standard coin, \\(p = 1/2\\))\nThird coin: bias is \\(2/2 = 1\\) (i.e. both sides are “heads”, \\(p = 1\\))\n\n\n\n\n\n\nConsider the following two steps sampling process\n\nStep 1: pick one of the three coins, but do not look at it!\nStep 2: flip the coin 4 times\n\nMathematically, this probability model can be written as follows: \\[\n\\begin{align*}\nX &\\sim {\\mathrm{Unif}}\\{0, 1, 2\\} \\\\\nY_i | X &\\sim {\\mathrm{Bern}}(X/2)\n\\end{align*}\n\\tag{1}\\]"
  },
  {
    "objectID": "w01_discrete_inference/topic05_decision_diagrams.html#decision-tree",
    "href": "w01_discrete_inference/topic05_decision_diagrams.html#decision-tree",
    "title": "Decision trees",
    "section": "Decision tree",
    "text": "Decision tree\n\nDecision tree: a recursive classification of all possible scenarios\nNodes in the tree are “groups of scenarios” which we call events\nChildren of a node partitions an event into an exhaustive set of sub-cases,\n\ni.e. \\(E_1, E_2, \\dots\\) is a partition of \\(E\\).\n\nIn the decision tree below, we partitioned events until we get events at the leaves each containing a single scenario\n\nWe call one individual scenario an outcome\nWe call the set of all outcomes the sample space, \\(S\\), and put it at the root of decision trees.\n\n\n\n\n\n\n\nflowchart TD\nS__and__X_0 -- 1.0 --&gt; S__and__X_0__and__Y1_false[\"Y1=false\"]\nS__and__X_2__and__Y1_true -- 1.0 --&gt; S__and__X_2__and__Y1_true__and__Y2_true[\"Y2=true\"]\nS -- 0.33 --&gt; S__and__X_0[\"X=0\"]\nS__and__X_1__and__Y1_true__and__Y2_true__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_true__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_true__and__Y2_false__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_true__and__Y4_false[\"Y4=false\"]\nS__and__X_0__and__Y1_false__and__Y2_false__and__Y3_false -- 1.0 --&gt; S__and__X_0__and__Y1_false__and__Y2_false__and__Y3_false__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_false__and__Y2_false__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_false__and__Y4_true[\"Y4=true\"]\nS -- 0.33 --&gt; S__and__X_1[\"X=1\"]\nS__and__X_2__and__Y1_true__and__Y2_true__and__Y3_true -- 1.0 --&gt; S__and__X_2__and__Y1_true__and__Y2_true__and__Y3_true__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_true__and__Y2_false__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_false__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_false__and__Y2_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_false[\"Y3=false\"]\nS__and__X_1__and__Y1_true__and__Y2_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_false[\"Y3=false\"]\nS__and__X_1__and__Y1_false__and__Y2_false__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_false__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_true__and__Y2_true__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_false__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_true__and__Y2_false__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_true__and__Y4_true[\"Y4=true\"]\nS__and__X_1 -- 0.5 --&gt; S__and__X_1__and__Y1_false[\"Y1=false\"]\nS__and__X_1__and__Y1_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false[\"Y2=false\"]\nS__and__X_1__and__Y1_false__and__Y2_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_true[\"Y3=true\"]\nS__and__X_1__and__Y1_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true[\"Y2=true\"]\nS__and__X_0__and__Y1_false -- 1.0 --&gt; S__and__X_0__and__Y1_false__and__Y2_false[\"Y2=false\"]\nS__and__X_1__and__Y1_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false[\"Y2=false\"]\nS__and__X_1__and__Y1_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true[\"Y2=true\"]\nS__and__X_2 -- 1.0 --&gt; S__and__X_2__and__Y1_true[\"Y1=true\"]\nS__and__X_1__and__Y1_false__and__Y2_true__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_true__and__Y4_false[\"Y4=false\"]\nS__and__X_1 -- 0.5 --&gt; S__and__X_1__and__Y1_true[\"Y1=true\"]\nS -- 0.33 --&gt; S__and__X_2[\"X=2\"]\nS__and__X_1__and__Y1_true__and__Y2_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_false[\"Y3=false\"]\nS__and__X_2__and__Y1_true__and__Y2_true -- 1.0 --&gt; S__and__X_2__and__Y1_true__and__Y2_true__and__Y3_true[\"Y3=true\"]\nS__and__X_1__and__Y1_false__and__Y2_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_true[\"Y3=true\"]\nS__and__X_1__and__Y1_false__and__Y2_true__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_false__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_true__and__Y2_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_true[\"Y3=true\"]\nS__and__X_1__and__Y1_true__and__Y2_true__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_true__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_false__and__Y2_true__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_false__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_true__and__Y2_true__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_false__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_false__and__Y2_false__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_true__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_true__and__Y2_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_true[\"Y3=true\"]\nS__and__X_1__and__Y1_true__and__Y2_false__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_false__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_false__and__Y2_false__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_true__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_false__and__Y2_true__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_true__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_false__and__Y2_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_false[\"Y3=false\"]\nS__and__X_0__and__Y1_false__and__Y2_false -- 1.0 --&gt; S__and__X_0__and__Y1_false__and__Y2_false__and__Y3_false[\"Y3=false\"]"
  },
  {
    "objectID": "w01_discrete_inference/topic05_decision_diagrams.html#nodes-and-events",
    "href": "w01_discrete_inference/topic05_decision_diagrams.html#nodes-and-events",
    "title": "Decision trees",
    "section": "Nodes and events",
    "text": "Nodes and events\nTo describe the event corresponding to a node \\(v\\) in the tree:\n\ntrace the path from the node \\(v\\) to the root\ntake the intersection of all node labels\n\nExample: find the node in the above tree corresponding to the event \\((X = 1) \\cap (Y_1 = 0)\\).\nProbability notation review:\n\n\\((X = 1) = \\{s \\in S : X(s) = 1\\}\\)\n\\((X = 1, Y_1 = 0) = (X = 1) \\cap (Y_1 = 0)\\)"
  },
  {
    "objectID": "w01_discrete_inference/topic05_decision_diagrams.html#edges-and-conditional-probabilities",
    "href": "w01_discrete_inference/topic05_decision_diagrams.html#edges-and-conditional-probabilities",
    "title": "Decision trees",
    "section": "Edges and conditional probabilities",
    "text": "Edges and conditional probabilities\nWhen there is an edge from events \\(E_1\\) to \\(E_2\\), we annotate it with \\(\\mathbb{P}(E_2 | E_1)\\).\nRecall: conditional probability of \\(E_2\\) given \\(E_1\\)\n\\[\\mathbb{P}(E_2 | E_1) = \\frac{\\mathbb{P}(E_1 \\cap E_2)}{\\mathbb{P}(E_1)}\\]\nExample:\n\ntake the edge from \\(E_1 = (X = 1)\\) to \\(E_2 = (X = 1, Y_1 = 0)\\). \\[\\mathbb{P}(E_2 | E_1) = \\frac{\\mathbb{P}(E_1 \\cap E_2)}{\\mathbb{P}(E_1)} = \\frac{\\mathbb{P}(E_2)}{\\mathbb{P}(E_1)} = \\mathbb{P}(Y_1 = 0 | X = 1)\\]\nTranslating \\(\\mathbb{P}(Y_1 = 0 | X = 1)\\) into words: “the probability that the first flip is ‘heads’ \\((Y_1 = 0)\\) given that you picked the standard coin \\((X = 1)\\).’’\nHence the edge from \\(E_1\\) to \\(E_2\\) is labelled \\(1/2\\)."
  },
  {
    "objectID": "drafts/ex02.html",
    "href": "drafts/ex02.html",
    "title": "Exercise 2: Bayesian inference on discrete spaces",
    "section": "",
    "text": "Caution\n\n\n\nPage under construction: information on this page may change."
  },
  {
    "objectID": "drafts/ex02.html#goals",
    "href": "drafts/ex02.html#goals",
    "title": "Exercise 2: Bayesian inference on discrete spaces",
    "section": "Goals",
    "text": "Goals\nA first contact with several Bayesian concepts. We use the same discrete model for all questions.\n\nNotions of priors, likelihood, posterior.\nPoint summary of posterior distribution: mean, median, mode.\nConstructing credible intervals.\nBasic decision theory.\nSequential update of posterior distribution."
  },
  {
    "objectID": "exercises/ex01.html",
    "href": "exercises/ex01.html",
    "title": "Exercise 1: discrete probabilistic inference",
    "section": "",
    "text": "Grading\n\n\n\nOur priority with the weekly exercises is to provide timely feedback and an incentive to stay on top of the material so that lectures can be more effective.\nWe will select one or more questions that will be graded in more detail. For the other question(s), we will use the following particiation-centric binary scheme:\n\n1 point if something reasonable was attempted,\n0 otherwise."
  },
  {
    "objectID": "exercises/ex01.html#goals",
    "href": "exercises/ex01.html#goals",
    "title": "Exercise 1: discrete probabilistic inference",
    "section": "Goals",
    "text": "Goals\n\nBring back to memory discrete probability (axioms, basic properties, conditioning, discrete Bayes rule).\nIntroduce forward discrete simulation.\nReview expectation and the law of large numbers."
  },
  {
    "objectID": "exercises/ex01.html#setup",
    "href": "exercises/ex01.html#setup",
    "title": "Exercise 1: discrete probabilistic inference",
    "section": "Setup",
    "text": "Setup\nThis exercise is centered around the following scenario:\n\nImagine a bag with 3 coins each with a different probability parameter \\(p\\)\nCoin \\(i\\in \\{0, 1, 2\\}\\) has bias \\(i/2\\)—in other words:\n\nFirst coin: bias is \\(0/2 = 0\\) (i.e. both sides are “tails”, \\(p = 0\\))\nSecond coin: bias is \\(1/2 = 0.5\\) (i.e. standard coin, \\(p = 1/2\\))\nThird coin: bias is \\(2/2 = 1\\) (i.e. both sides are “heads”, \\(p = 1\\))\n\n\n\n\n\n\nConsider the following two steps sampling process\n\nStep 1: pick one of the three coins, but do not look at it!\nStep 2: flip the coin 4 times\n\nMathematically, this probability model can be written as follows: \\[\n\\begin{align*}\nX &\\sim {\\mathrm{Unif}}\\{0, 1, 2\\} \\\\\nY_i | X &\\sim {\\mathrm{Bern}}(X/2)\n\\end{align*}\n\\tag{1}\\]"
  },
  {
    "objectID": "exercises/ex01.html#q.1-sampling-from-a-joint-distribution",
    "href": "exercises/ex01.html#q.1-sampling-from-a-joint-distribution",
    "title": "Exercise 1: discrete probabilistic inference",
    "section": "Q.1: sampling from a joint distribution",
    "text": "Q.1: sampling from a joint distribution\n\nCompute \\(\\mathbb{E}[(1+Y_1)^X]\\) mathematically (with a precise mathematical derivation).\nWrite an R function called forward_sample that samples (“simulates”) from the joint distribution of \\((X, Y_1, \\dots, Y_4)\\). As a general practice, fix the seed, and submit both the code and the output (here, a single sample).\nHow can your code and the law of large number be used to approximate \\(\\mathbb{E}[(1+Y_1)^X]\\)?\nCompare the approximation from your code with you answer in part 1.\n\n\n\n\n\n\n\nBig idea\n\n\n\nPart 4 of this question illustrates a big idea in this course:\nstrategies to validate inference, i.e. ensuring it is bug-free in both the code and in the math. In a nutshell, this is possible thanks to theory: we use results that provide two ways to do the same thing, and verifying they indeed agree."
  },
  {
    "objectID": "exercises/ex01.html#q.2-computing-a-conditional",
    "href": "exercises/ex01.html#q.2-computing-a-conditional",
    "title": "Exercise 1: discrete probabilistic inference",
    "section": "Q.2: computing a conditional",
    "text": "Q.2: computing a conditional\nSuppose now that you observe the outcome of the 4 coin flips, but not the type of coin that was picked. Say you observe: “heads”, “heads”, “heads”, “heads” = [0, 0, 0, 0]. Given that observation, what is the probability that you picked the standard coin (i.e., the one with \\(p = 1/2\\))?\n\nWrite mathematically: “Given you observe 4 heads, what is the probability that you picked the standard coin?”\nCompute the numerical value of the expression defined in part 1 (with a precise mathematical derivation)."
  },
  {
    "objectID": "exercises/ex01.html#q.3-non-uniform-prior-on-coin-types",
    "href": "exercises/ex01.html#q.3-non-uniform-prior-on-coin-types",
    "title": "Exercise 1: discrete probabilistic inference",
    "section": "Q.3: non uniform prior on coin types",
    "text": "Q.3: non uniform prior on coin types\nWe now modify the problem as follows: I stuffed the bag with 100 coins: 98 standard (fair) coins, 1 coin with only heads, and 1 coin with only tails. The rest is the same: pick one of the coins, flip it 4 times.\n\nWrite the joint distribution of this modified model. Use the \\(\\sim\\) notation as in Equation 1. Hint: use a Categorical distribution.\nCompute the probability that you picked one of the fair coins, given you see [0, 0, 0, 0]."
  },
  {
    "objectID": "exercises/ex01.html#q.4-a-first-posterior-inference-algorithm",
    "href": "exercises/ex01.html#q.4-a-first-posterior-inference-algorithm",
    "title": "Exercise 1: discrete probabilistic inference",
    "section": "Q.4: a first posterior inference algorithm",
    "text": "Q.4: a first posterior inference algorithm\nWe now generalize to having \\(K + 1\\) types of coins such that:\n\ncoin type \\(k \\in \\{0, 1, \\dots, K\\}\\) has bias \\(k/K\\)\nthe fraction of coins in the bag of type \\(k\\) is \\(\\rho_k\\).\n\nWe consider the same observation as before: “you observe 4 heads”. We want to find the conditional probability \\(\\pi_k\\) for all \\(k\\) that we picked coin type \\(k \\in \\{0, 1, \\dots, K\\}\\) from the bag given the observation.\n\nWrite an R function called posterior_given_four_heads taking as input a vector \\(\\rho = (\\rho_0, \\rho_1, \\dots, \\rho_K)\\) and returning \\(\\pi = (\\pi_0, \\pi_1, \\dots, \\pi_K)\\).\nTest your code by making sure you can recover the answer in Q. 3 as a special case. Report what values of \\(K\\) and \\(\\rho\\) you used.\nShow the output for \\(\\rho \\propto (1, 2, 3, \\dots, 10)\\). Here \\(\\propto\\) means “proportional to”; try to infer what it means in this context."
  },
  {
    "objectID": "exercises/ex01.html#q.5-generalizing-observations",
    "href": "exercises/ex01.html#q.5-generalizing-observations",
    "title": "Exercise 1: discrete probabilistic inference",
    "section": "Q.5: generalizing observations",
    "text": "Q.5: generalizing observations\nWe now generalize Q. 4 as follows: instead of observing 4 “heads” out of 4 observations, say we observe n_heads out of n_observations, where n_heads and n_observations will be additional arguments passed into a new R function.\n\nWrite the joint distribution of this modified model. Use the \\(\\sim\\) notation as in Equation 1. Hint: use a Binomial distribution.\nWrite an R function called posterior taking three input arguments in the following order: a vector \\(\\rho\\) as in Q. 4, as well as two integers, n_heads and n_observations.\nTest your code by making sure you can recover the answer in Q. 3 as a special case.\nShow the output for \\(\\rho \\propto (1, 2, 3, \\dots, 10)\\) and n_heads = 2 and n_observations = 10."
  },
  {
    "objectID": "w00_intro/topic04_examples.html",
    "href": "w00_intro/topic04_examples.html",
    "title": "Examples",
    "section": "",
    "text": "Origins of life/cancer/language and characterization of their respective evolutionary processes\nModelling high-throughput genomics data (single-cell sequencing, CRISPR-CAS9, ultra-deep, expression)\nBlack-hole imaging\n“Classical” tasks: classification, regression, clustering, etc\nA/B testing and Bayesian optimization\nDetermining fate of the universe from cosmic microwave background"
  },
  {
    "objectID": "w00_intro/topic04_examples.html#some-examples-of-bayesian-inference",
    "href": "w00_intro/topic04_examples.html#some-examples-of-bayesian-inference",
    "title": "Examples",
    "section": "",
    "text": "Origins of life/cancer/language and characterization of their respective evolutionary processes\nModelling high-throughput genomics data (single-cell sequencing, CRISPR-CAS9, ultra-deep, expression)\nBlack-hole imaging\n“Classical” tasks: classification, regression, clustering, etc\nA/B testing and Bayesian optimization\nDetermining fate of the universe from cosmic microwave background"
  },
  {
    "objectID": "w00_intro/topic04_examples.html#to-read-more-on-example-applications",
    "href": "w00_intro/topic04_examples.html#to-read-more-on-example-applications",
    "title": "Examples",
    "section": "To read more on example applications",
    "text": "To read more on example applications\nFor an entertaining popular science book on various applications of Bayesian statistics, have a look at The Theory That Would Not Die. Sharon Bertsch McGrayne. PDF available via UBC library."
  },
  {
    "objectID": "w00_intro/topic01_why.html",
    "href": "w00_intro/topic01_why.html",
    "title": "Why?",
    "section": "",
    "text": "Address most data analysis issues (missing data, non-standard data types, non-iid, weird loss functions, adding expert knowledge, …)\n\nBayesian analysis: address those in a (semi) automated fashion / principled framework (“reductionist”)\n\nReductionism can be bad or good (main con of reductionism is computational)\n\nFrequentist statistics: every problem is a new problem\n\nImplementation complexity\n\nEfficient in analyst’s time (thanks to PPLs)\nHarder to scale computationally\n\\(\\Longrightarrow\\) shines on small data problems (there a much more of those than the “big data” hype would like you to think)\n\nStatistical properties\n\nOptimal if the model is well-specified\nSub-optimal in certain cases when the model is mis-specified\n\nThankfully the modelling flexibility makes it easier to build better models\nImportant to make model checks"
  },
  {
    "objectID": "w00_intro/topic01_why.html#bayesian-analysis-pros-and-cons",
    "href": "w00_intro/topic01_why.html#bayesian-analysis-pros-and-cons",
    "title": "Why?",
    "section": "",
    "text": "Address most data analysis issues (missing data, non-standard data types, non-iid, weird loss functions, adding expert knowledge, …)\n\nBayesian analysis: address those in a (semi) automated fashion / principled framework (“reductionist”)\n\nReductionism can be bad or good (main con of reductionism is computational)\n\nFrequentist statistics: every problem is a new problem\n\nImplementation complexity\n\nEfficient in analyst’s time (thanks to PPLs)\nHarder to scale computationally\n\\(\\Longrightarrow\\) shines on small data problems (there a much more of those than the “big data” hype would like you to think)\n\nStatistical properties\n\nOptimal if the model is well-specified\nSub-optimal in certain cases when the model is mis-specified\n\nThankfully the modelling flexibility makes it easier to build better models\nImportant to make model checks"
  },
  {
    "objectID": "w00_intro/topic01_why.html#week-2-example",
    "href": "w00_intro/topic01_why.html#week-2-example",
    "title": "Why?",
    "section": "Week 2 example",
    "text": "Week 2 example\n\nWould you rather get strapped to…\n\n“shiny rocket”: 1 success, 0 failures\n“rugged rocket”: 98 successes, 2 failures"
  },
  {
    "objectID": "w00_intro/topic01_why.html#paradox",
    "href": "w00_intro/topic01_why.html#paradox",
    "title": "Why?",
    "section": "Paradox?",
    "text": "Paradox?\n\nMaximum likelihood point estimates:\n\n“shiny rocket”: 100% success rate (1 success, 0 failures)\n“rugged rocket”: 98% success rate (98 successes, 2 failures)\n\nWhat is missing?"
  },
  {
    "objectID": "w00_intro/topic01_why.html#uncertainty-estimates",
    "href": "w00_intro/topic01_why.html#uncertainty-estimates",
    "title": "Why?",
    "section": "Uncertainty estimates",
    "text": "Uncertainty estimates\n\nTake-home message:\n\nPoint estimates are often insufficient, and can be very dangerous\nWe want some measure of uncertainty\n\nBayesian inference provides one way to build uncertainty measures\n\nBayesian measures of uncertainty we will describe: credible intervals\n\nAlternatives exist:\n\nConfidence intervals, from frequentist statistics\n“End product” looks similar, but very different in interpretation and construction"
  },
  {
    "objectID": "w00_intro/topic01_why.html#uncertainty-will-not-go-away",
    "href": "w00_intro/topic01_why.html#uncertainty-will-not-go-away",
    "title": "Why?",
    "section": "Uncertainty will not go away",
    "text": "Uncertainty will not go away\n\n\n\n\nJust collect more data??\n\nJust launch more rockets and wait? Collecting more data might be too costly/dangerous/unethical.\nIn some cases the data is just “gone”, i.e. we will never be able to collect more after a point (e.g.: phylogenetic tree inference)"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Final project",
    "section": "",
    "text": "Caution\n\n\n\nPage under construction: information on this page may change."
  },
  {
    "objectID": "project.html#overview",
    "href": "project.html#overview",
    "title": "Final project",
    "section": "Overview",
    "text": "Overview\nThe course project involves independent work on a topic selected from a menu of project themes. These projects leave freedom for creativity within constraints designed for pedagogical and fair evaluation."
  },
  {
    "objectID": "project.html#logistics",
    "href": "project.html#logistics",
    "title": "Final project",
    "section": "Logistics",
    "text": "Logistics\n\nTeams of maximum 2 people are encouraged, in which case you should outline the final report who did what. Expectation will grow linearly in the group size.\nSubmit on canvas, a pdf document of maximum 5 pages/person (i.e. max length is 10 for groups of 2 people), excluding references and appendices. The appendix can have arbitrary length but may not be read in detail during grading."
  },
  {
    "objectID": "project.html#project-timeline",
    "href": "project.html#project-timeline",
    "title": "Final project",
    "section": "Project timeline",
    "text": "Project timeline\n\nFriday, Mar 8: prepare proposal, freeze teams, each team sends a 1 page abstract submitted on Canvas.\nFriday, April 19: due date for the reports."
  },
  {
    "objectID": "project.html#core-guideline",
    "href": "project.html#core-guideline",
    "title": "Final project",
    "section": "Core guideline",
    "text": "Core guideline\nEvery project should contain a component where Bayesian inference is applied to a real problem and a real dataset.\nIn addition, each team should pick one of the “project themes” listed below, exploring topics building and going beyond what we will cover during the course. If two project proposals are too similar, I reserve the right to assign changes to one of the projects (typically the one with the last proposal submission date).\nAfter selection of a project theme, you should start hunting for a few real-world candidate datasets appropriate for the selected theme. Two potential datasets should be listed in the proposal. The final report should analyze at least one real dataset.\nSome resources:\n\nVanderbilt Biostatistics datasets\nTidyTuesday\nInter-university Consortium for Political and Social Research\nWHO mortality data\nThe World Bank Data\nMore…\n\nYou should not pick a dataset that has already been analyzed using the same approach as you. Provide references for the closest analyses of the same data and explain how they differ from yours."
  },
  {
    "objectID": "project.html#menu-of-project-themes",
    "href": "project.html#menu-of-project-themes",
    "title": "Final project",
    "section": "Menu of project themes",
    "text": "Menu of project themes\nRoughly in increasing order of complexity. During grading I will take into account the complexity of the selected project theme. For example, if considerable coding is required in the project, it may be possible to use only synthetic data instead of real data (consult me and document this request in the proposal).\n\nGoing further on… (more details can be provided upon request)\n\nBayesian regression and classification (e.g. sparsity, hierarchical structure, etc)\nmodel selection (advanced computational approaches to Bayes factors, alternatives to Bayes factors, comparisons)\ntime series and state-space models\nspatial models\ncross-effect models\nBayesian non-parametric models\ndeep generative models\ntopics models\nvariational inference\n\nA careful and scientific comparison of a Bayesian estimator with another one, either Bayesian or non-Bayesian. Review the literature on both sides so as to be fair and critical to both sides of the comparison. State and defend the criteria you use. Consider calibration and M-open setups. Examples:\n\nBayesian vs frequentist… regression/classification, feature selection, density estimation, survival analysis, …\nIs there some structure that can be exploited (e.g. informed by the data types for the covariates/features, groups of related features i.e. feature templates, hierarchical approaches, etc), to get better Bayesian methods on these generic classes of inference problems?\n\nA Bayesian inference method over a non-standard data type. Acquire or write an efficient posterior inference method, either using a PPL or from scratch. Develop a novel Bayes estimator and implement it. Benchmark the Bayes estimator on synthetic data, comparing the performance with a naive baseline such as MAP. Examples:\n\nTypes of graphs such as matchings\nPhylogenetic trees or networks\nMultiple sequence alignments\nClustering or feature matrices\n\nCreate a twist on an existing MCMC sampling algorithm, or a novel one. Show it is invariant with respect to the distribution of interest. Benchmark the performance of the method against one baseline using best practices."
  },
  {
    "objectID": "project.html#rubric-for-the-project-proposal",
    "href": "project.html#rubric-for-the-project-proposal",
    "title": "Final project",
    "section": "Rubric for the project proposal",
    "text": "Rubric for the project proposal\n\nBasic requirements\n\nTeam is identified.\nThe proposal identifies which of the project themes it will address.\n\nTwo real-world candidate datasets appropriate for the selected theme are clearly described (e.g. a URL, showing the structure or head of a dataframe).\nA short summary of potential approaches to tackle the project theme.\nIf it is a team project, the proposal contains a short plan for ensuring the two team members will contribute roughly equally.\n\nAs long as the team submits a reasonable project proposal, I will give full grade (5/5) (along with some feedback). Late submission within 2 days will receive 4/5, and 0/5 after the grace period, but I can still provide feedback past the grace period but no later than April 1st. Details of the project can change after submission of the proposals. Larger changes are allowed but with my permission, so they should not be discussed at the last minute, i.e. before April 1st ideally and certainly before the last lecture.\nTotal: 5%"
  },
  {
    "objectID": "project.html#rubric-for-the-final-report",
    "href": "project.html#rubric-for-the-final-report",
    "title": "Final project",
    "section": "Rubric for the final report",
    "text": "Rubric for the final report\n\nBasic requirements (5%)\n\nThe report fits within the prescribed page limits.\nThe report follows best practices of technical writing.\nIf it is a team project, a short paragraph clearly explains and quantifies the contributions of each team member.\n\nProblem formulation. (15%) The report clearly describes:\n\na real-world inference task/problem,\nsuccinct but sufficient context (e.g. biological terminology) needed to understand the problem,\nthe key modelling/methodological/challenge, clearly associating it with one of the items under “menu of project themes” above.\n\nThe report contains a literature review: (10%) relevant literature is cited and properly summarized.\nData analysis (40%)\n\nA Bayesian model is precisely described (e.g. using the .. ~ .. notation)\nImplementation code in the appendix (e.g. using Stan)\nPrior choice is motivated. If appropriate, several choices are compared or sensitivity analysis is performed.\nCritical evaluation of the posterior approximation. An appropriate combination of diagnostics, synthetic datasets and other validation strategies.\n\nProject theme: methodological/theoretical aspect of the project (20%)\n\nIs the approach sound?\nCreative?\n\nDiscussion (5%)\n\nDoes the report describe key limitations?\n\n\nTotal: 95%"
  }
]