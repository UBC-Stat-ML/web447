[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Date\nTopics\nReadings\n\n\n\n\nTue, Jan 6\nOverview. Probabilistic inference on discrete spaces.\nBDA 1.1-1.10\n\n\nTh, Jan 8\n\n\n\n\nTue, Jan 13\nA tour of Bayesian inference (on discrete spaces).\nBDA 2.1-2.5\n\n\nTh, Jan 15\n\n\n\n\nTue, Jan 20\nUniversal probabilistic inference via importance sampling.\nBDA 9.1-9.5 (going further on Decision Theory)\n\n\nTh, Jan 22\n\n\n\n\nTue, Jan 27\nBayesian classification, regression, GLMs, and beyond.\nBDA 14.1-14.5\n\n\nTh, Jan 29\n\n\n\n\nTue, Feb 3\nProperties of Bayesian models.\nBDA 4.1-4.5\n\n\nTh, Feb 5\n\n\n\n\nTue, Feb 10\nHierarchical models.\nBDA 5.1-5.7\n\n\nTh, Feb 12\n\n\n\n\nTue, Feb 18\nReading week.\nBDA 16.1-16.7\n\n\nTh, Feb 19\nReading week.\n\n\n\nTue, Feb 24\nQuiz 1.\n\n\n\nTh, Feb 26\nQuiz debrief. Stan basics. Project logistics.\n\n\n\nTue, Mar 3\nMCMC user guide (via Stan).\nBDA 11.1-11.6\n\n\nTh, Mar 5\n\n\n\n\nTue, Mar 10\nBayesian workflow.\nBDA 6.1-6.5\n\n\nTh, Mar 12\n\n\n\n\nFri, Mar 13\nProject proposal due.\n\n\n\nTue, Mar 17\nModelling techniques (selected from prior design, mixtures, imputation, complex data collection).\nBDA 8.1-8.7\n\n\nTh, Mar 19\n\n\n\n\nTue, Mar 24\nQuiz 2.\n\n\n\nTh, Mar 26\nMCMC developer guide.\n\n\n\nTue, Mar 31\nMCMC developer guide.\nBDA 12.1-12.6\n\n\nTh, Apr 2\n\n\n\n\nTue, Apr 7\nMore inference techniques.\nBDA 13.7\n\n\nTh, Apr 9\n\n\n\n\nSat, Apr 18\nFinal project due.",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "w10_modelling/topic03_custom_bricks.html",
    "href": "w10_modelling/topic03_custom_bricks.html",
    "title": "Custom bricks",
    "section": "",
    "text": "What to do when you need a distribution not in the Stan built-in library.\nUnderstanding why Stan (and most MCMC methods) represents the density in log-scale.\n\n\n\n\nEven though Stan has a long list of built-in distribution, you will eventually run into a situation where you need a distribution not in the list.\nWhile learning how to do so, we will need to look more deeply at how Stan works under the hood.",
    "crumbs": [
      "Modelling techniques",
      "Custom bricks"
    ]
  },
  {
    "objectID": "w10_modelling/topic03_custom_bricks.html#outline",
    "href": "w10_modelling/topic03_custom_bricks.html#outline",
    "title": "Custom bricks",
    "section": "",
    "text": "What to do when you need a distribution not in the Stan built-in library.\nUnderstanding why Stan (and most MCMC methods) represents the density in log-scale.\n\n\n\n\nEven though Stan has a long list of built-in distribution, you will eventually run into a situation where you need a distribution not in the list.\nWhile learning how to do so, we will need to look more deeply at how Stan works under the hood.",
    "crumbs": [
      "Modelling techniques",
      "Custom bricks"
    ]
  },
  {
    "objectID": "w10_modelling/topic03_custom_bricks.html#example",
    "href": "w10_modelling/topic03_custom_bricks.html#example",
    "title": "Custom bricks",
    "section": "Example",
    "text": "Example\nWe will show how to implement the Kumaraswamy distribution in Stan. As of March 2024, it is not in Stan’s built-in library.\n\n\n\nThe Kumaraswamy distribution has density: \\[f(x) = ab x^{a-1} (1 - x^a)^{b-1},\\] where \\(x \\in (0, 1)\\) and \\(a, b\\) are positive parameters.",
    "crumbs": [
      "Modelling techniques",
      "Custom bricks"
    ]
  },
  {
    "objectID": "w10_modelling/topic03_custom_bricks.html#executive-version",
    "href": "w10_modelling/topic03_custom_bricks.html#executive-version",
    "title": "Custom bricks",
    "section": "Executive version",
    "text": "Executive version\n\nIn the model block:\n\nUse target += [logarithm (base e) of the log density]\nApplying this to our Kumaraswamy example: \\[\\log(f(x)) = \\log(a) + \\log(b) + (a-1) \\log(x) + (b-1) \\log(1 - x^a).\\]\n\nIn the parameters block:\n\nMake sure you enforce the support (here \\(x \\in (0, 1)\\))\n\n\n\nsuppressPackageStartupMessages(require(rstan))\n\n\ndata {\n  real&lt;lower=0&gt; a;\n  real&lt;lower=0&gt; b;  \n}\n\nparameters {\n1  real&lt;lower=0, upper=1&gt; x;\n}\n\nmodel {\n2  target += log(a) + log(b) + (a-1) * log(x) + (b-1) * log1p(-x^a);\n                                                    // ^ log1p(z) = log(1+z)\n}\n\n\n1\n\nSupport enforced.\n\n2\n\nContribution of the Kumaraswamy log density added the log joint density.\n\n\n\n\n\nfit = sampling(\n  Kumaraswamy,\n  seed = 1,\n  refresh = 0,\n  data = list(a = 2, b = 5),       \n  iter = 10000                   \n)\n\nChecking that the histogram roughly matches the one from the wikipedia article (see top of page Figure, purple line).\n\nsuppressPackageStartupMessages(require(bayesplot))\nsuppressPackageStartupMessages(require(ggplot2))\nmcmc_hist(fit, pars = c(\"x\")) + theme_minimal()",
    "crumbs": [
      "Modelling techniques",
      "Custom bricks"
    ]
  },
  {
    "objectID": "w10_modelling/topic03_custom_bricks.html#explanation",
    "href": "w10_modelling/topic03_custom_bricks.html#explanation",
    "title": "Custom bricks",
    "section": "Explanation",
    "text": "Explanation\n\nLog-scale computation\nQuestion: why is Stan computing the logarithm of \\(\\gamma(x)\\)?\nKey fact: Stan, like most numerical methods, use double-precision floating point approximation of real numbers.\n\nSuppose you have 500 observations.\nEach have a likelihood of about \\(1/10\\).\nWhat is the joint likelihood?\n\n\nlikelihood = 1.0\nfor (i in 1:500) {\n  likelihood = likelihood * (1/10)\n}\nlikelihood\n\n[1] 0\n\n\nQuestion: Why R returns zero?\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nBecause \\((1/10)^{500}\\) is smaller than the smallest positive number representeable in double precision (\\(\\approx 10^{-300}\\))\n\nSo it gets rounded to zero.\nThis is called an underflow\n\n\n\n\n\nLog-scale computation: let us say we store instead the log-likelihood:\n\nloglikelihood = log(1.0)\nfor (i in 1:500) {\n  loglikelihood = loglikelihood + log(1/10)\n}\nloglikelihood\n\n[1] -1151.293\n\n500*log(1/10)\n\n[1] -1151.293\n\n\n\nWe have avoided the underflow issue!\nThat’s why a Stan program computes \\(\\log \\gamma(x)\\) instead of \\(\\gamma(x)\\).",
    "crumbs": [
      "Modelling techniques",
      "Custom bricks"
    ]
  },
  {
    "objectID": "w10_modelling/topic03_custom_bricks.html#encapsulation-into-a-function",
    "href": "w10_modelling/topic03_custom_bricks.html#encapsulation-into-a-function",
    "title": "Custom bricks",
    "section": "Encapsulation into a function",
    "text": "Encapsulation into a function\nIf you are using a custom density for several variables, encapsulate it into a function:\n\n1functions {\n2  real Kumaraswamy_lpdf(real x, real a, real b) {\n    return log(a) + log(b) + (a-1) * log(x) + (b-1) * log1p(-x^a);\n  }\n}\n\ndata {\n  real&lt;lower=0&gt; a;\n  real&lt;lower=0&gt; b;  \n}\n\nparameters {\n  real&lt;lower=0, upper=1&gt; x; \n}\n\nmodel {\n3  x ~ Kumaraswamy(a, b);\n}\n\n\n1\n\nThe function block allows you to create custom functions. See the Stan documentation for more details.\n\n2\n\nFor Stan to pick up your function in its ~ syntax, you must use the special suffix _lpdf for your function name, which stands for “log probability density function.”\n\n3\n\nThen you can use your distribution name using ~ as usual (notice we do not include the _lpdf when we do such call). From this it is apparent that Stan’s ~ notation is just a shortcut for target += distributionName_lpdf(...).",
    "crumbs": [
      "Modelling techniques",
      "Custom bricks"
    ]
  },
  {
    "objectID": "w10_modelling/topic01_intro.html#this-weeks-outline",
    "href": "w10_modelling/topic01_intro.html#this-weeks-outline",
    "title": "Bayesian Lego",
    "section": "This week’s outline",
    "text": "This week’s outline\n\nThe success of Bayesian methods and Lego Blocks can be traced to the same basic reasons:\n\nA varied set of basic building blocks.\nThe ability to combine them freely.\n\nThis week, we introduce:\n\nmore examples of basic building blocks (i.e., distributions),\ncreating your own building blocks (i.e., custom distributions),\nmore examples of using simple building blocks to construct complex things (mixtures).",
    "crumbs": [
      "Modelling techniques",
      "Bayesian Lego"
    ]
  },
  {
    "objectID": "w10_modelling/topic05_collection.html",
    "href": "w10_modelling/topic05_collection.html",
    "title": "Data collection mechanisms",
    "section": "",
    "text": "Generalizing the censoring problem to other data collection mechanisms.\nExamples of other data collection mechanisms.\n\n\n\n\nRather than memorizing several data collection mechanisms, it is more important to recognize that it is simply a special (but important) example of probabilistic modelling and the first step of our Bayesian recipe.",
    "crumbs": [
      "Modelling techniques",
      "Data collection mechanisms"
    ]
  },
  {
    "objectID": "w10_modelling/topic05_collection.html#outline",
    "href": "w10_modelling/topic05_collection.html#outline",
    "title": "Data collection mechanisms",
    "section": "",
    "text": "Generalizing the censoring problem to other data collection mechanisms.\nExamples of other data collection mechanisms.\n\n\n\n\nRather than memorizing several data collection mechanisms, it is more important to recognize that it is simply a special (but important) example of probabilistic modelling and the first step of our Bayesian recipe.",
    "crumbs": [
      "Modelling techniques",
      "Data collection mechanisms"
    ]
  },
  {
    "objectID": "w10_modelling/topic05_collection.html#examples",
    "href": "w10_modelling/topic05_collection.html#examples",
    "title": "Data collection mechanisms",
    "section": "Examples",
    "text": "Examples\n\nTruncation\n\nIn censoring, we knew how many \\(H_i\\)’s were above the detection limit.\nIn truncation, a different setup, we now have even less information:\n\nwe only observe the \\(H_i\\)’s that are below the limit…\n…we don’t know how many were above the limit.\n\nMathematically, when the \\(H_i\\) have a continuous distribution this can be modelled as:\n\n\\[\\begin{align*}\nX &\\sim \\text{prior}() \\\\\nN &\\sim \\text{DiscreteDistribution}() \\\\\nH_1, \\dots, H_N &\\sim \\text{likelihood}(X) \\\\\nI_i &= \\mathbb{1}[H_i \\le L] \\\\\nY &= \\{ H_i : I_i = 1 \\}.\n\\end{align*}\\]\n\nHere \\(I_i\\) is an “inclusion indicator”.\nBayesian analysis will be based on \\(X | Y\\)\n\nExample: CRISPR-Cas9 unique molecular identifier (UMI) family size. “Families of cells” that left zero progenies are not observed!\n\n\n\nNon-ignorable missingness\nTruncation can be generalized as follows:\n\nInstead of a deterministic criterion based on \\(H_i\\) to decide if to include in the set of observations or not,\nmake that decision based on some probability model \\(p\\) that could depend on \\(h_i\\) and \\(x\\), \\(p(x, h_i) \\in [0, 1]\\):\n\n\\[\\begin{align*}\nX &\\sim \\text{prior}() \\\\\nN &\\sim \\text{DiscreteDistribution}() \\\\\nH_1, \\dots, H_N &\\sim \\text{likelihood}(X) \\\\\nI_i &\\sim {\\mathrm{Bern}}(p(X, H_i)) \\\\\nY &= \\{ H_i : I_i = 1 \\}.\n\\end{align*}\\]\nQuestion: how would you set \\(p(x, h)\\) to recover truncation as a special case of non-ignorable missingness?\n\n\nAdditional readings\nSee this week’s readings, Chapter 8 of Gelman et al., 2013",
    "crumbs": [
      "Modelling techniques",
      "Data collection mechanisms"
    ]
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Final project",
    "section": "",
    "text": "Caution\n\n\n\nPage under construction: information on this page may change.",
    "crumbs": [
      "Final project"
    ]
  },
  {
    "objectID": "project.html#overview",
    "href": "project.html#overview",
    "title": "Final project",
    "section": "Overview",
    "text": "Overview\nThe course project involves independent work on topics selected from a menu of project themes. These projects leave freedom for creativity within constraints designed for pedagogical and fair evaluation.",
    "crumbs": [
      "Final project"
    ]
  },
  {
    "objectID": "project.html#logistics",
    "href": "project.html#logistics",
    "title": "Final project",
    "section": "Logistics",
    "text": "Logistics\n\nTeams of maximum 2-3 people are strongly encouraged (we will provide help forming teams)\nYou should outline in the final report who did what\nSubmit on canvas, a pdf document of maximum 4 pages for a 1 person project, 6 for a 2-person project, 8 for a 3-person project, excluding references and appendices. The appendix can have arbitrary length but may not be read in detail during grading.\nBoth proposal and submitted project should contain a link to a public git repository containing the source code for generating the report and figures. For team projects, make sure each team member commit their change using their account to document the contributions of each team member.",
    "crumbs": [
      "Final project"
    ]
  },
  {
    "objectID": "project.html#project-timeline",
    "href": "project.html#project-timeline",
    "title": "Final project",
    "section": "Project timeline",
    "text": "Project timeline\nSee schedule.",
    "crumbs": [
      "Final project"
    ]
  },
  {
    "objectID": "project.html#core-guideline",
    "href": "project.html#core-guideline",
    "title": "Final project",
    "section": "Core guideline",
    "text": "Core guideline\nEvery project should contain a component where Bayesian inference is applied to a real problem and a real dataset. This is important: unless you have not received explicit permission in the proposal phase to deviate from this you may be severely penalized if you do not do this!\nMore detailed structure and rubric will be posted soon.",
    "crumbs": [
      "Final project"
    ]
  },
  {
    "objectID": "w00_intro/topic02_what.html",
    "href": "w00_intro/topic02_what.html",
    "title": "What?",
    "section": "",
    "text": "MAP estimators (maximum a posteriori)\nposterior means\nBayes rule\nmodels where some unknown quantities are treated as random\nnone of the above\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nAll these popular answers are misleading and/or very incomplete:\n\nMAP estimators (maximum a posteriori)\n\nMAP is seldom used by expert Bayesians (mode is misleading in high dimensions)\n\nposterior means\n\nthe posterior mean is often undefined (e.g. Bayesian analysis over combinatorial objects such as graphs)\n\nBayes rule\n\nBayes rule is intractable in most practical situations (we use MCMC/variational methods)\n\nmodels where some unknown quantities are treated as random\n\ntrue for Bayesian models, but also for many non-Bayesian models, e.g., random effect models\n\n\nSo… what is Bayesian Analysis?\n\n\nBayesian Analysis: statistical discipline centered around the use of Bayes estimators.\nBayes estimators: for data \\(Y\\), unobserved \\(X\\), loss \\(L\\), and possible actions \\(A\\), the Bayes estimator is defined as:\n\\[\\operatorname{arg\\,min}\\{ \\mathbb{E}[L(a, X) | Y] : a \\in A \\}\\]\nNote: you are not expected to understand this equation at this point!\n\n\n\nThe primary objective of this course is to understand Bayes estimators:\n\nWhy they are so powerful.\nTheir limitations (model misspecification, computational challenges).\nImportant special cases (posterior means, credible intervals, MAP).\nHow to do it in practice\n\nhow to build models\nhow to approximate conditional expectations.",
    "crumbs": [
      "Introduction",
      "What?"
    ]
  },
  {
    "objectID": "w00_intro/topic02_what.html#poll-what-characterizes-bayesian-analysis",
    "href": "w00_intro/topic02_what.html#poll-what-characterizes-bayesian-analysis",
    "title": "What?",
    "section": "",
    "text": "MAP estimators (maximum a posteriori)\nposterior means\nBayes rule\nmodels where some unknown quantities are treated as random\nnone of the above\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nAll these popular answers are misleading and/or very incomplete:\n\nMAP estimators (maximum a posteriori)\n\nMAP is seldom used by expert Bayesians (mode is misleading in high dimensions)\n\nposterior means\n\nthe posterior mean is often undefined (e.g. Bayesian analysis over combinatorial objects such as graphs)\n\nBayes rule\n\nBayes rule is intractable in most practical situations (we use MCMC/variational methods)\n\nmodels where some unknown quantities are treated as random\n\ntrue for Bayesian models, but also for many non-Bayesian models, e.g., random effect models\n\n\nSo… what is Bayesian Analysis?\n\n\nBayesian Analysis: statistical discipline centered around the use of Bayes estimators.\nBayes estimators: for data \\(Y\\), unobserved \\(X\\), loss \\(L\\), and possible actions \\(A\\), the Bayes estimator is defined as:\n\\[\\operatorname{arg\\,min}\\{ \\mathbb{E}[L(a, X) | Y] : a \\in A \\}\\]\nNote: you are not expected to understand this equation at this point!\n\n\n\nThe primary objective of this course is to understand Bayes estimators:\n\nWhy they are so powerful.\nTheir limitations (model misspecification, computational challenges).\nImportant special cases (posterior means, credible intervals, MAP).\nHow to do it in practice\n\nhow to build models\nhow to approximate conditional expectations.",
    "crumbs": [
      "Introduction",
      "What?"
    ]
  },
  {
    "objectID": "w00_intro/topic03_high_level.html",
    "href": "w00_intro/topic03_high_level.html",
    "title": "High-level picture",
    "section": "",
    "text": "Construct a probability model including\n\nrandom variables for what we will measure/observe\nrandom variables for the unknown quantities\n\nthose we are interested in (“parameters”, “predictions”)\nothers that just help us formulate the problem (“nuisance”, “random effects”).\n\n\nCompute the posterior distribution (condition on the data)\nUse the posterior distribution to (decision theory):\n\nmake prediction (point estimate)\nestimate uncertainty (credible intervals)\nmake a decision",
    "crumbs": [
      "Introduction",
      "High-level picture"
    ]
  },
  {
    "objectID": "w00_intro/topic03_high_level.html#bayesian-recipe-high-level-picture",
    "href": "w00_intro/topic03_high_level.html#bayesian-recipe-high-level-picture",
    "title": "High-level picture",
    "section": "",
    "text": "Construct a probability model including\n\nrandom variables for what we will measure/observe\nrandom variables for the unknown quantities\n\nthose we are interested in (“parameters”, “predictions”)\nothers that just help us formulate the problem (“nuisance”, “random effects”).\n\n\nCompute the posterior distribution (condition on the data)\nUse the posterior distribution to (decision theory):\n\nmake prediction (point estimate)\nestimate uncertainty (credible intervals)\nmake a decision",
    "crumbs": [
      "Introduction",
      "High-level picture"
    ]
  },
  {
    "objectID": "w00_intro/topic03_high_level.html#plan",
    "href": "w00_intro/topic03_high_level.html#plan",
    "title": "High-level picture",
    "section": "Plan",
    "text": "Plan\n\nFirst week: probability essentials (foundations for steps 1 and 2 of the Bayesian Recipe)\nSecond week: steps 1, 2, 3 for one specific discrete probability models\nThird week and beyond: step 1, 2, 3 for arbitrary models",
    "crumbs": [
      "Introduction",
      "High-level picture"
    ]
  },
  {
    "objectID": "w00_intro/topic03_high_level.html#first-step-of-the-recipe-constructing-a-probability-model",
    "href": "w00_intro/topic03_high_level.html#first-step-of-the-recipe-constructing-a-probability-model",
    "title": "High-level picture",
    "section": "First step of the Recipe: “constructing a probability model”",
    "text": "First step of the Recipe: “constructing a probability model”\n\nWhat is a model?\nWhat is a probability model?\nExample (week 2): building a probability model for the rocket launch problem.",
    "crumbs": [
      "Introduction",
      "High-level picture"
    ]
  },
  {
    "objectID": "w00_intro/topic03_high_level.html#what-is-a-model",
    "href": "w00_intro/topic03_high_level.html#what-is-a-model",
    "title": "High-level picture",
    "section": "What is a model?",
    "text": "What is a model?\n(Scientific) model: A simplification of reality amenable to mathematical investigation.\n\\[\\text{Reality} \\xrightarrow{\\text{Art + Scientific method}} \\text{Model} \\xrightarrow{\\text{Mathematics}} \\text{Prediction}\\]\n\nIn this course “mathematics” will be Bayesian analysis/probability theory.\nBayesian analysis/probability theory assume a model as starting point.\n\nTo create a first model is a bit of an art. It comes with data analysis experience.\nThen after we start with an initial model we can improve it by checking predictions against reality.",
    "crumbs": [
      "Introduction",
      "High-level picture"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus: STAT405 Bayesian Statistics",
    "section": "",
    "text": "Bayesian inference is a flexible and powerful approach to modeling reality, making optimal predictions from data, and quantifying uncertainty in a coherent manner. Thanks to their versatility, Bayesian methods are now widely used in virtually all fields of science, engineering, and beyond.\nIn STAT 405, you will:\n\ndesign probabilistic models to approach real-world inferential problems;\nperform inference using Bayesian modelling languages;\ncritically assess, debug, and iteratively improve Bayesian workflows;\ndevelop and analyze custom posterior approximation machinery.\n\nNote: this course was called 447C in previous years.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Syllabus: STAT405 Bayesian Statistics",
    "section": "",
    "text": "Bayesian inference is a flexible and powerful approach to modeling reality, making optimal predictions from data, and quantifying uncertainty in a coherent manner. Thanks to their versatility, Bayesian methods are now widely used in virtually all fields of science, engineering, and beyond.\nIn STAT 405, you will:\n\ndesign probabilistic models to approach real-world inferential problems;\nperform inference using Bayesian modelling languages;\ncritically assess, debug, and iteratively improve Bayesian workflows;\ndevelop and analyze custom posterior approximation machinery.\n\nNote: this course was called 447C in previous years.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#lecture-time-and-place",
    "href": "syllabus.html#lecture-time-and-place",
    "title": "Syllabus: STAT405 Bayesian Statistics",
    "section": "Lecture time and place",
    "text": "Lecture time and place\nLecture dates: Tuesday and Thursday, 9:30-11:00. See Canvas for location. Detailed schedule",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#teaching-team",
    "href": "syllabus.html#teaching-team",
    "title": "Syllabus: STAT405 Bayesian Statistics",
    "section": "Teaching team",
    "text": "Teaching team\n\nAlexandre Bouchard-Côté (Instructor)\nCaden Hewlett (TA)\nJunsong Tang (TA)",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#prerequisite",
    "href": "syllabus.html#prerequisite",
    "title": "Syllabus: STAT405 Bayesian Statistics",
    "section": "Prerequisite",
    "text": "Prerequisite\n\nProbability: One of MATH 302, STAT 302, or MATH 418\nInference: either STAT 305 or STAT 460\nComputing: we will use R in the homework and during lectures. If you know another programming language but not R, you can still take this course but be prepared to spend a bit of extra time to get familiar with the R syntax. We will have special office hours sessions at the beginning of the term to help you doing that.\n\nCome talk to me at the end of the first lecture if you are unsure about your preparation for this course.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#software",
    "href": "syllabus.html#software",
    "title": "Syllabus: STAT405 Bayesian Statistics",
    "section": "Software",
    "text": "Software\nAll software used is free and open source. Some key tools we will use:\n\nR\nRStudio\nRStan\n\nWe assume you have a laptop on which you can install these tools, if not, you may be able to borrow one from UBC library.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#textbook",
    "href": "syllabus.html#textbook",
    "title": "Syllabus: STAT405 Bayesian Statistics",
    "section": "Textbook",
    "text": "Textbook\nNotes will be provided and complemented with readings from the following freely available textbook:\n\nBayesian Data Analysis, Third Edition. Andrew Gelman, John Carlin, Hal Stern, David Dunson, Aki Vehtari, and Donald Rubin. PDF freely available.\n\nAdditional readings and case studies will be drawn from other textbooks that are either freely available or available within UBC VPN:\n\nBayesian essentials with R, Second Edition. Jean-Michel Marin and Christian Robert. PDF available via UBC library. Solution to exercises.\nBayes Rules! Alicia A. Johnson, Miles Q. Ott, Mine Dogucu. HTML freely available\nDoing Bayesian data analysis: a tutorial with R, JAGS, and Stan, Second Edition. John K. Kruschke. PDF freely available.\nProbability and Bayesian modeling. Jim Albert and Jingchen Hu. PDF/HTML/EPUB freely available.\nStatistical Rethinking, Second Edition. Richard McElreath. HTML available via UBC library.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#assessments",
    "href": "syllabus.html#assessments",
    "title": "Syllabus: STAT405 Bayesian Statistics",
    "section": "Assessments",
    "text": "Assessments\nClick on each item for details.\n\nParticipation: 7%. Each week, do at least one of:\n\nWeekly reading assignment: ask one thoughtful question, or participate meaningfully in an answer/discussion thread about the readings on Piazza, or,\ngo to a lab or office hour on that week and fill the sign up sheet, or,\nif you really do not have any question, post a comment, e.g., website you found useful to learn the material.\n\nLow-stake questions: 8%. In-class iClicker questions: binary participation points (attempted vs not), unless your score is indistinguishable from random.1 Setup iClicker Cloud on Canvas. See also challenge questions below.\nHomework exercises: 15%\n\nWeekly (look for the ‘Exercises’ page for the corresponding week on this website).\nSubmit on Canvas.\n\nQuizzes (2 x 20%): 40%\n\nIn-class, closed-book.\nDates: February 24 and March 24\n\nFinal project: 30%\n\nFor the participation and homework, we will drop the lowest week. For the iClicker, we will automatically skip up to two missed lectures. Do not ask for additional accommodations, this is the accommodation, i.e., keep these for sick days/unforeseen circumstances. On the positive side, no need to ask for permission/provide doctor’s note, this will be done automatically for everyone.\nFor exercise grading, I will take the \\(\\max(\\text{mean}(e_1, e_2, ...), \\text{mean}(e_2, e_3, ...)), \\text{mean}(e_3, e_4, ...))\\), where \\(e_1, e_2, e_3, \\dots\\) are the scores for the different exercises. The same applies for clicker participation points. This is in place to help a couple students travelling in the first two weeks, but will be automatically applied for everyone so no need to request it.\nOnce in a while, I will post some challenge questions. These are not essential for learning the material and can be skipped. Submit your answer at any time. For each that you successfully solve, a week of iClicker participation will be waived (it does not have to be the same week you submit the challenge question). I will not post solutions for the challenge questions.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#policy-on-the-use-of-ai-tools",
    "href": "syllabus.html#policy-on-the-use-of-ai-tools",
    "title": "Syllabus: STAT405 Bayesian Statistics",
    "section": "Policy on the use of AI tools",
    "text": "Policy on the use of AI tools\n\nPiazza discussion: use of AI tools is prohibited to compose or answer questions, with the exception of spell-checking tools.\nHomework: use of AI tools is prohibited, with the exception of spell-checking tools.\nQuizzes: use of AI tools is impossible (closed book, pen and paper).\nProject: if you think AI tool(s) will be used, explain the scope of their use in the project proposal. Also describe all uses in the final report.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#office-hours",
    "href": "syllabus.html#office-hours",
    "title": "Syllabus: STAT405 Bayesian Statistics",
    "section": "Office hours",
    "text": "Office hours\nFriday, 1pm, ESB 3125.\nAvailable by appointment if you are unable to attend drop-in hours.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-communication",
    "href": "syllabus.html#course-communication",
    "title": "Syllabus: STAT405 Bayesian Statistics",
    "section": "Course communication",
    "text": "Course communication\n\nAnnouncements\nCourse announcements will be posted on Canvas.\n\n\nQuestions\nUse Piazza for questions about the material, logistics, etc. Use public posts as much as possible so that other students can learn from the discussion.\nUse canvas messages if, and only if the question is about a personal matter.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#footnotes",
    "href": "syllabus.html#footnotes",
    "title": "Syllabus: STAT405 Bayesian Statistics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHow is it possible to determine if a score is indistinguishable from random? This is exactly the kind of problem you will be able to solve after taking this course. We will cover a Bayesian hierarchical model that can do that in week 10 of this course.↩︎",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "w07_quiz1/topic01_terminology.html",
    "href": "w07_quiz1/topic01_terminology.html",
    "title": "Terminology review",
    "section": "",
    "text": "Review of key terminology covered so far.\n\n\n\n\nPart of quiz 1 involves knowing these terms without having to look them up.\nWhile in “the real world” you can look things up, to attain mastery of a subject you need a critical mass of concepts in memory. Too much google look-ups prevent thinking at the speed of thought.",
    "crumbs": [
      "Quiz 1",
      "Terminology review"
    ]
  },
  {
    "objectID": "w07_quiz1/topic01_terminology.html#outline",
    "href": "w07_quiz1/topic01_terminology.html#outline",
    "title": "Terminology review",
    "section": "",
    "text": "Review of key terminology covered so far.\n\n\n\n\nPart of quiz 1 involves knowing these terms without having to look them up.\nWhile in “the real world” you can look things up, to attain mastery of a subject you need a critical mass of concepts in memory. Too much google look-ups prevent thinking at the speed of thought.",
    "crumbs": [
      "Quiz 1",
      "Terminology review"
    ]
  },
  {
    "objectID": "w07_quiz1/topic01_terminology.html#probability-essentials",
    "href": "w07_quiz1/topic01_terminology.html#probability-essentials",
    "title": "Terminology review",
    "section": "Probability essentials",
    "text": "Probability essentials\n\nProbability vs PMF vs density\nEvent vs outcome\nRandom variable\nRealization\nProbability and conditional probability\nExpectation vs probability\nIndicator function",
    "crumbs": [
      "Quiz 1",
      "Terminology review"
    ]
  },
  {
    "objectID": "w07_quiz1/topic01_terminology.html#common-distributions-pmfs-and-densities",
    "href": "w07_quiz1/topic01_terminology.html#common-distributions-pmfs-and-densities",
    "title": "Terminology review",
    "section": "Common distributions (PMFs and densities)",
    "text": "Common distributions (PMFs and densities)\nKnowing the support (no need to memorize other details)\n\nBernoulli\nBinomial\nUniform (discrete and continuous)\nCategorical\nNormal\nExponential\nBeta",
    "crumbs": [
      "Quiz 1",
      "Terminology review"
    ]
  },
  {
    "objectID": "w07_quiz1/topic01_terminology.html#basic-bayesian-terms",
    "href": "w07_quiz1/topic01_terminology.html#basic-bayesian-terms",
    "title": "Terminology review",
    "section": "Basic Bayesian terms",
    "text": "Basic Bayesian terms\n\nPrior distribution (PMF/density)\nLikelihood\nPosterior distribution (PMF/density)\nJoint distribution\nJoint vs marginal posterior\nPredictive distribution",
    "crumbs": [
      "Quiz 1",
      "Terminology review"
    ]
  },
  {
    "objectID": "w07_quiz1/topic01_terminology.html#sampling",
    "href": "w07_quiz1/topic01_terminology.html#sampling",
    "title": "Terminology review",
    "section": "Sampling",
    "text": "Sampling\n\nSample\nForward vs posterior sampling\nSimple Monte Carlo vs SNIS\nTest function",
    "crumbs": [
      "Quiz 1",
      "Terminology review"
    ]
  },
  {
    "objectID": "w07_quiz1/topic01_terminology.html#decision-theory",
    "href": "w07_quiz1/topic01_terminology.html#decision-theory",
    "title": "Terminology review",
    "section": "Decision theory",
    "text": "Decision theory\n\nLoss function\nBayes estimator",
    "crumbs": [
      "Quiz 1",
      "Terminology review"
    ]
  },
  {
    "objectID": "w07_quiz1/topic01_terminology.html#posterior-summaries",
    "href": "w07_quiz1/topic01_terminology.html#posterior-summaries",
    "title": "Terminology review",
    "section": "Posterior summaries",
    "text": "Posterior summaries\n\nPoint estimate vs credible regions",
    "crumbs": [
      "Quiz 1",
      "Terminology review"
    ]
  },
  {
    "objectID": "w07_quiz1/topic01_terminology.html#point-estimation",
    "href": "w07_quiz1/topic01_terminology.html#point-estimation",
    "title": "Terminology review",
    "section": "Point estimation",
    "text": "Point estimation\n\nPosterior mean\nPosterior mode",
    "crumbs": [
      "Quiz 1",
      "Terminology review"
    ]
  },
  {
    "objectID": "w07_quiz1/topic01_terminology.html#credible-regions",
    "href": "w07_quiz1/topic01_terminology.html#credible-regions",
    "title": "Terminology review",
    "section": "Credible regions",
    "text": "Credible regions\n\nNominal coverage\nQuantile-based credible interval\nHighest Density Set",
    "crumbs": [
      "Quiz 1",
      "Terminology review"
    ]
  },
  {
    "objectID": "w07_quiz1/topic01_terminology.html#model-criticism",
    "href": "w07_quiz1/topic01_terminology.html#model-criticism",
    "title": "Terminology review",
    "section": "Model criticism",
    "text": "Model criticism\n\nModel mis-specification\nCromwell’s rule",
    "crumbs": [
      "Quiz 1",
      "Terminology review"
    ]
  },
  {
    "objectID": "w07_quiz1/topic01_terminology.html#ppls",
    "href": "w07_quiz1/topic01_terminology.html#ppls",
    "title": "Terminology review",
    "section": "PPLs",
    "text": "PPLs\n\nPPL\nConsistency\nRate of convergence\n(Importance) weights (normalized vs un-normalized)",
    "crumbs": [
      "Quiz 1",
      "Terminology review"
    ]
  },
  {
    "objectID": "w07_quiz1/topic01_terminology.html#model-terminology",
    "href": "w07_quiz1/topic01_terminology.html#model-terminology",
    "title": "Terminology review",
    "section": "Model terminology",
    "text": "Model terminology\n\nClassification vs regression\nInput/output/covariate",
    "crumbs": [
      "Quiz 1",
      "Terminology review"
    ]
  },
  {
    "objectID": "exercises/ex09.html",
    "href": "exercises/ex09.html",
    "title": "Exercise 9: MCMC Hacking",
    "section": "",
    "text": "Caution\n\n\n\nPage under construction: information on this page may change.",
    "crumbs": [
      "MCMC Hacking",
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/ex08.html",
    "href": "exercises/ex08.html",
    "title": "Exercise 8: Bayesian Workflow",
    "section": "",
    "text": "Caution\n\n\n\nPage under construction: information on this page may change.",
    "crumbs": [
      "Bayesian workflow",
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/ex07.html",
    "href": "exercises/ex07.html",
    "title": "Exercise 7: Introduction to MCMC",
    "section": "",
    "text": "Caution\n\n\n\nPage under construction: information on this page may change.",
    "crumbs": [
      "Intro to MCMC",
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/ex10.html",
    "href": "exercises/ex10.html",
    "title": "Exercise 10: Advanced inference",
    "section": "",
    "text": "Caution\n\n\n\nPage under construction: information on this page may change.",
    "crumbs": [
      "Advanced inference",
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/ex01.html",
    "href": "exercises/ex01.html",
    "title": "Exercise 1: discrete probabilistic inference",
    "section": "",
    "text": "Grading\n\n\n\nOur priority with the weekly exercises is to provide timely feedback and an incentive to stay on top of the material so that lectures can be more effective.\nWe will select one or more questions that will be graded in more detail. For the other question(s), we will use the following participation-centric binary scheme:\n\n1 point if something reasonable was attempted,\n0 otherwise.",
    "crumbs": [
      "Probability essentials",
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/ex01.html#goals",
    "href": "exercises/ex01.html#goals",
    "title": "Exercise 1: discrete probabilistic inference",
    "section": "Goals",
    "text": "Goals\n\nBring back to memory discrete probability (axioms, basic properties, conditioning, discrete Bayes rule).\nIntroduce forward discrete simulation.\nReview expectation and the law of large numbers.",
    "crumbs": [
      "Probability essentials",
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/ex01.html#setup",
    "href": "exercises/ex01.html#setup",
    "title": "Exercise 1: discrete probabilistic inference",
    "section": "Setup",
    "text": "Setup\nThis exercise is centered around the following scenario:\n\nImagine a bag with 3 coins each with a different probability parameter \\(p\\)\nCoin \\(i\\in \\{0, 1, 2\\}\\) has bias \\(i/2\\)—in other words:\n\nFirst coin: bias is \\(0/2 = 0\\) (i.e. both sides are “heads”, \\(p = 0\\))\nSecond coin: bias is \\(1/2 = 0.5\\) (i.e. standard coin, \\(p = 1/2\\))\nThird coin: bias is \\(2/2 = 1\\) (i.e. both sides are “tails”, \\(p = 1\\))\n\n\n\n\n\n\nConsider the following two steps sampling process\n\nStep 1: pick one of the three coins, but do not look at it!\nStep 2: flip the coin 4 times\n\nMathematically, this probability model can be written as follows: \\[\n\\begin{align*}\nX &\\sim {\\mathrm{Unif}}\\{0, 1, 2\\} \\\\\nY_i | X &\\sim {\\mathrm{Bern}}(X/2)\n\\end{align*}\n\\tag{1}\\]",
    "crumbs": [
      "Probability essentials",
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/ex01.html#sec-q1",
    "href": "exercises/ex01.html#sec-q1",
    "title": "Exercise 1: discrete probabilistic inference",
    "section": "Q.1: sampling from a joint distribution",
    "text": "Q.1: sampling from a joint distribution\n\nCompute \\(\\mathbb{E}[(1+Y_1)^X]\\) mathematically (with a precise mathematical derivation).\nWrite an R function called forward_sample that samples (“simulates”) from the joint distribution of \\((X, Y_1, \\dots, Y_4)\\). As a general practice, fix the seed, and submit both the code and the output (here, a single sample).\nHow can your code and the law of large number be used to approximate \\(\\mathbb{E}[(1+Y_1)^X]\\)?\nCompare the approximation from your code with you answer in part 1.\n\n\n\n\n\n\n\nBig idea\n\n\n\nPart 4 of this question illustrates a big idea in this course:\nstrategies to validate inference, i.e. ensuring it is bug-free in both the code and in the math. In a nutshell, this is possible thanks to theory: we use results that provide two ways to do the same thing, and verifying they indeed agree.",
    "crumbs": [
      "Probability essentials",
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/ex01.html#sec-q2",
    "href": "exercises/ex01.html#sec-q2",
    "title": "Exercise 1: discrete probabilistic inference",
    "section": "Q.2: computing a conditional",
    "text": "Q.2: computing a conditional\nSuppose now that you observe the outcome of the 4 coin flips, but not the type of coin that was picked. Say you observe: “heads”, “heads”, “heads”, “heads” = [0, 0, 0, 0]. Given that observation, what is the probability that you picked the standard coin (i.e., the one with \\(p = 1/2\\))?\n\nWrite mathematically: “Given you observe 4 heads, what is the probability that you picked the standard coin?”\nCompute the numerical value of the expression defined in part 1 (with a precise mathematical derivation).",
    "crumbs": [
      "Probability essentials",
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/ex01.html#sec-q3",
    "href": "exercises/ex01.html#sec-q3",
    "title": "Exercise 1: discrete probabilistic inference",
    "section": "Q.3: non uniform prior on coin types",
    "text": "Q.3: non uniform prior on coin types\nWe now modify the problem as follows: I stuffed the bag with 100 coins: 98 standard (fair) coins, 1 coin with only heads, and 1 coin with only tails. The rest is the same: pick one of the coins, flip it 4 times.\n\nWrite the joint distribution of this modified model. Use the \\(\\sim\\) notation as in Equation 1. Hint: use a Categorical distribution.\nCompute the probability that you picked one of the fair coins, given you see [0, 0, 0, 0].",
    "crumbs": [
      "Probability essentials",
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/ex01.html#sec-q4",
    "href": "exercises/ex01.html#sec-q4",
    "title": "Exercise 1: discrete probabilistic inference",
    "section": "Q.4: a first posterior inference algorithm",
    "text": "Q.4: a first posterior inference algorithm\nWe now generalize to having \\(K + 1\\) types of coins such that:\n\ncoin type \\(k \\in \\{0, 1, \\dots, K\\}\\) has bias \\(k/K\\)\nthe fraction of coins in the bag of type \\(k\\) is \\(\\rho_k\\).\n\nWe consider the same observation as before: “you observe 4 heads”. We want to find the conditional probability \\(\\pi_k\\) for all \\(k\\) that we picked coin type \\(k \\in \\{0, 1, \\dots, K\\}\\) from the bag given the observation.\n\nWrite an R function called posterior_given_four_heads taking as input a vector \\(\\rho = (\\rho_0, \\rho_1, \\dots, \\rho_K)\\) and returning \\(\\pi = (\\pi_0, \\pi_1, \\dots, \\pi_K)\\).\nTest your code by making sure you can recover the answer in Q. 3 as a special case. Report what values of \\(K\\) and \\(\\rho\\) you used.\nShow the output for \\(\\rho \\propto (1, 2, 3, \\dots, 10)\\). Here \\(\\propto\\) means “proportional to”; try to infer what it means in this context.",
    "crumbs": [
      "Probability essentials",
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/ex01.html#sec-q5",
    "href": "exercises/ex01.html#sec-q5",
    "title": "Exercise 1: discrete probabilistic inference",
    "section": "Q.5: generalizing observations",
    "text": "Q.5: generalizing observations\nWe now generalize Q. 4 as follows: instead of observing 4 “heads” out of 4 observations, say we observe n_heads out of n_observations, where n_heads and n_observations will be additional arguments passed into a new R function.\n\nWrite the joint distribution of this modified model. Use the \\(\\sim\\) notation as in Equation 1. Hint: use a Binomial distribution.\nWrite an R function called posterior taking three input arguments in the following order: a vector \\(\\rho\\) as in Q. 4, as well as two integers, n_heads and n_observations.\nTest your code by making sure you can recover the answer in Q. 3 as a special case.\nShow the output for \\(\\rho \\propto (1, 2, 3, \\dots, 10)\\) and n_heads = 2 and n_observations = 10.",
    "crumbs": [
      "Probability essentials",
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/ex02.html",
    "href": "exercises/ex02.html",
    "title": "Exercise 2: Bayesian inference first contact",
    "section": "",
    "text": "Grading\n\n\n\nOur priority with the weekly exercises is to provide timely feedback and an incentive to stay on top of the material so that lectures can be more effective.\nWe will select one or more questions that will be graded in more detail. For the other question(s), we will use the following participation-centric binary scheme:\n\n1 point if something reasonable was attempted,\n0 otherwise.",
    "crumbs": [
      "Bayes on a discrete model",
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/ex02.html#goals",
    "href": "exercises/ex02.html#goals",
    "title": "Exercise 2: Bayesian inference first contact",
    "section": "Goals",
    "text": "Goals\n\nBuild a probability model for a concrete example.\nIntroduce the concept of Bayes estimators.",
    "crumbs": [
      "Bayes on a discrete model",
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/ex02.html#sec-setup",
    "href": "exercises/ex02.html#sec-setup",
    "title": "Exercise 2: Bayesian inference first contact",
    "section": "Setup",
    "text": "Setup\nThis exercise is centered around the following scenario:\n\nYou are consulting for a satellite operator\nThey are about to send a $100M satellite on a Delta 7925H rocket\n\n\n\n\n\nData: as of Jan 2025, Delta 7925H rockets have been launched 3 times, with 0 failed launches \n\nNote: Delta 7925H is not reusable, so each rocket is “copy- built” from the same blueprint\n\nShould you recommend buying a $2M insurance policy?\n\nConvention: use 1 for a success, 0 for a failure.",
    "crumbs": [
      "Bayes on a discrete model",
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/ex02.html#sec-q1",
    "href": "exercises/ex02.html#sec-q1",
    "title": "Exercise 2: Bayesian inference first contact",
    "section": "Q.1: define a Bayesian model",
    "text": "Q.1: define a Bayesian model\nIn order to perform inference on the unknown quantities, we must specify how they relate to the data; i.e., we need a probabilistic model. Assume that every Delta 7925H rocket has the same probability \\(p\\) of success. For simplicity, let us assume that \\(p\\) is allowed to take values on an evenly space grid \\[\np \\in \\left\\{\\frac{k}{K}: k\\in \\{0,\\dots,K\\}\\right\\}\n\\] for some fixed \\(K\\in\\mathbb{N}\\). Furthermore, we have access to a collection of numbers \\(\\rho_k\\in[0,1]\\) such that1 \\[\n\\forall k\\in\\{0,\\dots,K\\}:\\ \\mathbb{P}\\left(p=\\frac{k}{K}\\right) = \\rho_k.\n\\tag{1}\\]\nLet \\(Y_i\\) denote a binary variable with \\(Y_i=1\\) encoding a success, and \\(Y_i=0\\) a failure. We assume that, conditionally on \\(p\\), the \\(Y_i\\)’s are independent of each other.\nWe will use the following prior: \\[\n\\rho_k \\propto \\frac{k}{K}\\left(1-\\frac{k}{K}\\right).\n\\tag{2}\\] From now on, use \\(K = 20\\).\n\nWhat are the unknown quantities in this scenario? And what is the data?\nWrite the joint distribution of this model (use the \\(\\sim\\) notation).",
    "crumbs": [
      "Bayes on a discrete model",
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/ex02.html#sec-q2",
    "href": "exercises/ex02.html#sec-q2",
    "title": "Exercise 2: Bayesian inference first contact",
    "section": "Q.2: posterior and point estimates",
    "text": "Q.2: posterior and point estimates\nTo help you answer the following questions, create the two vectors:\n\nprior_probabilities where entry \\(i\\) containing the prior probability \\(\\rho_{i-1}\\) defined in Q1 (the minus one reflects the fact that R uses indexing starting at 1), and\nrealizations, a vector of possible realizations of \\(p\\) in the same order, namely \\((0, 1/K, 2/K \\dots, 1)\\).\n\n\nPlot the prior PMF. Do you think this is a reasonable prior? Hint: use the same type of plot as used last week to plot PMFs.\nLet \\(\\pi_k = \\mathbb{P}(p = k/K | Y_{1:3} = (1, 1, 1))\\) denote the posterior probabilities, for \\(k \\in \\{0, 1, 2, \\dots, K\\}\\). Create a vector posterior_probabilities where entry \\(i\\) is \\(\\pi_{i-1}\\). Plot the posterior PMF.\nWhat is the posterior mode?\nWrite a function that compute the posterior mean of \\(p\\). Hint: you should obtain \\(\\mathbb{E}[p | Y_{1:3} = (1, 1, 1)] \\approx 0.7\\).",
    "crumbs": [
      "Bayes on a discrete model",
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/ex02.html#sec-q3",
    "href": "exercises/ex02.html#sec-q3",
    "title": "Exercise 2: Bayesian inference first contact",
    "section": "Q.3: Bayes action",
    "text": "Q.3: Bayes action\nLet \\(a\\in\\{0,1\\}\\) be a binary variable denoting the decision of buying the insurance (\\(a=1\\)) or not (\\(a=0\\)).\n\nBased on the problem description from the Setup Section, define a loss function \\(L(a, y)\\) that summarizes the cost of having taken decision \\(a\\in\\{0,1\\}\\) depending on whether the next launch is successful (\\(y = 1\\)) or not (\\(y = 0\\)). Hint: use indicator functions (i.e. binary functions taking either the value zero or one).\nWe now consider the expected loss under the posterior predictive distribution: \\[\n\\mathcal{L}(a) := \\mathbb{E}[L(a,Y_4)|Y_{1:3}=(1, 1, 1)]\n\\] Write \\(\\mathcal{L}(a)\\) in terms of \\(\\mathbb{P}\\left(Y_4=1 \\middle| Y_{1:3}=(1, 1, 1) \\right)\\). Important: you can use without proof that \\(\\mathbb{P}\\left(Y_4=1 \\middle| Y_{1:3}=(1, 1, 1) \\right)\\) is the same as the posterior mean, which we computed earlier to be \\(\\approx 0.7\\) for our choice of prior.2\nFormulate a recommendation to the owner of the satellite (again, you can use without proof that \\(\\mathbb{P}\\left(Y_4=1 \\middle| Y_{1:3}=(1, 1, 1) \\right) \\approx 0.7\\)).",
    "crumbs": [
      "Bayes on a discrete model",
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/ex02.html#footnotes",
    "href": "exercises/ex02.html#footnotes",
    "title": "Exercise 2: Bayesian inference first contact",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNotice that in Equation 1 we are using (small cap) \\(p\\) as a random variable, i.e. starting to move away from the probability theory capitalization convention towards the Bayesian convention where the same capitalization is used for both the random variable and its realization, as discussed in the first week.↩︎\nThe proof is as follows, where here we do disambiguate between the random variable \\(P\\) and its realization \\(p\\) (not to be confused with \\(\\mathbb{P}\\) and PMF \\(p(\\cdot)\\)!): \\[\\begin{align*}\n  \\mathbb{P}(Y_4 = y_4 | Y_{1:3} = \\boldsymbol{1}) &= \\sum_p \\mathbb{P}(P = p, Y_4 = y_4 | Y_{1:3} = \\boldsymbol{1}) \\;\\;\\text{(additivity axiom)} \\\\\n  &= \\sum_p  \\mathbb{P}(P = p | Y_{1:3} = \\boldsymbol{1}) \\mathbb{P}(Y_4 = y_4 | P = p, Y_{1:3} = \\boldsymbol{1}) \\;\\;\\text{(chain rule)} \\\\\n  &= \\sum_p \\pi(p) \\mathbb{P}(Y_4 = y_4 | P = p, Y_{1:3} = \\boldsymbol{1}) \\;\\;\\text{(definition)} \\\\\n  &= \\sum_p \\pi(p) \\mathbb{P}(Y_4 = y_4 | P = p) \\;\\;\\text{(conditional independence)} \\\\\n  &= \\sum_p \\pi(p) p \\;\\;\\text{(since each flip is assumed to be Bernoulli)} \\\\\n  &= \\mathbb{E}[p | Y_{1:3} = \\boldsymbol{1}].\n  \\end{align*}\\] Note however that this argument is very specific to this Bernoulli likelihood model and will not generalize. We will cover in class the general method to compute predictive distribution, see lecture notes.↩︎",
    "crumbs": [
      "Bayes on a discrete model",
      "Exercises"
    ]
  },
  {
    "objectID": "w13_advanced_infer/topic02_variational.html",
    "href": "w13_advanced_infer/topic02_variational.html",
    "title": "Variational methods",
    "section": "",
    "text": "Variational approximations\nDistribution families\nPosterior approximation as optimization\n\n\n\n\nIn certain situations, and for a finite time budget, variational methods can provide a better approximation than MCMC methods.",
    "crumbs": [
      "Advanced inference",
      "Variational methods"
    ]
  },
  {
    "objectID": "w13_advanced_infer/topic02_variational.html#outline",
    "href": "w13_advanced_infer/topic02_variational.html#outline",
    "title": "Variational methods",
    "section": "",
    "text": "Variational approximations\nDistribution families\nPosterior approximation as optimization\n\n\n\n\nIn certain situations, and for a finite time budget, variational methods can provide a better approximation than MCMC methods.",
    "crumbs": [
      "Advanced inference",
      "Variational methods"
    ]
  },
  {
    "objectID": "w13_advanced_infer/topic02_variational.html#what-is-a-variational-method",
    "href": "w13_advanced_infer/topic02_variational.html#what-is-a-variational-method",
    "title": "Variational methods",
    "section": "What is a variational method?",
    "text": "What is a variational method?\nSetup: as in MCMC…\n\nwe try to approximate a posterior \\(\\pi(x)\\) with \\(x \\in \\mathscr{X}\\),\nknown only up to normalization constant, \\(\\pi(x) = \\gamma(x) / Z\\).\n\nAdditional ingredient: a family (set) of distributions, \\(\\mathscr{Q}= \\{q_\\phi : \\phi \\in \\Phi\\}\\) called the variational family.1\n\nEach element of the variational family is a density/PMF \\(q_\\phi(x)\\) defined on the same space as \\(\\pi(x)\\).\nEach element is indexed by a variational parameter \\(\\phi \\in \\Phi\\).\n\nVariational inference: find a variational approximation \\(q^*_\\phi\\in\\mathscr{Q}\\) as close as possible to the posterior \\(\\pi\\).",
    "crumbs": [
      "Advanced inference",
      "Variational methods"
    ]
  },
  {
    "objectID": "w13_advanced_infer/topic02_variational.html#distribution-families",
    "href": "w13_advanced_infer/topic02_variational.html#distribution-families",
    "title": "Variational methods",
    "section": "Distribution families",
    "text": "Distribution families\nExample: consider the beta-binomial example.\n\n\n\n\nHere we could take \\(\\mathscr{Q}\\) to be the set of all beta distributions.\nThe beta family has two positive parameters, \\(\\phi = (\\alpha, \\beta)\\), \\(\\alpha, \\beta &gt; 0\\)…\n… so \\(\\Phi = \\mathbb{R}^+ \\times \\mathbb{R}^+\\).\nThis is the “best case” for variational inference:\n\nfrom a practice quiz exercise, \\(\\pi \\in \\mathscr{Q}\\),\nso \\(q^*_\\phi = \\pi\\), i.e., variational inference incurs no approximation error in this special case (see figure).\n\nTypically, we do not have tractable (easy to compute) conjugate family though.\n\nExample: when \\(\\mathscr{X}= \\mathbb{R}\\), a typical choice of variational family is to use a normal family.\n\n\n\nQuestion: for the normal variational family, complete the definition of variational approximation \\(q_\\phi\\), variational parameter \\(\\phi\\) and parameters \\(\\Phi\\).",
    "crumbs": [
      "Advanced inference",
      "Variational methods"
    ]
  },
  {
    "objectID": "w13_advanced_infer/topic02_variational.html#footnotes",
    "href": "w13_advanced_infer/topic02_variational.html#footnotes",
    "title": "Variational methods",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNotice we also used \\(q\\) for MCMC proposal and importance sampling’s importance distribution. These are different but related. It is a general convention to use \\(q\\) for “quantities that help us to approximate a complex distribution.” The connection between variational inference’s \\(q\\) and importance sampling’s \\(q\\) is more direct compared to MCMC’s proposal \\(q\\).↩︎",
    "crumbs": [
      "Advanced inference",
      "Variational methods"
    ]
  },
  {
    "objectID": "w13_advanced_infer/topic03_kl.html",
    "href": "w13_advanced_infer/topic03_kl.html",
    "title": "The KL divergence",
    "section": "",
    "text": "Kullback-Leibler (KL) divergence\nEvidence Lower Bound (ELBO)\n\n\n\n\nIn the previous page we saw that we need a notion of “closeness” of distributions. The KL divergence is the most frequent choice in the context of variational inference. We define the KL and explain why it is used in variational inference.",
    "crumbs": [
      "Advanced inference",
      "The KL divergence"
    ]
  },
  {
    "objectID": "w13_advanced_infer/topic03_kl.html#outline",
    "href": "w13_advanced_infer/topic03_kl.html#outline",
    "title": "The KL divergence",
    "section": "",
    "text": "Kullback-Leibler (KL) divergence\nEvidence Lower Bound (ELBO)\n\n\n\n\nIn the previous page we saw that we need a notion of “closeness” of distributions. The KL divergence is the most frequent choice in the context of variational inference. We define the KL and explain why it is used in variational inference.",
    "crumbs": [
      "Advanced inference",
      "The KL divergence"
    ]
  },
  {
    "objectID": "w13_advanced_infer/topic03_kl.html#definition",
    "href": "w13_advanced_infer/topic03_kl.html#definition",
    "title": "The KL divergence",
    "section": "Definition",
    "text": "Definition\nGiven two distribution \\(\\pi\\) and \\(q\\), define the Kullback-Leibler (KL) divergence as: \\[\\operatorname{KL}(q \\| \\pi) = \\int q(x) \\log \\frac{q(x)}{\\pi(x)} \\mathrm{d}x.\\]\nNote:\n\nthe KL is asymmetric in its two arguments.\nWe will put the variational approximation \\(q\\) as the distribution we average over.\nThis is sometimes called the “backward” or “reverse” KL.\nMore on this choice below.",
    "crumbs": [
      "Advanced inference",
      "The KL divergence"
    ]
  },
  {
    "objectID": "w13_advanced_infer/topic03_kl.html#why-variational-inference-uses-the-reverse-kl",
    "href": "w13_advanced_infer/topic03_kl.html#why-variational-inference-uses-the-reverse-kl",
    "title": "The KL divergence",
    "section": "Why variational inference uses the reverse KL?",
    "text": "Why variational inference uses the reverse KL?\nWe cover the two key reasons below: it captures “closeness” and it can be optimized.\n\nReverse KL captures the notion of “closeness”\nProperty: \\(\\operatorname{KL}(q_1 \\| q_2 ) \\ge 0\\) with equality iff \\(q_1 = q_2\\).\nProof: since \\(\\log\\) is a concave function, by Jensen’s inequality,\n\\[\\begin{align*}\n\\operatorname{KL}(q_1 \\| q_2 ) &= \\int q_1(x) \\log \\frac{q_1(x)}{q_2(x)} \\mathrm{d}x \\;\\;\\text{(definition)} \\\\\n&= - \\int q_1(x) \\log \\frac{q_2(x)}{q_1(x)} \\mathrm{d}x \\;\\;\\text{($-\\log a = \\log a^{-1}$)} \\\\\n&\\ge - \\log \\int {\\color{red} q_1(x)} \\frac{q_2(x)}{{\\color{red} q_1(x)}} \\mathrm{d}x \\;\\;\\text{(Jensen's)} \\\\\n&= - \\log \\int q_2(x) \\mathrm{d}x \\;\\;\\text{(red factors cancel)} \\\\\n&= 0 \\;\\;\\text{($q_2$ is a probability density)}.\n\\end{align*}\\]\n\n\nReverse KL can be optimized\nRequirement: we want to be able to optimize the objective function without having to compute the intractable normalization constant \\(Z\\).\nMany other notions of distribution “closeness” do not satisfy this.",
    "crumbs": [
      "Advanced inference",
      "The KL divergence"
    ]
  },
  {
    "objectID": "w13_advanced_infer/topic03_kl.html#towards-optimization-of-the-reverse-kl",
    "href": "w13_advanced_infer/topic03_kl.html#towards-optimization-of-the-reverse-kl",
    "title": "The KL divergence",
    "section": "Towards optimization of the reverse KL",
    "text": "Towards optimization of the reverse KL\nWe show that optimizing the reverse KL does not require knowing the intractable normalization constant \\(Z\\):\n\\[\\begin{align*}\n\\operatorname{arg\\,min}_\\phi \\operatorname{KL}(q_\\phi \\| \\pi) &= \\operatorname{arg\\,min}_\\phi \\int q_\\phi(x) \\log \\frac{q_\\phi(x)}{\\pi(x)} \\mathrm{d}x \\\\\n&= \\operatorname{arg\\,min}_\\phi  \\int q_\\phi(x) \\log \\frac{q_\\phi(x) Z}{\\gamma(x)} \\mathrm{d}x \\\\\n&= \\operatorname{arg\\,min}_\\phi  \\int q_\\phi(x) \\left[ \\log q_\\phi(x) + \\log Z - \\log \\gamma(x)   \\right] \\mathrm{d}x \\\\\n&= \\operatorname{arg\\,min}_\\phi  \\underbrace{\\int q_\\phi(x) \\left[ \\log q_\\phi(x)  - \\log \\gamma(x)   \\right] \\mathrm{d}x}_{L(\\phi)} + {\\color{red} \\log Z} \\\\\n&= \\operatorname{arg\\,min}_\\phi L(\\phi) \\;\\;\\text{(red term does not depend on $\\phi$)}.\n\\end{align*}\\]\nNotice: \\(L(\\phi)\\) does not involve \\(Z\\)!\nTerminology: the negative value of \\(L\\) is called the Evidence Lower BOund (ELBO), \\(\\operatorname{ELBO}(\\phi) = -L(\\phi)\\).\nQuestion: how did we get \\(\\log Z\\) outside of the integral (step in red)?",
    "crumbs": [
      "Advanced inference",
      "The KL divergence"
    ]
  },
  {
    "objectID": "w13_advanced_infer/topic05_sgd.html",
    "href": "w13_advanced_infer/topic05_sgd.html",
    "title": "Stochastic gradient descent",
    "section": "",
    "text": "Gradient descent\nStochastic gradient descent (SGD)\n\n\n\n\nBlack-box VI uses SGD (or similar algorithms) to perform optimization over the variational parameters.\nThe “stochasticity” comes from the fact we approximate the integral in \\(L\\) using Monte Carlo.1",
    "crumbs": [
      "Advanced inference",
      "Stochastic gradient descent"
    ]
  },
  {
    "objectID": "w13_advanced_infer/topic05_sgd.html#outline",
    "href": "w13_advanced_infer/topic05_sgd.html#outline",
    "title": "Stochastic gradient descent",
    "section": "",
    "text": "Gradient descent\nStochastic gradient descent (SGD)\n\n\n\n\nBlack-box VI uses SGD (or similar algorithms) to perform optimization over the variational parameters.\nThe “stochasticity” comes from the fact we approximate the integral in \\(L\\) using Monte Carlo.1",
    "crumbs": [
      "Advanced inference",
      "Stochastic gradient descent"
    ]
  },
  {
    "objectID": "w13_advanced_infer/topic05_sgd.html#gradient-descent",
    "href": "w13_advanced_infer/topic05_sgd.html#gradient-descent",
    "title": "Stochastic gradient descent",
    "section": "Gradient descent",
    "text": "Gradient descent\nRecall: the gradient \\(\\nabla L(\\phi) = (\\partial L/\\partial \\phi_1, \\dots, \\partial L/\\partial \\phi_K)\\) is a vector pointing towards the direction of steepest ascent.\n\n\n\nGradient descent: at each iteration, move the current point in the direction opposite to the gradient, \\[\\phi^{(t+1)} \\gets \\phi^{(t)} - \\epsilon_t \\nabla L(\\phi^{(t)}),\\] where \\(\\epsilon_t &gt; 0\\) is called a step size.\nProperty: for the right choice of step size \\(\\epsilon_t\\), the gradient descent algorithm converges to the minimum of the convex function (Boyd and Vandenberghe, 2004).",
    "crumbs": [
      "Advanced inference",
      "Stochastic gradient descent"
    ]
  },
  {
    "objectID": "w13_advanced_infer/topic05_sgd.html#stochastic-gradient-descent",
    "href": "w13_advanced_infer/topic05_sgd.html#stochastic-gradient-descent",
    "title": "Stochastic gradient descent",
    "section": "Stochastic gradient descent",
    "text": "Stochastic gradient descent\nIdea: replace \\(\\nabla L\\) by a randomized approximation, \\(\\hat \\nabla L\\).\nDefinition: we say \\(\\hat \\nabla L\\) is unbiased if \\(\\mathbb{E}[\\hat \\nabla L] = \\nabla L\\).\nNote: the expectation in “\\(\\mathbb{E}[\\hat \\nabla L]\\)” is with respect to the randomness used to create the random approximation. In our situation it will correspond to the randomness of sampling from our current variational family, \\(q_{\\phi^{(t)}}\\).\nProperty: if \\(\\hat \\nabla L\\) is unbiased, the objective function is convex, and the variance of \\(\\mathbb{E}[\\hat \\nabla L]\\) is bounded, stochastic gradient descent converges to the optimal solution \\(t \\to \\infty\\) with \\(\\epsilon_t = t^{-\\alpha}\\) for \\(\\alpha \\in (1/2, 1]\\) (Benveriste et al., 1990).",
    "crumbs": [
      "Advanced inference",
      "Stochastic gradient descent"
    ]
  },
  {
    "objectID": "w13_advanced_infer/topic05_sgd.html#footnotes",
    "href": "w13_advanced_infer/topic05_sgd.html#footnotes",
    "title": "Stochastic gradient descent",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOn top of that, we may “sub-sample” datapoints at each iteration, adding additional stochasticity.↩︎",
    "crumbs": [
      "Advanced inference",
      "Stochastic gradient descent"
    ]
  },
  {
    "objectID": "w11_quiz2/topic01_terminology.html",
    "href": "w11_quiz2/topic01_terminology.html",
    "title": "Terminology review continued",
    "section": "",
    "text": "Review additional terminology covered since Quiz 1.\nAlso make sure to review the terminology before quiz 1.\n\n\n\n\nJust as for quiz 1, part of quiz 2 involves knowing these terms without having to look them up.\nWhile in “the real world” you can look things up, to attain mastery of a subject you need a critical mass of concepts in memory. Too much google look-ups prevent thinking at the speed of thought.",
    "crumbs": [
      "Quiz 2",
      "Terminology review continued"
    ]
  },
  {
    "objectID": "w11_quiz2/topic01_terminology.html#outline",
    "href": "w11_quiz2/topic01_terminology.html#outline",
    "title": "Terminology review continued",
    "section": "",
    "text": "Review additional terminology covered since Quiz 1.\nAlso make sure to review the terminology before quiz 1.\n\n\n\n\nJust as for quiz 1, part of quiz 2 involves knowing these terms without having to look them up.\nWhile in “the real world” you can look things up, to attain mastery of a subject you need a critical mass of concepts in memory. Too much google look-ups prevent thinking at the speed of thought.",
    "crumbs": [
      "Quiz 2",
      "Terminology review continued"
    ]
  },
  {
    "objectID": "w11_quiz2/topic01_terminology.html#common-distributions-pmfs-and-densities",
    "href": "w11_quiz2/topic01_terminology.html#common-distributions-pmfs-and-densities",
    "title": "Terminology review continued",
    "section": "Common distributions (PMFs and densities)",
    "text": "Common distributions (PMFs and densities)\nAs in quiz 1, knowing the support (no need to memorize other details) is needed, but note additional distributions have been introduced see the list.",
    "crumbs": [
      "Quiz 2",
      "Terminology review continued"
    ]
  },
  {
    "objectID": "w11_quiz2/topic01_terminology.html#general-mcmc-terms",
    "href": "w11_quiz2/topic01_terminology.html#general-mcmc-terms",
    "title": "Terminology review continued",
    "section": "General MCMC terms",
    "text": "General MCMC terms\n\nMarkov chain Monte Carlo (MCMC)\nContrasting advantages and disadvantages of MCMC vs SNIS",
    "crumbs": [
      "Quiz 2",
      "Terminology review continued"
    ]
  },
  {
    "objectID": "w11_quiz2/topic01_terminology.html#stan-terminology",
    "href": "w11_quiz2/topic01_terminology.html#stan-terminology",
    "title": "Terminology review continued",
    "section": "Stan terminology",
    "text": "Stan terminology\n\nDifferentiating between Stan and R code\n“data block”\n“parameters block”\n“model block”\n“transformed parameters block”\n“generated quantities block”",
    "crumbs": [
      "Quiz 2",
      "Terminology review continued"
    ]
  },
  {
    "objectID": "w11_quiz2/topic01_terminology.html#metropolis-hastings",
    "href": "w11_quiz2/topic01_terminology.html#metropolis-hastings",
    "title": "Terminology review continued",
    "section": "Metropolis-Hastings",
    "text": "Metropolis-Hastings\n\nMH\nProposal (and contrasting it with SNIS’s proposal)\nSymmetric proposal\nTest function \\(g\\)\nTarget \\(\\pi\\)\nUn-normalized target \\(\\gamma\\)\nNormalization constant \\(Z\\)\nNotion of acceptance and rejection\n\nWhat happens in each case\nMH ratio (memorize that equation)\nAcceptance probability",
    "crumbs": [
      "Quiz 2",
      "Terminology review continued"
    ]
  },
  {
    "objectID": "w11_quiz2/topic01_terminology.html#mcmc-plots",
    "href": "w11_quiz2/topic01_terminology.html#mcmc-plots",
    "title": "Terminology review continued",
    "section": "MCMC plots",
    "text": "MCMC plots\n\nTrace plot\nPosterior histogram\nRank plot",
    "crumbs": [
      "Quiz 2",
      "Terminology review continued"
    ]
  },
  {
    "objectID": "w11_quiz2/topic01_terminology.html#basic-mcmc-theory",
    "href": "w11_quiz2/topic01_terminology.html#basic-mcmc-theory",
    "title": "Terminology review continued",
    "section": "Basic MCMC theory",
    "text": "Basic MCMC theory\n\nConsistency of MCMC\nIrreducibility\nMixing (informal definition is OK)",
    "crumbs": [
      "Quiz 2",
      "Terminology review continued"
    ]
  },
  {
    "objectID": "w11_quiz2/topic01_terminology.html#notion-of-unidentifiability",
    "href": "w11_quiz2/topic01_terminology.html#notion-of-unidentifiability",
    "title": "Terminology review continued",
    "section": "Notion of unidentifiability",
    "text": "Notion of unidentifiability\n\nInformal definition\nWriting an example model",
    "crumbs": [
      "Quiz 2",
      "Terminology review continued"
    ]
  },
  {
    "objectID": "w11_quiz2/topic01_terminology.html#goodness-of-fit",
    "href": "w11_quiz2/topic01_terminology.html#goodness-of-fit",
    "title": "Terminology review continued",
    "section": "Goodness-of-fit",
    "text": "Goodness-of-fit\n\nGoodness-of-fit\nPosterior predictive check",
    "crumbs": [
      "Quiz 2",
      "Terminology review continued"
    ]
  },
  {
    "objectID": "w11_quiz2/topic01_terminology.html#monte-carlo-standard-error-mcse",
    "href": "w11_quiz2/topic01_terminology.html#monte-carlo-standard-error-mcse",
    "title": "Terminology review continued",
    "section": "Monte Carlo Standard Error (MCSE)",
    "text": "Monte Carlo Standard Error (MCSE)\n\nMCSE\nEffective sample size (ESS)\nCentral Limit Theorem (CLT) for IID: writing the result and assumptions formally.\nCentral Limit Theorem (CLT) for Markov chain: writing the result formally and stating at least informally the conditions.\nAsymptotic variance\nBatch mean estimator",
    "crumbs": [
      "Quiz 2",
      "Terminology review continued"
    ]
  },
  {
    "objectID": "w11_quiz2/topic01_terminology.html#implementing-custom-distributions",
    "href": "w11_quiz2/topic01_terminology.html#implementing-custom-distributions",
    "title": "Terminology review continued",
    "section": "Implementing custom distributions",
    "text": "Implementing custom distributions\n\nLog scale computation.\nUnderflow.",
    "crumbs": [
      "Quiz 2",
      "Terminology review continued"
    ]
  },
  {
    "objectID": "w11_quiz2/topic01_terminology.html#data-collection-mechanisms",
    "href": "w11_quiz2/topic01_terminology.html#data-collection-mechanisms",
    "title": "Terminology review continued",
    "section": "Data collection mechanisms",
    "text": "Data collection mechanisms\n\nCensoring\nTruncation\nNon-ignorable missingness\nRao-blackwellization",
    "crumbs": [
      "Quiz 2",
      "Terminology review continued"
    ]
  },
  {
    "objectID": "w09_workflow/topic01_overview.html",
    "href": "w09_workflow/topic01_overview.html",
    "title": "Overview",
    "section": "",
    "text": "Overview of the Bayesian workflow.\nPointer to resources on generic data science workflows.\n\n\n\n\nEnd-to-end Bayesian analysis contains many steps. This week covers some of the key steps with an emphasis on software tools useful to accomplish them, while building connections with the concepts covered in the class so far.",
    "crumbs": [
      "Bayesian workflow",
      "Overview"
    ]
  },
  {
    "objectID": "w09_workflow/topic01_overview.html#outline",
    "href": "w09_workflow/topic01_overview.html#outline",
    "title": "Overview",
    "section": "",
    "text": "Overview of the Bayesian workflow.\nPointer to resources on generic data science workflows.\n\n\n\n\nEnd-to-end Bayesian analysis contains many steps. This week covers some of the key steps with an emphasis on software tools useful to accomplish them, while building connections with the concepts covered in the class so far.",
    "crumbs": [
      "Bayesian workflow",
      "Overview"
    ]
  },
  {
    "objectID": "w09_workflow/topic01_overview.html#bayesian-workflows",
    "href": "w09_workflow/topic01_overview.html#bayesian-workflows",
    "title": "Overview",
    "section": "Bayesian workflows",
    "text": "Bayesian workflows\nSome examples of steps involved in effective Bayesian analysis:\n\nPrior predictive checks: see logistic regression lecture.\nValidation of MCMC posterior approximation.\nValidation of the model.\nTechniques to identify software defects.\netc.\n\nOn a given problem, depending on the outcome of these steps, different tools will be used, forming a graph of techniques.\nJump to page 5 in Gelman et al., 2020 for a more detailed overview.",
    "crumbs": [
      "Bayesian workflow",
      "Overview"
    ]
  },
  {
    "objectID": "w09_workflow/topic01_overview.html#general-data-science-workflow-resources",
    "href": "w09_workflow/topic01_overview.html#general-data-science-workflow-resources",
    "title": "Overview",
    "section": "General data science workflow resources",
    "text": "General data science workflow resources\nWe focus in this course on the Bayesian-specific aspects of the data analysis workflow, but these should be combined with generic data analysis workflow practices:\n\nOrganizing your files in a project specific folder.\nUsing version control.\nCapturing library dependency versions for reproducibility.\nFollow good coding practice.\netc.\n\nSee, e.g., the software carpentry lessons.",
    "crumbs": [
      "Bayesian workflow",
      "Overview"
    ]
  },
  {
    "objectID": "w09_workflow/topic05_mcmc_ess.html",
    "href": "w09_workflow/topic05_mcmc_ess.html",
    "title": "Effective sample size",
    "section": "",
    "text": "Monte Carlo standard error (MCSE)\nEffective sample size (ESS)\nMathematical underpinning: central limit theorem for Markov chains\n\n\n\n\nAfter running an MCMC chain and convincing yourself the chain is mixing well, the next question is: how many digits are reliable when reporting an MCMC approximation (e.g., a posterior mean)?\nTo answer this question, we will encounter an MCMC version of the notion of effective sample size. This is distinct from SNIS’s ESS but has the same underlying intuition.",
    "crumbs": [
      "Bayesian workflow",
      "Effective sample size"
    ]
  },
  {
    "objectID": "w09_workflow/topic05_mcmc_ess.html#outline",
    "href": "w09_workflow/topic05_mcmc_ess.html#outline",
    "title": "Effective sample size",
    "section": "",
    "text": "Monte Carlo standard error (MCSE)\nEffective sample size (ESS)\nMathematical underpinning: central limit theorem for Markov chains\n\n\n\n\nAfter running an MCMC chain and convincing yourself the chain is mixing well, the next question is: how many digits are reliable when reporting an MCMC approximation (e.g., a posterior mean)?\nTo answer this question, we will encounter an MCMC version of the notion of effective sample size. This is distinct from SNIS’s ESS but has the same underlying intuition.",
    "crumbs": [
      "Bayesian workflow",
      "Effective sample size"
    ]
  },
  {
    "objectID": "w09_workflow/topic05_mcmc_ess.html#context",
    "href": "w09_workflow/topic05_mcmc_ess.html#context",
    "title": "Effective sample size",
    "section": "Context",
    "text": "Context\nTwo types of errors involved in Bayesian analysis based on Monte Carlo (see also: the two kinds of asymptotics):\n\nStatistical error: inherent uncertainty due to e.g., the fact that we have finite data.\nComputational error: additional error due to the fact that we use an approximation of the posterior instead of the exact posterior.\n\n\n\n\n\nThis page focuses on the second type of error.\nFull picture should include both (i.e., the first step of this joke can be taken seriously).\nInterestingly, the mathematical toolbox to study 2 is similar to the non-Bayesian toolbox (i.e. normal approximation) for studying 1.\n\n\nBayesian statisticians using central limit theorems?\nEarlier on, when building credible interval, (a Bayesian measure of statistical error) we avoided central limit theorems.\nQuestion: why are Bayesians OK with using central limit theorems for MCMC error analysis (i.e., for computational error)?",
    "crumbs": [
      "Bayesian workflow",
      "Effective sample size"
    ]
  },
  {
    "objectID": "w09_workflow/topic05_mcmc_ess.html#executive-version",
    "href": "w09_workflow/topic05_mcmc_ess.html#executive-version",
    "title": "Effective sample size",
    "section": "Executive version",
    "text": "Executive version\nHow many digits are reliable?\nSuppose you are approximating a posterior mean in Stan. We now show how to determine how many digits are reliable:\n\nprint the fit object,\nroughly twice1 the column se_mean provides the radius of a 95% confidence interval.\n\nExample: our simple doomsday model…\n\nsuppressPackageStartupMessages(require(rstan))\n\n\ndata { \n  real y; \n}\n\nparameters { \n  real&lt;lower=0, upper=5&gt; x;\n}\n\nmodel {\n  x ~ uniform(0, 5); \n  y ~ uniform(0, x); \n}\n\n\n\n\n\nfit = sampling(\n  doomsday,         \n  data = list(y = 0.06), \n  chains = 1,\n  seed = 1,\n  refresh = 0, \n  iter = 20000\n)\n\n\nfit\n\nInference for Stan model: anon_model.\n1 chains, each with iter=20000; warmup=10000; thin=1; \npost-warmup draws per chain=10000, total post-warmup draws=10000.\n\n      mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nx     1.16    0.03 1.29  0.07  0.18  0.57  1.76  4.56  1624    1\nlp__ -0.39    0.02 0.68 -2.44 -0.43 -0.12 -0.04 -0.01  1580    1\n\nSamples were drawn using NUTS(diag_e) at Tue Mar 12 22:45:46 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\nQuestion: construct a 95% confidence interval for the posterior mean of \\(X\\). Is the true value contained in the interval?",
    "crumbs": [
      "Bayesian workflow",
      "Effective sample size"
    ]
  },
  {
    "objectID": "w09_workflow/topic05_mcmc_ess.html#mathematical-underpinnings",
    "href": "w09_workflow/topic05_mcmc_ess.html#mathematical-underpinnings",
    "title": "Effective sample size",
    "section": "Mathematical underpinnings",
    "text": "Mathematical underpinnings\nWe answer two questions:\n\nHow can we compute Monte Carlo Standard Errors (MCSE) (i.e., numbers such as in se_mean)?\nWhat underlying theory justifies that computation?\n\nAlong the way we define the notion of Effective Sample Size (ESS) for MCMC.\n\nBackground\n\nRecall the central limit theorem (CLT) for independent and identically distributed (i.i.d.) random variables:\n\nif some random variables \\(V_i\\)’s are i.i.d. and\neach has finite variance, then we have2 \\[\\sqrt{n}(\\bar V - \\mu) \\to \\mathcal{N}(0, \\operatorname{SD}[V]), \\tag{1}\\] where \\(\\bar V = \\frac{1}{n} \\sum_{i=1}^n V_i\\) and \\(\\mu = \\mathbb{E}[V]\\).\n\nFrom the central limit theorem, recall that standard frequentist arguments give: \\[\\mathbb{P}(\\mu \\in [\\bar V \\pm 1.96 \\text{SE}]) \\approx 95\\%, \\tag{2}\\] where: the Standard Error (SE) is given by \\(\\text{SE} = \\operatorname{SD}[V] / \\sqrt{n}\\).\n\n\n\nCentral limit theorem (CLT) for Markov chains\n\nWe would like to have something like Equation 2 for our MCMC algorithm,\nhowever, the samples from MCMC have dependence, they are not i.i.d….\n… so we cannot use the above i.i.d. central limit theorem.\nBut fortunately there is generalization of the central limit theorem that applies!\nNamely: The central limit theorem for Markov chains.\n\nDefinition: the random variables \\(X^{(1)}, X^{(2)}, \\dots\\) are called a Markov chain if they admit the following “chain” graphical model.\n\n\n\n\nHere we state a version of the CLT for Markov chains specialized to our situation:\n\nLet \\(X^{(1)}, X^{(2)}, \\dots\\) denote the states visited by an MH algorithm.\n\\(\\mu = \\int x \\pi(x) \\mathrm{d}x\\) is the quantity we seek to approximate,\n\ni.e., a posterior mean.\nRecall \\(\\pi(x) = \\gamma(x) / Z\\), where \\(\\gamma(x) = p(x, y)\\) and \\(Z = p(y)\\).\n\nLet \\(\\bar X\\) denote our MCMC estimator, i.e., the average of the samples.\n\n\nTheorem: assuming \\(\\sigma^2 = \\int (x - \\mu)^2 \\pi(x) \\mathrm{d}x &lt; \\infty\\) (in our context: posterior variance is finite) and under appropriate fast mixing conditions,3 \\[\\sqrt{n}(\\bar X - \\mu) \\to \\mathcal{N}(0, \\sigma_a), \\tag{3}\\] where the constant \\(\\sigma^2_a &gt; 0\\) is known as the asymptotic variance.\n\n\nAsymptotic variance\nNotice: there is a difference between the limiting distributions in the i.i.d. and Markov CLTs (Equation 1 and Equation 3)!\n\nFor i.i.d. CLT: variance of the limiting distribution is equal to the variance of \\(X_1\\).\nFor Markov CLT: we left the variance of the limiting distribution more vague (\\(\\sigma_a^2\\)).\n\nIntuition: because of the dependences between MCMC iterations, the noise of the approximation can be larger compared to the i.i.d. setting.4\n\n\nEffective sample size\nThe MCMC Effective Sample Size (ESS) is an answer to the following:\nQuestion: How many i.i.d. samples \\(n_e\\) would be equivalent5 to my \\(M\\) samples obtained from MCMC?\n\n\n\n\n\n\nClick for hint\n\n\n\n\n\nApply the following formula, \\(\\operatorname{Var}[a X + b] = a^2 \\operatorname{Var}[X]\\) to both the CLT for i.i.d. samples, and then the CLT for Markov chains.\n\n\n\n\n\nEstimating the asymptotic variance with many chains\n\nThere are many ways to estimate the asymptotic variance (see references below).\nWe start here with the simplest possible scenario:\n\nsuppose we have \\(C\\) independent chains\neach with \\(S\\) MCMC samples.\n\nSince we have \\(C\\) chains, we get \\(C\\) different Monte Carlo estimators:\n\n\\(E_1, E_2, \\dots, E_C\\).\nDenote also the overall estimator by: \\[E = \\frac{1}{C} \\sum_{c=1}^C E_c.\\]\n\nFirst, by the CLT for Markov chains: for any \\(c\\), \\[S \\operatorname{Var}[E_c] \\approx \\sigma^2_a. \\tag{4}\\]\nSecond, since the \\(E_1, \\dots, E_C\\) are i.i.d., \\[\\operatorname{Var}[E_c] \\approx \\frac{1}{C} \\sum_{c=1}^C (E_c - E)^2. \\tag{5}\\]\n\nQuestion: combine Equation 4 and Equation 5 to obtain an estimator for the asymptotic variance.\n\n\nEstimating the asymptotic variance with one chain\n\nView a trace of length \\(M\\) as \\(C\\) subsequent batches of length \\(S\\) for \\(M = C \\cdot S\\).\nCommon choice: \\(C = S = \\sqrt{M}\\).\nThis is known as the batch mean estimator.",
    "crumbs": [
      "Bayesian workflow",
      "Effective sample size"
    ]
  },
  {
    "objectID": "w09_workflow/topic05_mcmc_ess.html#additional-references",
    "href": "w09_workflow/topic05_mcmc_ess.html#additional-references",
    "title": "Effective sample size",
    "section": "Additional references",
    "text": "Additional references\n\nSee Flegal, 2008 for a nice exposition on the technique described here for estimating the asymptotic variance, known as the batch mean estimator, as well as many other methods for asymptotic variance estimation.\nSee Vats et al., 2017 for a multivariate generalization of the effective sample size.",
    "crumbs": [
      "Bayesian workflow",
      "Effective sample size"
    ]
  },
  {
    "objectID": "w09_workflow/topic05_mcmc_ess.html#footnotes",
    "href": "w09_workflow/topic05_mcmc_ess.html#footnotes",
    "title": "Effective sample size",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhere does “twice” come from? More precisely, it is \\(1.96\\), which comes from the quantile function of the normal evaluated at \\(100\\% - 5\\%/2\\):\n\nqnorm(1-0.025)\n\n[1] 1.959964\n\n\n↩︎\nIn this page, \\(\\to\\) refers to convergence in distribution.↩︎\nSeveral different conditions can be used to state the central limit theorem for Markov chains, see Jones, 2004 for a review. For example, Corollary 4 in that review can be used since the MH algorithm is reversible as we will see soon. Corollary 4 requires the following conditions: reversibility, a finite variance, \\(\\sigma^2 &lt; \\infty\\) (reasonable, as the CLT for i.i.d. requires this as well), geometric ergodicity (which we discussed in a footnote of MCMC diagnostics), and Harris ergodicity, which is more of a technical condition that would be covered in a Markov chain course.↩︎\nInterestingly, the noise can also be lower in certain situations! These are called super-efficient MCMC algorithms. Consider for example an MH algorithm over the state space \\(\\{1, 2, 3\\}\\) that proposes, at each iteration, uniformly over \\(\\{1, 2, 3\\}\\) while excluding its current position. It can be shown that this algorithm will have lower variance compared to the i.i.d. simple Monte Carlo algorithm.↩︎\nBy equivalent, we mean: “have the same variance”.↩︎",
    "crumbs": [
      "Bayesian workflow",
      "Effective sample size"
    ]
  },
  {
    "objectID": "w09_workflow/topic03_checking_correctness.html",
    "href": "w09_workflow/topic03_checking_correctness.html",
    "title": "Checking correctness",
    "section": "",
    "text": "Using calibration to check correctness of model code.\n\n\n\n\nCode implementing Bayesian models can get complex in real applications. Complex code invariably means software defects (bugs) will creep in. We review a powerful method to detect bugs in Bayesian inference software.\nThe topic covered here is typically known as “software testing”. However we avoid the terminology “testing” here as it is already used in the statistical literature.",
    "crumbs": [
      "Bayesian workflow",
      "Checking correctness"
    ]
  },
  {
    "objectID": "w09_workflow/topic03_checking_correctness.html#outline",
    "href": "w09_workflow/topic03_checking_correctness.html#outline",
    "title": "Checking correctness",
    "section": "",
    "text": "Using calibration to check correctness of model code.\n\n\n\n\nCode implementing Bayesian models can get complex in real applications. Complex code invariably means software defects (bugs) will creep in. We review a powerful method to detect bugs in Bayesian inference software.\nThe topic covered here is typically known as “software testing”. However we avoid the terminology “testing” here as it is already used in the statistical literature.",
    "crumbs": [
      "Bayesian workflow",
      "Checking correctness"
    ]
  },
  {
    "objectID": "w09_workflow/topic03_checking_correctness.html#from-goodness-of-fit-to-correctness-check",
    "href": "w09_workflow/topic03_checking_correctness.html#from-goodness-of-fit-to-correctness-check",
    "title": "Checking correctness",
    "section": "From goodness-of-fit to correctness check",
    "text": "From goodness-of-fit to correctness check\n\nIn the last page, we developed a procedure for goodness-of-fit.\nHowever we identified several factors that can lead to a “warning”.\n\nQuestion: how can we modify last page’s check to exclude “model mis-specification” as a potential cause?",
    "crumbs": [
      "Bayesian workflow",
      "Checking correctness"
    ]
  },
  {
    "objectID": "w09_workflow/topic03_checking_correctness.html#additional-references",
    "href": "w09_workflow/topic03_checking_correctness.html#additional-references",
    "title": "Checking correctness",
    "section": "Additional references",
    "text": "Additional references\nNote: there are more sophisticated method for checking correctness of MCMC code, see for example:\n\nThe Exact Invariance Test in Bouchard-Côté, 2022, Section 10.5.\nThis line of work was initiated in Geweke, 2004. Reverse citation search on that article provides a comprehensive view of the literature on checking correctness of MCMC algorithms.",
    "crumbs": [
      "Bayesian workflow",
      "Checking correctness"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic04_invariance.html",
    "href": "w12_mcmc2/topic04_invariance.html",
    "title": "Invariance: intuition",
    "section": "",
    "text": "Intuition behind invariance\nInvariance as a fixed point\n\n\n\n\nInvariance is the key condition needed to establish a law of large numbers for Markov chains. Hence it is useful to build some intuition on what it means.",
    "crumbs": [
      "MCMC Hacking",
      "Invariance: intuition"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic04_invariance.html#outline",
    "href": "w12_mcmc2/topic04_invariance.html#outline",
    "title": "Invariance: intuition",
    "section": "",
    "text": "Intuition behind invariance\nInvariance as a fixed point\n\n\n\n\nInvariance is the key condition needed to establish a law of large numbers for Markov chains. Hence it is useful to build some intuition on what it means.",
    "crumbs": [
      "MCMC Hacking",
      "Invariance: intuition"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic04_invariance.html#analogy-fixed-point-iteration",
    "href": "w12_mcmc2/topic04_invariance.html#analogy-fixed-point-iteration",
    "title": "Invariance: intuition",
    "section": "Analogy: fixed point iteration",
    "text": "Analogy: fixed point iteration\nGoal here: gaining some intuition on what is the notion of \\(\\pi\\)-invariance, using an analogy.\n\nFixed point iteration\n\n\n\n\nYou are given a function \\(f(x)\\)\nGame:\n\nPick any starting point \\(x_1\\) (e.g. in figure, \\(x_1 = -1\\))\nCompute \\(x_2 = f(x_1)\\) (e.g. in figure, \\(x_2 = 0.5\\))\nCompute \\(x_3 = f(x_2)\\)\nCompute \\(x_4 = f(x_3)\\)\netc.\n\nCan we predict where will \\(x_n\\) end up after an infinite number of iterations?\n\nHint: try to understand why the figure has a blue line \\(y = x\\).\n\n\n\n\nFixed points\nSurprise: sometimes, we can predict where \\(\\lim_{n\\to\\infty} x_n\\) ends up!\nDefinition: a fixed point of a function \\(f\\) is a point \\(x\\) such that \\(f(x) = x\\).\n\n\n\nQuestion: what are the fixed point in this picture?\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nThe 3 yellow points. These are the points where the curve intersect the line \\(y = x\\).\n\n\n\nExample:\n\nlook at the fixed point iteration picture more closely,\nnote that indeed here we end up at a fixed point (blue diagonal line shows the linear function \\(y = x\\))\n\n\n\n\nBack to Markov chain\nIntuition: \\(\\pi\\)-invariance means “\\(\\pi\\) acts like a fixed-point of \\(K\\).”\nConnection:\n\nreplace “points” by “probability distributions”,\nreplace “function application” by “simulating the Markov chain for 1 step”,\nreplace “fixed point” by “\\(\\pi\\)-invariance”.",
    "crumbs": [
      "MCMC Hacking",
      "Invariance: intuition"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic06_mh_invariance.html",
    "href": "w12_mcmc2/topic06_mh_invariance.html",
    "title": "MH is invariant",
    "section": "",
    "text": "Invariance of MH\n\n\n\n\nWe can now complete the plan laid earlier.",
    "crumbs": [
      "MCMC Hacking",
      "MH is invariant"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic06_mh_invariance.html#outline",
    "href": "w12_mcmc2/topic06_mh_invariance.html#outline",
    "title": "MH is invariant",
    "section": "",
    "text": "Invariance of MH\n\n\n\n\nWe can now complete the plan laid earlier.",
    "crumbs": [
      "MCMC Hacking",
      "MH is invariant"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic06_mh_invariance.html#invariance-of-symmetric-proposal-mh",
    "href": "w12_mcmc2/topic06_mh_invariance.html#invariance-of-symmetric-proposal-mh",
    "title": "MH is invariant",
    "section": "Invariance of symmetric proposal MH",
    "text": "Invariance of symmetric proposal MH\n\nRecall: invariance is a synonym for “satisfying the global balance equation”\nStrategy\n\nSince detailed balance implies global balance,\nIf we can prove detailed balance we are done, i.e. it is enough to show \\[\\pi(x) K(x' | x) = \\pi(x') K(x | x'). \\tag{1}\\]\n\n\n\n\n\nProof:\n\nStart with the subcase \\(x' \\neq x\\).\nWe know from our previous calculation that: \\[K(x' | x) = q(x'|x) \\alpha(x, x'),\\] where \\(\\alpha(x, x') = \\min(1, r(x, x'))\\) and \\(r(x, x') = \\gamma(x') / \\gamma(x) = \\pi(x') / \\pi(x)\\).\nHence: the left hand side of Equation 1 is: \\[\\begin{align*}\n\\pi(x) K(x' | x) &= \\pi(x) q(x'|x) \\alpha(x, x') \\;\\;\\text{(previous calculation)} \\\\\n&= \\pi(x) q(x'|x) \\min(1, r(x, x'))  \\;\\;\\text{(by definition)} \\\\\n&= \\pi(x) q(x'|x) \\min(1, \\pi(x') / \\pi(x)) \\;\\;\\text{(by definition)}. \\\\\n\\end{align*}\\]\n\nNow note that when \\(a \\ge 0\\), we have \\(a \\min(1, b) = \\min(a, ab)\\), hence using this identity with \\(a = \\pi(x) \\ge 0\\),\n\\[\\begin{align*}\n&=  q(x'|x) \\min(\\pi(x), \\pi(x')).\n\\end{align*}\\]\nNow the last expression has the nice property that we can swap \\(x\\) and \\(x'\\), since:\n\n\\(\\min(a, b) = \\min(b, a)\\),\n\\(q(x'|x) = q(x | x')\\) by the symmetric proposal assumption.\n\nHence, doing all the steps in reverse with \\(x\\) and \\(x'\\) permuted, we get \\[\\pi(x) K(x' | x) = \\pi(x') K(x | x'),\\] i.e., detailed balance.\nExercise: finish the argument by considering the case where \\(x' = x\\).",
    "crumbs": [
      "MCMC Hacking",
      "MH is invariant"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic09_why_hmc.html",
    "href": "w12_mcmc2/topic09_why_hmc.html",
    "title": "Why HMC?",
    "section": "",
    "text": "Motivation behind Hamiltonian Monte Carlo (HMC)\nComputational cost of MCMC methods\nComputational cost scaling for popular Monte Carlo methods on normal targets\n\n\n\n\nWe have used Stan heavily in the second half of the course. Stan uses an algorithm called NUTS which is based on HMC.\nWe will now spend time understanding HMC. This will illustrate two important ideas used to construct advanced MCMC methods (involutions, augmentation).\nWe start by motivating HMC.",
    "crumbs": [
      "MCMC Hacking",
      "Why HMC?"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic09_why_hmc.html#outline",
    "href": "w12_mcmc2/topic09_why_hmc.html#outline",
    "title": "Why HMC?",
    "section": "",
    "text": "Motivation behind Hamiltonian Monte Carlo (HMC)\nComputational cost of MCMC methods\nComputational cost scaling for popular Monte Carlo methods on normal targets\n\n\n\n\nWe have used Stan heavily in the second half of the course. Stan uses an algorithm called NUTS which is based on HMC.\nWe will now spend time understanding HMC. This will illustrate two important ideas used to construct advanced MCMC methods (involutions, augmentation).\nWe start by motivating HMC.",
    "crumbs": [
      "MCMC Hacking",
      "Why HMC?"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic09_why_hmc.html#why-is-hmc-popular-the-short-answer",
    "href": "w12_mcmc2/topic09_why_hmc.html#why-is-hmc-popular-the-short-answer",
    "title": "Why HMC?",
    "section": "Why is HMC popular: the short answer",
    "text": "Why is HMC popular: the short answer\nFor many models, HMC is less computationally costly than the basic MH algorithm we saw earlier.",
    "crumbs": [
      "MCMC Hacking",
      "Why HMC?"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic09_why_hmc.html#notion-of-computational-cost",
    "href": "w12_mcmc2/topic09_why_hmc.html#notion-of-computational-cost",
    "title": "Why HMC?",
    "section": "Notion of computational cost",
    "text": "Notion of computational cost\nWe first have to define what we mean by an MCMC algorithm being less “costly” than another one.\nQuestion: is the time to compute one iteration (in e.g., in seconds or FLOPS) a reasonable notion of computational cost for MCMC algorithms?\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nNo, it is incomplete.\nFor example, according to this incomplete metric, an algorithm with proposal \\(q(x' | x) = \\mathbb{1}[x = x']\\), would be really fast (can skip the MH ratio calculation!)…\n\n… but is useless because its effective sample size will be 1 no matter how long you run it.\n\nWhat we care about: how long does it take to get to a certain Monte Carlo standard error.\n\ni.e.: seconds per effective sample (seconds/ESS; lower is better).",
    "crumbs": [
      "MCMC Hacking",
      "Why HMC?"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic09_why_hmc.html#scaling-of-monte-carlo-methods-as-a-function-of-dimensionality",
    "href": "w12_mcmc2/topic09_why_hmc.html#scaling-of-monte-carlo-methods-as-a-function-of-dimensionality",
    "title": "Why HMC?",
    "section": "Scaling of Monte Carlo methods as a function of dimensionality",
    "text": "Scaling of Monte Carlo methods as a function of dimensionality\nTerminology: to differentiate HMC from “the basic MH algorithm we saw earlier”, the latter is often called random walk MH (to avoid confusion since HMC is just an MH algorithm with a fancy proposal).\nProposition: (Beskos et al., 2010) For \\(d\\)-dimensional i.i.d. normal targets,\n\noptimally tuned HMC’s cost (seconds/ESS) scales in \\(O(d^{5/4})\\), whereas,\noptimally tuned random walk MH’ cost (seconds/ESS) scales in \\(O(d^2)\\).\n\nNotes:\n\nThis means as \\(d\\) increases, there is an increasing performance gap in favour of HMC.\nWe would not use HMC for normal targets…\n\n…but many real models are approximately normal,\nso this result is a useful rule of thumb.",
    "crumbs": [
      "MCMC Hacking",
      "Why HMC?"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic13_autodiff.html",
    "href": "w12_mcmc2/topic13_autodiff.html",
    "title": "Autodiff",
    "section": "",
    "text": "Reverse mode automatic differentiation (autodiff)\nUsing Stan as a stand-alone autodiff system\n\n\n\n\nAs we just saw, HMC requires computing the gradient (vector of partial derivatives) at each iteration. But notice we did not have to manually derive the gradient when writing Stan models.\nWhy? Thanks to autodiff!",
    "crumbs": [
      "MCMC Hacking",
      "Autodiff"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic13_autodiff.html#outline",
    "href": "w12_mcmc2/topic13_autodiff.html#outline",
    "title": "Autodiff",
    "section": "",
    "text": "Reverse mode automatic differentiation (autodiff)\nUsing Stan as a stand-alone autodiff system\n\n\n\n\nAs we just saw, HMC requires computing the gradient (vector of partial derivatives) at each iteration. But notice we did not have to manually derive the gradient when writing Stan models.\nWhy? Thanks to autodiff!",
    "crumbs": [
      "MCMC Hacking",
      "Autodiff"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic13_autodiff.html#automatic-differentiation",
    "href": "w12_mcmc2/topic13_autodiff.html#automatic-differentiation",
    "title": "Autodiff",
    "section": "Automatic differentiation",
    "text": "Automatic differentiation\n\nA Stan program specifies \\(\\log \\gamma(x)\\), a function from \\(\\mathbb{R}^d\\) to \\(\\mathbb{R}\\).\nStan knows how to differentiate “primitive operations” such as \\(+, \\cdots, \\exp, \\dots\\)\nStan knows how to recurse based on the chain rule: \\((f\\circ g)' = (f' \\circ g)  g'\\).\n\nReverse-mode autodiff: see wikipedia for details; in summary,\n\nStan uses known derivatives for primitive operations, combined with chain rule to compute \\(\\nabla \\log \\gamma\\) automatically.\nThe “reverse-mode” variant of autodiff allows us to do that computation with the same running time as computing \\(\\log \\gamma\\).1",
    "crumbs": [
      "MCMC Hacking",
      "Autodiff"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic13_autodiff.html#using-stan-as-a-stand-alone-autodiff-system",
    "href": "w12_mcmc2/topic13_autodiff.html#using-stan-as-a-stand-alone-autodiff-system",
    "title": "Autodiff",
    "section": "Using Stan as a stand-alone autodiff system",
    "text": "Using Stan as a stand-alone autodiff system\nIf you just need just the gradient of the log density of your Stan model and not HMC (e.g., to develop new inference algorithms), use BridgeStan (bridges Stan with R, Julia, Rust, etc).",
    "crumbs": [
      "MCMC Hacking",
      "Autodiff"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic13_autodiff.html#footnotes",
    "href": "w12_mcmc2/topic13_autodiff.html#footnotes",
    "title": "Autodiff",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn contrast, both forward mode autodiff and numerical differentiation would be \\(d\\) time slower at computing \\(\\nabla \\log \\gamma\\) compared to \\(\\log \\gamma\\).↩︎",
    "crumbs": [
      "MCMC Hacking",
      "Autodiff"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic01_intro.html",
    "href": "w12_mcmc2/topic01_intro.html",
    "title": "Overview",
    "section": "",
    "text": "By “MCMC hacking,” I mean learning the tools to:\n\nPredict when some MCMC algorithm will fail.\nStrategies to fix it.\nHow to write a new MCMC algorithm.\nHow to debut your MCMC code.",
    "crumbs": [
      "MCMC Hacking",
      "Overview"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic01_intro.html#this-weeks-outline",
    "href": "w12_mcmc2/topic01_intro.html#this-weeks-outline",
    "title": "Overview",
    "section": "",
    "text": "By “MCMC hacking,” I mean learning the tools to:\n\nPredict when some MCMC algorithm will fail.\nStrategies to fix it.\nHow to write a new MCMC algorithm.\nHow to debut your MCMC code.",
    "crumbs": [
      "MCMC Hacking",
      "Overview"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic01_intro.html#rationale",
    "href": "w12_mcmc2/topic01_intro.html#rationale",
    "title": "Overview",
    "section": "Rationale",
    "text": "Rationale\nWe need to write new MCMC algorithm because there are situations that Stan/other PPLs cannot handle automatically.\nSome examples:\n\nPhylogenetic tree inference\nAnalysis of network data\nProto-language reconstruction\nMultiple sequence alignment",
    "crumbs": [
      "MCMC Hacking",
      "Overview"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic12_hmc_as_involution.html",
    "href": "w12_mcmc2/topic12_hmc_as_involution.html",
    "title": "HMC as involution",
    "section": "",
    "text": "Inspiration from continuous time dynamics\nLeap-frog integrator\n“Flip trick”\n\n\n\n\nWe can complete our goal of writing formally HMC, as a special case of the MH with deterministic proposal we just saw.",
    "crumbs": [
      "MCMC Hacking",
      "HMC as involution"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic12_hmc_as_involution.html#outline",
    "href": "w12_mcmc2/topic12_hmc_as_involution.html#outline",
    "title": "HMC as involution",
    "section": "",
    "text": "Inspiration from continuous time dynamics\nLeap-frog integrator\n“Flip trick”\n\n\n\n\nWe can complete our goal of writing formally HMC, as a special case of the MH with deterministic proposal we just saw.",
    "crumbs": [
      "MCMC Hacking",
      "HMC as involution"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic12_hmc_as_involution.html#plan",
    "href": "w12_mcmc2/topic12_hmc_as_involution.html#plan",
    "title": "HMC as involution",
    "section": "Plan",
    "text": "Plan\n\nHow to build and involution for “rolling a ball for time \\(t\\) starting at current location and momentum”?\nStart with an exact description of the ball’s motion (in continuous time).\nDiscretize time carefully to be able to compute trajectory while maintaining the involution property.",
    "crumbs": [
      "MCMC Hacking",
      "HMC as involution"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic12_hmc_as_involution.html#inspiration-from-physics",
    "href": "w12_mcmc2/topic12_hmc_as_involution.html#inspiration-from-physics",
    "title": "HMC as involution",
    "section": "Inspiration from physics",
    "text": "Inspiration from physics\n\nPhysics gives us precise expression for quantifying the notion of “rolling a ball”.\nAs often, it is easier to describe how things change rather than how thing are,\n\ni.e. to provide an ordinary differential equation (ODE).\n\nODE for the continuous time evolution of frictionless ball’s position and momentum \\((x^{(t)}, p^{(t)})\\): \\[\\begin{align*}\nx' = \\frac{\\mathrm{d}x^{(t)}}{\\mathrm{d}t} &= p \\;\\;\\text{(in a small time interval the change in position is equal to the momentum)} \\\\\np' = \\frac{\\mathrm{d}p^{(t)}}{\\mathrm{d}t} &= - \\nabla U(x) \\;\\;\\text{(the velocity decreases proportionally to the steepness of the landscape)}.\n\\end{align*}\\]\n\n\n\n\nRecall: the gradient \\(\\nabla U(x) = (\\partial U/\\partial x_1, \\dots, \\partial U/\\partial x_d)\\) is a vector pointing towards the direction of steepest ascent.",
    "crumbs": [
      "MCMC Hacking",
      "HMC as involution"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic12_hmc_as_involution.html#flip-trick",
    "href": "w12_mcmc2/topic12_hmc_as_involution.html#flip-trick",
    "title": "HMC as involution",
    "section": "Flip trick",
    "text": "Flip trick\n\nAt the very end of the trajectory, in the analysis we flip the momentum: \\[T_\\text{flip}(x, p) = (x, -p).\\]\n\n\n\n\n\nThis will allow us to prove the involution property:\n\nAfter the momentum sign flip,\nthe ball will retrace its step!\n\nSince we alternate between an HMC kernel and sampling a new momentum…\n\n…this flipped momentum will be overwritten right away,\ni.e. this is just a device for theoretical analysis in the vanilla HMC algorithm.",
    "crumbs": [
      "MCMC Hacking",
      "HMC as involution"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic12_hmc_as_involution.html#discretization",
    "href": "w12_mcmc2/topic12_hmc_as_involution.html#discretization",
    "title": "HMC as involution",
    "section": "Discretization",
    "text": "Discretization\n\nwe typically do not have close form solution for the ODE of motion\nwe want to discretize it in a way that preserves the involution property\nsolution: “leap frog integrator”\n\n\nNotation\n\nlet \\(\\epsilon\\) denote a small time interval (discretization),\nlet \\(L\\) denote the number of small steps we will use,\n\nhence, \\(t = \\epsilon L\\).\n\n\n\n\nLeap frog integrator\nKey idea: update only one of \\(\\{x, p\\}\\) at the time to get triangular Jacobian matrices (easier to analyze).\nDrift operator\n\nLet the current momentum move the current position a bit.\nDiscretization of \\(x' = p\\): \\[T_\\text{drift}(x, p) = (x + \\epsilon p, p).\\]\n\nQuestion: compute the Jacobian determinant of \\(T_\\text{drift}\\).\nKick operator\n\nLet the steepness of the landscape at current position update the momentum a bit.\nDiscretization of \\(p' = - \\nabla U\\): \\[T_\\text{kick}(x, p) = \\left(x, p + \\frac{\\epsilon}{2} \\nabla \\log \\pi(x)\\right).\\]\n\n\n# example: a normal\n# This function return gradient_x of log pi(x)\ngradient = function(x) {\n  -2*x # = - 0.5 x^2 / sigma^2, i.e. a normal with variance sigma^2 = 0.5\n}\n\nepsilon = 0.1\n\nkick = function(s) {\n  x = s[[1]]\n  p = s[[2]]\n  c(x, p + epsilon * gradient(x) / 2)\n}\n\nflip = function(s) {\n  x = s[[1]]\n  p = s[[2]]\n  c(x, -p)\n}\n\ninitial = c(1.1, 2.3) \n\n# flip o kick o flip o kick\n#   where 'o' denotes 'function composition',\n#   which recall are read right to left\nnewpoint = kick(initial) \nflipped = flip(newpoint)\ntraceback = kick(flipped)\nfipped_again = flip(traceback)\n\ninitial\n\n[1] 1.1 2.3\n\nnewpoint\n\n[1] 1.10 2.19\n\nflipped\n\n[1]  1.10 -2.19\n\ntraceback\n\n[1]  1.1 -2.3\n\nfipped_again\n\n[1] 1.1 2.3\n\n\nAh! Ah! we come back where we started!\nI.e., the code above illustrates that \\(T_\\text{flip}\\circ T_\\text{kick}\\circ T_\\text{flip}\\circ T_\\text{kick}= I\\), which is easy to show and a building block for showing that the full proposal is an involution. (And a great way to check correctness of involution-based MH algorithms!)\nWe have the same property for drifts:\n\ndrift = function(s) {\n  x = s[[1]]\n  p = s[[2]]\n  c(x + epsilon * p, p)\n}\n\n\ninitial = c(1.1, 2.3) \n\n# flip o kick o flip o kick\nnewpoint = drift(initial) \nflipped = flip(newpoint)\ntraceback = drift(flipped)\nfipped_again = flip(traceback)\n\ninitial\n\n[1] 1.1 2.3\n\nnewpoint\n\n[1] 1.33 2.30\n\nflipped\n\n[1]  1.33 -2.30\n\ntraceback\n\n[1]  1.1 -2.3\n\nfipped_again\n\n[1] 1.1 2.3\n\n\nLeap frog:\n\nAlternate between kick and drift.\nDo so in a palindromic fashion to obtain involution after flip. \\[T_\\text{leap-frog}= T_\\text{kick}\\circ T_\\text{drift}\\circ T_\\text{kick}.\\]\n\n\nL = 5\n\nhmc_proposal = function(s) {\n  for (i in 1:L) {\n    s = kick(s)\n    s = drift(s)\n    s = kick(s)\n  }\n  flip(s)\n}\n\n\nnewpoint = hmc_proposal(initial) \napply_twice = hmc_proposal(newpoint)\n\ninitial \n\n[1] 1.1 2.3\n\nnewpoint\n\n[1]  1.8957642 -0.7389151\n\napply_twice\n\n[1] 1.1 2.3\n\n\nProposition: \\(T = T_\\text{flip}\\circ T_\\text{leap-frog}^L\\) is an involution, where \\(T^n = T^{n-1} \\circ T\\).\nProof idea: use the property \\(T_\\text{flip}\\circ T_\\text{kick}\\circ T_\\text{flip}\\circ T_\\text{kick}= I\\) we encountered (and similar one for the drift) and the fact that \\(T_\\text{leap-frog}^l\\) is a palindrome (can be read both ways, like “madam” or “nurses run”).\nProposition: \\(T = T_\\text{flip}\\circ T_\\text{leap-frog}^L\\) has Jacobian one.\nProof idea: show that each constituent has Jacobian determinant one, so the product of these is also one.",
    "crumbs": [
      "MCMC Hacking",
      "HMC as involution"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic12_hmc_as_involution.html#putting-it-all-together",
    "href": "w12_mcmc2/topic12_hmc_as_involution.html#putting-it-all-together",
    "title": "HMC as involution",
    "section": "Putting it all together",
    "text": "Putting it all together\nRecall the high-level organization of HMC is the deterministic alternation of:\n\na complicated kernel \\(K_\\text{hmc}\\): “rolling the ball”,\na simple kernel \\(K_\\text{gibbs}\\): sampling the momentum.\n\nWe can now define \\(K_\\text{hmc}\\): it it the deterministic proposal MH with proposal \\(T = T_\\text{flip}\\circ T_\\text{leap-frog}^L\\).",
    "crumbs": [
      "MCMC Hacking",
      "HMC as involution"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic12_hmc_as_involution.html#reference",
    "href": "w12_mcmc2/topic12_hmc_as_involution.html#reference",
    "title": "HMC as involution",
    "section": "Reference",
    "text": "Reference\nSee Neal, 2012 for an in-depth tutorial on HMC, in particular how to tune it.",
    "crumbs": [
      "MCMC Hacking",
      "HMC as involution"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic06_mix.html",
    "href": "w12_mcmc2/topic06_mix.html",
    "title": "Kernel mixtures",
    "section": "",
    "text": "Mixture of MCMC kernels\nCode implementation\nMixtures preserve invariance\n\n\n\n\nTo prove invariance of MCMC algorithms, a common strategy is to “break down” the algorithm into simpler pieces, and to first prove invariance of each of the pieces.\nThis pages shows one way we can establish invariance of the overall algorithm “for free” once we have established invariance of each of its parts.\nNote: we used mixtures in the context of model building. Here we use the same mathematical construction (“mixtures”), but in a different context, i.e. constructing and analyzing MCMC algorithms instead of model building.",
    "crumbs": [
      "MCMC Hacking",
      "Kernel mixtures"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic06_mix.html#outline",
    "href": "w12_mcmc2/topic06_mix.html#outline",
    "title": "Kernel mixtures",
    "section": "",
    "text": "Mixture of MCMC kernels\nCode implementation\nMixtures preserve invariance\n\n\n\n\nTo prove invariance of MCMC algorithms, a common strategy is to “break down” the algorithm into simpler pieces, and to first prove invariance of each of the pieces.\nThis pages shows one way we can establish invariance of the overall algorithm “for free” once we have established invariance of each of its parts.\nNote: we used mixtures in the context of model building. Here we use the same mathematical construction (“mixtures”), but in a different context, i.e. constructing and analyzing MCMC algorithms instead of model building.",
    "crumbs": [
      "MCMC Hacking",
      "Kernel mixtures"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic06_mix.html#mixture-of-mcmc-kernels",
    "href": "w12_mcmc2/topic06_mix.html#mixture-of-mcmc-kernels",
    "title": "Kernel mixtures",
    "section": "Mixture of MCMC kernels",
    "text": "Mixture of MCMC kernels\n\nExample\nLet us use the simple beta-binomial example from question 3 of exercise 7.\nRecall the target is:\n\n# prior: Beta(alpha, beta)\nalpha = 1\nbeta = 2 \n\n# observations: binomial draws\nn_successes = 3 \nn_trials = 3\n\ngamma_beta_binomial = function(p) {\n  if (p &lt; 0 || p &gt; 1) return(0.0)\n  dbeta(p, alpha, beta) * dbinom(x = n_successes, size = n_trials, prob = p)\n}\n\n\n\nIntuition\n\nWe will write an MCMC algorithm for the above beta-binomial model (\\(x\\) in math notation will be p in the above code).\nSuppose we want to use a symmetric MH algorithm with a normal proposal…\n….but we are not sure if we should use a proposal with standard deviation 1 or 2.\nLet:\n\n\\(K_1\\) denote the MH kernel with proposal standard deviation 1, and\n\\(K_2\\), the MH kernel with proposal standard deviation 2.\n\nIdea of kernel mixture: at each MCMC iteration, suppose we start at \\(x\\)\n\nFlip a coin,\nif heads, use \\(K_1\\), i.e., \\(x' \\sim K_1(\\cdot | x)\\),\nif tails, \\(K_2\\), i.e., \\(x' \\sim K_2(\\cdot | x)\\).\n\n\n\n\nCoding up kernel mixtures\n\nWe start by defining our kernels.\nCalling the code below with kernel(gamma, x, i) corresponds to sampling from \\(K_i(\\cdot | x)\\) for \\(i \\in \\{1, 2\\}\\):\n\n\nkernel = function(gamma, current_point, proposal_sd) {\n  dim = length(current_point)\n  proposal = rnorm(dim, mean = current_point, sd = proposal_sd) \n  ratio = gamma(proposal) / gamma(current_point) \n  if (runif(1) &lt; ratio) {\n    return(proposal)\n  } else {\n    return(current_point)\n  }\n}\n\nBased on these these kernels, here is an example how to implement the mixture:\n\n# simple Metropolis-Hastings algorithm (normal proposal)\nmcmc_mixture = function(gamma, initial_point, n_iters) {\n  samples = numeric(n_iters) \n  dim = length(initial_point)\n  current_point = initial_point\n  for (i in 1:n_iters) {\n    \n1    kernel_index_choice = if (runif(1) &lt; 0.5) 1 else 2\n2    current_point = kernel(gamma, current_point, kernel_index_choice)\n    \n    samples[i] = current_point\n  }\n  return(samples)\n}\n\nsource(\"../blocks/plot_traces_and_hist.R\")\nsamples = mcmc_mixture(gamma_beta_binomial, 0.5,  1000)\nplot_traces_and_hist(samples)\n\n\n1\n\nFlip a coin.\n\n2\n\nUse kernel corresponding to the coin flip outcome.\n\n\n\n\n\n\n\n\n\n\n\n\n\nMathematical notation for kernel mixtures\n\nInstead of using \\(K_1\\) and \\(K_2\\) with equal probability…\n…let \\(\\rho_1\\) denote the probability to use \\(K_1\\),\n…and \\(\\rho_2 = 1 - \\rho_1\\).\n\nExpression for the mixture of the two kernels: \\[K_\\text{mix}(x' | x) = \\sum_{i=1}^2 \\rho_i K_i(x' | x). \\]",
    "crumbs": [
      "MCMC Hacking",
      "Kernel mixtures"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic06_mix.html#invariance-result",
    "href": "w12_mcmc2/topic06_mix.html#invariance-result",
    "title": "Kernel mixtures",
    "section": "Invariance result",
    "text": "Invariance result\n\nMixtures preserve invariance\nProposition: if \\(K_i\\) are \\(\\pi\\)-invariant, then their mixture is \\(\\pi\\)-invariant.\nQuestion: can you prove this result? Convince yourself at the same time that the argument crucially depends on \\(\\rho\\) not varying with the current state.\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\\[\\begin{align*}\n\\sum_x \\pi(x) K(x' | x) &= \\sum_x \\pi(x) \\sum_i \\rho_i K_i(x' | x) \\;\\;\\text{(def of mixture)} \\\\\n&=  \\sum_i \\rho_i  \\sum_x \\pi(x)  K_i(x' | x)  \\\\\n&=  \\sum_i \\rho_i   \\pi(x') \\;\\;\\text{(invariance of each $K_i$)}   \\\\\n&=  \\pi(x') \\sum_i \\rho_i     \\\\\n&=  \\pi(x') \\;\\;\\text{($\\rho_i$ are probabilities)}. \\\\\n\\end{align*}\\]",
    "crumbs": [
      "MCMC Hacking",
      "Kernel mixtures"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic06_mix.html#methodological-application",
    "href": "w12_mcmc2/topic06_mix.html#methodological-application",
    "title": "Kernel mixtures",
    "section": "Methodological application",
    "text": "Methodological application\nQuestion: would it be a good idea to use the current state to adapt the mixture proportion (i.e., using \\(\\rho_i(x)\\) instead of \\(\\rho_i\\)?",
    "crumbs": [
      "MCMC Hacking",
      "Kernel mixtures"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic11_involution.html",
    "href": "w12_mcmc2/topic11_involution.html",
    "title": "Deterministic proposals",
    "section": "",
    "text": "Deterministic proposals\nInvolutions\nProposal Jacobian determinants\n\n\n\n\nIn our first exposure to MH, we defined the proposal as a probability distribution \\(q(\\cdot|x)\\) based on the current point \\(x\\).\nFor HMC, it is more natural to consider the proposal as a deterministic function \\(T(x)\\) (namely, as we saw, the proposal is to “let the ball roll the ball a little bit”).\nWe state a version of the MH algorithm based on such deterministic proposal here.",
    "crumbs": [
      "MCMC Hacking",
      "Deterministic proposals"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic11_involution.html#outline",
    "href": "w12_mcmc2/topic11_involution.html#outline",
    "title": "Deterministic proposals",
    "section": "",
    "text": "Deterministic proposals\nInvolutions\nProposal Jacobian determinants\n\n\n\n\nIn our first exposure to MH, we defined the proposal as a probability distribution \\(q(\\cdot|x)\\) based on the current point \\(x\\).\nFor HMC, it is more natural to consider the proposal as a deterministic function \\(T(x)\\) (namely, as we saw, the proposal is to “let the ball roll the ball a little bit”).\nWe state a version of the MH algorithm based on such deterministic proposal here.",
    "crumbs": [
      "MCMC Hacking",
      "Deterministic proposals"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic11_involution.html#mh-with-deterministic-proposals",
    "href": "w12_mcmc2/topic11_involution.html#mh-with-deterministic-proposals",
    "title": "Deterministic proposals",
    "section": "MH with deterministic proposals",
    "text": "MH with deterministic proposals\nAlgorithm:\n\nDenote the current state by \\(x \\in \\mathbb{R}^d\\)\nPropose: \\(x^* = T(x)\\) for some deterministic proposal mapping \\(T : \\mathbb{R}^d \\to \\mathbb{R}^d\\)\n\nAssume that \\(T\\) satisfies \\(T = T^{-1}\\).\nTerminology: \\(T\\) is an involution.\n\nCompute the ratio, \\[r(x) = \\frac{\\gamma(x^*)}{\\gamma(x)} | \\nabla T(x) |,\\] where \\(| \\nabla T(x) |\\) denotes the Jacobian determinant.\nAccept \\(x^*\\) with probability \\(\\alpha(x) = \\min(1, r(x))\\), otherwise stay at \\(x\\).\n\nCalculus review: do you remember what is the Jacobian determinant?\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\\[\\begin{align*}\n\\nabla T(x) &= \\begin{bmatrix}\n    \\dfrac{\\partial T_1}{\\partial x_1} & \\cdots & \\dfrac{\\partial T_1}{\\partial x_n}\\\\\n    \\vdots                             & \\ddots & \\vdots\\\\\n    \\dfrac{\\partial T_m}{\\partial x_1} & \\cdots & \\dfrac{\\partial T_m}{\\partial x_n}\n\\end{bmatrix} \\\\\n|\\nabla T(x)| &= |\\det  \\nabla T(x)|.\n\\end{align*}\\]\n\n\n\nProposition: (Tierney, 1998) if \\(T\\) is a continuously differentiable involution, then the above algorithm is \\(\\pi\\)-invariant.",
    "crumbs": [
      "MCMC Hacking",
      "Deterministic proposals"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic11_involution.html#examples-of-involution",
    "href": "w12_mcmc2/topic11_involution.html#examples-of-involution",
    "title": "Deterministic proposals",
    "section": "Examples of involution",
    "text": "Examples of involution\nSwaps:\n\n\n\n\nSet \\(T(x_1, x_2) = (x_2, x_1)\\).\nTo show involution, compute \\(T^2(x) = T(T(x))\\) and show \\(T^2 = I\\), the identity map.\n\\(T(T(x_1, x_2)) = T(x_2, x_1) = (x_1, x_2)\\) ✅\n\nMultiplicative proposal: complete the definition of \\(T\\) to make it an involution on \\(\\mathbb{R}\\times \\mathbb{R}^+\\). \\[T(x_1, x_2) = T(x, m) = (mx, \\_\\_).\\]\nQuestion: Compute the Jacobian determinant of the multiplicative proposal involution.\n\n\n\n\n\n\nClick for hint\n\n\n\n\n\nYou should get a triangular matrix i.e. in our context of the form\n\\[\\begin{align*}\n\\nabla T &= \\begin{bmatrix}\n    a_{11} & a_{12} \\\\\n    0 & a_{22}\n\\end{bmatrix}.\n\\end{align*}\\]\nNow recall that from properties of triangular matrices, this means that to compute it determinant, you just have to compute the product of the diagonal entries, \\(\\det \\nabla T = a_{11} a_{22}\\).",
    "crumbs": [
      "MCMC Hacking",
      "Deterministic proposals"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic11_involution.html#references",
    "href": "w12_mcmc2/topic11_involution.html#references",
    "title": "Deterministic proposals",
    "section": "References",
    "text": "References\n\nSee Tierney, 1998 for the introduction of the notion of involutions and deterministic proposals to the MCMC literature.\nSee Green, 1995 for early use of the multivariate change of variable formula in MH acceptance ratios.",
    "crumbs": [
      "MCMC Hacking",
      "Deterministic proposals"
    ]
  },
  {
    "objectID": "w05_properties/topic03_decision_to_point.html",
    "href": "w05_properties/topic03_decision_to_point.html",
    "title": "Decision theoretic point estimation",
    "section": "",
    "text": "Deriving a point estimate from decision theory.\n\n\n\n\nWe have seen in week 2 some examples of point estimates (posterior mean, posterior mode).\nThese are actually special cases of decision theory with specific choices of loss functions.\nThis page provides a general framework to answer the question: “how to summarize a posterior distribution with one point?”",
    "crumbs": [
      "Some theory",
      "Decision theoretic point estimation"
    ]
  },
  {
    "objectID": "w05_properties/topic03_decision_to_point.html#outline",
    "href": "w05_properties/topic03_decision_to_point.html#outline",
    "title": "Decision theoretic point estimation",
    "section": "",
    "text": "Deriving a point estimate from decision theory.\n\n\n\n\nWe have seen in week 2 some examples of point estimates (posterior mean, posterior mode).\nThese are actually special cases of decision theory with specific choices of loss functions.\nThis page provides a general framework to answer the question: “how to summarize a posterior distribution with one point?”",
    "crumbs": [
      "Some theory",
      "Decision theoretic point estimation"
    ]
  },
  {
    "objectID": "w05_properties/topic03_decision_to_point.html#setup",
    "href": "w05_properties/topic03_decision_to_point.html#setup",
    "title": "Decision theoretic point estimation",
    "section": "Setup",
    "text": "Setup\nThe decision theoretic setup from week 2.",
    "crumbs": [
      "Some theory",
      "Decision theoretic point estimation"
    ]
  },
  {
    "objectID": "w05_properties/topic03_decision_to_point.html#example",
    "href": "w05_properties/topic03_decision_to_point.html#example",
    "title": "Decision theoretic point estimation",
    "section": "Example",
    "text": "Example\nAssume: a square loss, \\(L(a, p) = (a - p)^2\\), where \\(a \\in A = \\mathbb{R}\\).\nSome initial simplification: on the objective function…\n\\[\n\\begin{aligned}\n\\delta_{\\text{B}}(Y) &= \\operatorname{arg\\,min}\\{ \\mathbb{E}[L(a, X) | Y] : a \\in A \\} \\\\\n&=  \\operatorname{arg\\,min}\\{ \\mathbb{E}[(X - a)^2 | Y] : a \\in A \\} \\\\\n&=  \\operatorname{arg\\,min}\\{ \\mathbb{E}[X^2 | Y] - 2a\\mathbb{E}[X | Y]] + a^2 : a \\in A \\} \\\\\n&=  \\operatorname{arg\\,min}\\{ - 2a\\mathbb{E}[X | Y]] + a^2 : a \\in A \\}\n\\end{aligned}\n\\]\nQuestion: under a square loss, \\(\\delta_{\\text{B}}\\) can be simplified to…",
    "crumbs": [
      "Some theory",
      "Decision theoretic point estimation"
    ]
  },
  {
    "objectID": "w05_properties/topic06_calibration_well_specified.html",
    "href": "w05_properties/topic06_calibration_well_specified.html",
    "title": "Bayesian calibration: well-specified case",
    "section": "",
    "text": "Bayesian calibration.\nGuarantees when the model is well-specified.\n\n\n\n\nWe introduced in the previous page the important notion of calibration.\nThe question we consider now: are credible intervals calibrated?\nWe will give a two-part answer: first, answering the above question for well-specified models (this page), then, for mis-specified models (next page).",
    "crumbs": [
      "Some theory",
      "Bayesian calibration: well-specified case"
    ]
  },
  {
    "objectID": "w05_properties/topic06_calibration_well_specified.html#outline",
    "href": "w05_properties/topic06_calibration_well_specified.html#outline",
    "title": "Bayesian calibration: well-specified case",
    "section": "",
    "text": "Bayesian calibration.\nGuarantees when the model is well-specified.\n\n\n\n\nWe introduced in the previous page the important notion of calibration.\nThe question we consider now: are credible intervals calibrated?\nWe will give a two-part answer: first, answering the above question for well-specified models (this page), then, for mis-specified models (next page).",
    "crumbs": [
      "Some theory",
      "Bayesian calibration: well-specified case"
    ]
  },
  {
    "objectID": "w05_properties/topic06_calibration_well_specified.html#bayesian-calibration",
    "href": "w05_properties/topic06_calibration_well_specified.html#bayesian-calibration",
    "title": "Bayesian calibration: well-specified case",
    "section": "Bayesian calibration",
    "text": "Bayesian calibration\nThought experiment: what if a certain Bayesian inference method was used many times?\n\nFor example, different labs, each studying similar but different datasets,\ni.e., they are replicating an “experiment.”\nThis leads to a frequentist analysis of Bayesian procedures—a very useful thing to do!\n\nCalibration of credible intervals: a 90% credible interval is calibrated if it contains the true parameter 90% of the time.\nWhat do we mean by “true parameter”?\n\nStart with a joint distribution, \\(p^\\star(x, y)\\), which we will call “nature.”\nFor each “experiment”: use \\(p^\\star\\) to simulate both a true parameter, \\(x^\\star\\), and an associated dataset \\(y^\\star\\): \\((x^\\star, y^\\star) \\sim p^\\star\\). (\\(=\\) perform forward simulation).\n\nWhat do we mean by “90% credible interval”? A function \\(C(y) = [L(y), R(y)]\\) which:\n\ncomputes the posterior \\(\\pi(\\cdot | y)\\), and\nselects left and right end points, \\(l\\), \\(r\\) such that \\(\\int_l^r \\pi(x | y) \\mathrm{d}x = 0.9\\).\n\nRecall an examples of credible interval we discussed: quantile-based intervals.\nWhat do we mean by “90% of the time”?\n\nLoop over \\(1, 2, \\dots,\\) numberOfExperiments\n\ngenerate synthetic data \\((x^\\star, y^\\star) \\sim p^\\star\\)\ncompute the credible \\(C(y^\\star)\\)\nrecord if the true parameter is in the interval, i.e. if \\(x^\\star \\in C(y^\\star)\\)\n\nConsider the limit, as numberOfExperiments \\(\\to \\infty\\), of the fraction of times the true parameter is in the interval\n\nIf the limit is equal to 0.9 we say the credible interval is calibrated (great!)\nIf the limit is close to 0.9 we say the credible interval is approximately calibrated (that’s not too bad)\nIf the limit is higher than 0.9, we say the credible interval is over-conservative (that’s not too bad)\nIf the limit is lower than 0.9, we say the credible interval is anti-conservative (bad!)",
    "crumbs": [
      "Some theory",
      "Bayesian calibration: well-specified case"
    ]
  },
  {
    "objectID": "w05_properties/topic06_calibration_well_specified.html#well-specified-vs-mis-specified-models",
    "href": "w05_properties/topic06_calibration_well_specified.html#well-specified-vs-mis-specified-models",
    "title": "Bayesian calibration: well-specified case",
    "section": "Well-specified vs mis-specified models",
    "text": "Well-specified vs mis-specified models\n\nThere are two joint distributions involved in the thought experiment\n\n\\(p^*\\), used to generate data\n\\(p\\), used internally by the credible interval procedure to define a posterior \\(\\pi(x | y) \\propto p(x, y)\\)\n\nWe can consider the following setups\n\nWell-specified setup \\(p^* = p\\)\nMis-specified, \\(p^* \\neq p\\)\n\nFor now: focus on the well-specified setup.",
    "crumbs": [
      "Some theory",
      "Bayesian calibration: well-specified case"
    ]
  },
  {
    "objectID": "w05_properties/topic06_calibration_well_specified.html#numerical-exploration",
    "href": "w05_properties/topic06_calibration_well_specified.html#numerical-exploration",
    "title": "Bayesian calibration: well-specified case",
    "section": "Numerical exploration",
    "text": "Numerical exploration\nUsing the discrete model for Delta launches from Exercise 2, we will now implement and run the algorithm described above under “What do we mean by 90% of the time?”…\nWhat do you expect?\n\ncalibrated for small data, calibrated for large data\nnot calibrated for small data, calibrated for large data\nonly approximately calibrated for both small and large data\nnone of the above\n\n\n\nCode\nsuppressPackageStartupMessages(require(\"ggplot2\"))\nsuppressPackageStartupMessages(require(\"dplyr\"))\nsuppressPackageStartupMessages(require(\"tidyr\"))\ntheme_set(theme_bw())\n\nhigh_density_intervals &lt;- function(alpha, posterior_probs){\n  ordered_probs = posterior_probs[,order(posterior_probs[2,], decreasing = TRUE)]\n  cumulative_probs = cumsum(ordered_probs[2,])\n  index = which.max(cumulative_probs &gt;= (1-alpha))\n  return(ordered_probs[,1:index, drop=FALSE])\n}\n\nrdunif &lt;- function(max) { return(ceiling(max*runif(1))) } # unif 1, 2, .., max\n\nposterior_distribution &lt;- function(prior_probs, n_successes, n_trials){         \n  K &lt;- length(prior_probs)-1 # K+1 values that your p can assume\n  n_fails &lt;- n_trials - n_successes\n  p &lt;- seq(0, 1, 1/K)\n  posterior_probs &lt;-                                       # 1. this computes gamma(i)\n    prior_probs *                                          #    - pr. for picking coin (prior) (=1/3 in picture)\n    p^n_successes * (1-p)^n_fails                          #    - conditional for the flips \n  posterior_probs &lt;- posterior_probs/sum(posterior_probs)  # 2. normalize gamma(i)\n  post_prob &lt;- rbind(p, posterior_probs)\n  return(post_prob)\n}\n\nset.seed(1)\nK &lt;- 1000\nprior_used_for_computing_posterior &lt;- rep(1/(K+1), K+1) # Use same prior for simulation and posterior computation\nn_repeats &lt;- 1000\nalpha &lt;- 0.1\n\nhdi_coverage_pr &lt;- function(n_datapoints) {\n  n_inclusions &lt;- 0\n  for (repetition in seq(1:n_repeats)) {\n    i &lt;- rdunif(K + 1) - 1  # Always generate the data using a uniform prior\n    true_p &lt;- i/K\n    x &lt;- rbinom(1, n_datapoints, true_p)\n    post &lt;- posterior_distribution(prior_used_for_computing_posterior, x, n_datapoints)\n    \n    # This if is just a hacky way to check if true parameter is in the HDI credible interval\n    if (sum(abs(true_p - high_density_intervals(alpha, post)[1,]) &lt; 10e-10) == 1) {\n      n_inclusions &lt;- n_inclusions + 1\n    }\n  }\n  return(n_inclusions/n_repeats) # Fraction of simulation where the true parameter was in interval\n}\n\ndf &lt;- data.frame(\"n_observations\" = c(10, 100, 1000))\ndf$coverage_pr &lt;- sapply(df$n_observations, hdi_coverage_pr)\n\nggplot(data=df, aes(x=n_observations, y=coverage_pr)) +\n  ylim(0, 1) +\n  xlab(\"Number of observations\") + \n  ylab(\"Actual coverage\") +\n  geom_hline(yintercept=1-alpha, linetype=\"dashed\", color = \"black\") +\n  geom_line()",
    "crumbs": [
      "Some theory",
      "Bayesian calibration: well-specified case"
    ]
  },
  {
    "objectID": "w05_properties/topic06_calibration_well_specified.html#mathematical-underpinnings",
    "href": "w05_properties/topic06_calibration_well_specified.html#mathematical-underpinnings",
    "title": "Bayesian calibration: well-specified case",
    "section": "Mathematical underpinnings",
    "text": "Mathematical underpinnings\n\nDenote the data by \\(Y\\).\nParameter of interest: \\(X\\).\nDenote the credible interval obtained from \\(y\\) by \\(C(y)\\).\nWe want to show \\(\\mathbb{P}^*(X \\in C(Y)) = 0.9\\).\nBy construction, the interval \\(C(y)\\) satisfies, for all observed data \\(y\\), \\(\\mathbb{P}(X \\in C(y) | Y = y) = 0.9\\).\n\n\n\n\n\nRecall the law of total probability:\n\nfor any partition \\(E_i\\) of the sample space,\nwe have \\(\\mathbb{P}(A) = \\sum_i \\mathbb{P}(A, E_i)\\).\n\nHere: \\(A = (X \\in C(Y))\\), \\(E_i = (Y = y_i)\\)\n\nwhere \\(\\{y_i\\}\\) denotes the possible values the data can potentially take across all possible simulated experiments.\n\nPutting it all together and applying chain rule: \\[\\begin{align*}\n\\mathbb{P}^*(X \\in C(Y)) &= \\sum_i \\mathbb{P}^*(X \\in C(Y), Y = y_i) \\\\\n&= \\sum_i \\mathbb{P}^*(Y = y_i) \\mathbb{P}^*(X \\in C(y_i) | Y = y_i) \\\\\n&= \\sum_i \\mathbb{P}^*(Y = y_i) \\mathbb{P}(X \\in C(y_i) | Y = y_i) \\quad\\quad \\text{(assuming model is well-specified)} \\\\\n&= \\sum_i \\mathbb{P}^*(Y = y_i) 0.9 \\\\\n&= 0.9 \\sum_i \\mathbb{P}^*(Y = y_i) = 0.9.\n\\end{align*}\\]",
    "crumbs": [
      "Some theory",
      "Bayesian calibration: well-specified case"
    ]
  },
  {
    "objectID": "w05_properties/topic02_optimality.html",
    "href": "w05_properties/topic02_optimality.html",
    "title": "Optimality",
    "section": "",
    "text": "Average loss optimality of Bayes estimators\nLimitations of this notion of optimality.\n\n\n\n\nFormalize and justify one of the motivations for Bayesian methods covered in the first lecture.",
    "crumbs": [
      "Some theory",
      "Optimality"
    ]
  },
  {
    "objectID": "w05_properties/topic02_optimality.html#outline",
    "href": "w05_properties/topic02_optimality.html#outline",
    "title": "Optimality",
    "section": "",
    "text": "Average loss optimality of Bayes estimators\nLimitations of this notion of optimality.\n\n\n\n\nFormalize and justify one of the motivations for Bayesian methods covered in the first lecture.",
    "crumbs": [
      "Some theory",
      "Optimality"
    ]
  },
  {
    "objectID": "w05_properties/topic02_optimality.html#setup",
    "href": "w05_properties/topic02_optimality.html#setup",
    "title": "Optimality",
    "section": "Setup",
    "text": "Setup\nWe go back to the decision theoretic setup from week 2.",
    "crumbs": [
      "Some theory",
      "Optimality"
    ]
  },
  {
    "objectID": "w05_properties/topic02_optimality.html#estimator",
    "href": "w05_properties/topic02_optimality.html#estimator",
    "title": "Optimality",
    "section": "Estimator",
    "text": "Estimator\nDefinition: an estimator is a function \\(\\delta\\) such that:\n\n\\(\\delta\\) takes a dataset \\(y\\) as input (i.e. the observations), \\(\\delta(y)\\), and\nreturns an action, i.e., \\(\\delta(y) \\in A\\).\n\nExamples:\n\nthe Bayes estimator,\nmaximum likelihood estimator (when the action is a parameter you are trying to estimate).",
    "crumbs": [
      "Some theory",
      "Optimality"
    ]
  },
  {
    "objectID": "w05_properties/topic02_optimality.html#optimality-motivation",
    "href": "w05_properties/topic02_optimality.html#optimality-motivation",
    "title": "Optimality",
    "section": "Optimality: motivation",
    "text": "Optimality: motivation\nConsider the following situation—you are watching a talk, where the presenter explains they did the following:\n\nLet:\n\n\\(\\delta_{\\text{B}}\\) denote the Bayes estimator, and\n\\(\\delta_{\\text{o}}\\), another estimator, say based on a sophisticated deep neural network.\n\nTo compare the two estimators, the presenter generated a large number of synthetic datasets \\((X^{(m)}, Y^{(m)})\\) by forward simulation from the model.\nThey evaluated the performance of the Bayes estimator as follows:\n\napply the estimator: \\(a^{(m)}= \\delta_{\\text{B}}(Y^{(m)})\\),\ncompute the loss: \\(l^{(m)}= L(a^{(m)}, X^{(m)})\\), and\nreturn the average loss as the measure of performance: \\[\\text{average loss} = \\frac{1}{M} \\sum l^{(m)}.\\]\n\nThen, they did the same with the other estimator, \\(\\delta_{\\text{o}}\\).\nSuppose that the presenter reported a lower average loss for the other estimator, \\(\\delta_{\\text{o}}\\)\n\nPoll: Which of these statements is true?\n\nThe deep neural network can perform better because it involves more parameters (over-parameterization)\nThe deep neural network can perform better because of a combination of lucky initialization and noisy gradients\nThe deep neural network can perform better because of some other reason\nThis cannot happen unless the loss function is non-convex\nThis cannot happen because of some other reason",
    "crumbs": [
      "Some theory",
      "Optimality"
    ]
  },
  {
    "objectID": "w03_ppl/topic06_ess.html",
    "href": "w03_ppl/topic06_ess.html",
    "title": "SNIS Effective Sample Size",
    "section": "",
    "text": "Estimating SNIS’ error based on running it only once.\nUnderlying theoretical guarantee.\n\n\n\n\nSo far we have relied on heuristics to determine the number of Monte Carlo iterations. Here we describe a better approach.",
    "crumbs": [
      "A first look at PPLs",
      "SNIS Effective Sample Size"
    ]
  },
  {
    "objectID": "w03_ppl/topic06_ess.html#outline",
    "href": "w03_ppl/topic06_ess.html#outline",
    "title": "SNIS Effective Sample Size",
    "section": "",
    "text": "Estimating SNIS’ error based on running it only once.\nUnderlying theoretical guarantee.\n\n\n\n\nSo far we have relied on heuristics to determine the number of Monte Carlo iterations. Here we describe a better approach.",
    "crumbs": [
      "A first look at PPLs",
      "SNIS Effective Sample Size"
    ]
  },
  {
    "objectID": "w03_ppl/topic06_ess.html#example",
    "href": "w03_ppl/topic06_ess.html#example",
    "title": "SNIS Effective Sample Size",
    "section": "Example",
    "text": "Example\nImportant: we are back again to SNIS!\nRecall our “bag of coins” model can be written as:\n\n\n\n\\[\n\\begin{align*}\nX &\\sim {\\mathrm{Unif}}\\{0, 1, 2\\} \\\\\nY_i | X &\\sim {\\mathrm{Bern}}(X/2)\n\\end{align*}\n\\tag{1}\\]\nand using our simPPLe implementation for computing \\(\\mathbb{P}(X = 1 | Y = (0, 0, 0, 0))\\):\n\nsource(\"../exercises/ex03_scaffold.R\")\nsource(\"../exercises/ex03_ppl.R\")\nsource(\"../../solutions/sol03_posterior.R\")\nset.seed(123)\n\nn_mc_iterations = 10000\n\nposterior(my_first_probabilistic_program, n_mc_iterations)\n\n[1] 0.0582658\n\nposterior(my_first_probabilistic_program, n_mc_iterations)\n\n[1] 0.05938761\n\nposterior(my_first_probabilistic_program, n_mc_iterations)\n\n[1] 0.06055204\n\n\nFrom the above it looks like the error is in the third digit after the dot, roughly.\nOne way to assess the variability of our Monte Carlo estimator without having to run it several times is to estimate the effective sample size (ESS):\n\nsource(\"../blocks/simple_utils.R\")\ness_estimate = ess(posterior_particles(my_first_probabilistic_program, n_mc_iterations))\ness_estimate\n\n[1] 3842.793\n\n\nThen from an ESS estimate, we can approximate the root means squared error as follows:\n\nrmse_estimate = 2 / sqrt(ess_estimate)\nrmse_estimate\n\n[1] 0.03226313\n\n\nAs you can see, this is pessimistic, i.e. the actually error (roughly estimated from our 3 independent runs) seems lower than the estimate derived from the formula \\(2/\\sqrt{\\text{ESS}}\\).",
    "crumbs": [
      "A first look at PPLs",
      "SNIS Effective Sample Size"
    ]
  },
  {
    "objectID": "w03_ppl/topic06_ess.html#underlying-theory",
    "href": "w03_ppl/topic06_ess.html#underlying-theory",
    "title": "SNIS Effective Sample Size",
    "section": "Underlying theory",
    "text": "Underlying theory\nTheorem: From Theorem 2.1 in Agapiou et al. (2017): if \\(g\\) is such that \\(|g(x)| \\le 1\\) (for example, an indicator function), \\[\\text{RMSE} := \\sqrt{\\mathbb{E}(G_M - g^*)^2} \\le 2 \\sqrt{\\frac{\\mathbb{E}[W^2]}{M (\\mathbb{E}W)^2}} =: \\frac{2}{\\sqrt{\\text{ESS}}},\\] where \\(W\\) is the un-normalized SNIS weight random variable.\nEstimating SNIS ESS: compute the average weight and the average squared weights: \\[\\begin{align*}\n\\text{SNIS ESS} = \\frac{M (\\mathbb{E}W)^2}{\\mathbb{E}[W^2]} &\\approx \\text{SNIS ESS estimator} \\\\\n&= \\frac{M (\\frac{1}{M} \\sum_{m=1}^M W^{(m)})^2}{\\frac{1}{M} \\sum_{m=1}^M (W^{(m)})^2} \\\\\n&= \\frac{(\\sum_{m=1}^M W^{(m)})^2}{\\sum_{m=1}^M (W^{(m)})^2}.\n\\end{align*}\\]\nThat’s exactly what our ess function does:\neffective_sample_size = function(w) {\n  (sum(w)^2)/sum(w^2)\n}\nThe intuition behind ESS is:\n\nif I were able to sample iid from the posterior and use simple Monte Carlo on those,\nhow many simple Monte Carlo samples would give similar variability as my present SNIS estimator?\n\nNote:\n\nlater, when we talk about MCMC, we will use a different estimator for the ESS…\n… however, both share the same underlying intuition described above.",
    "crumbs": [
      "A first look at PPLs",
      "SNIS Effective Sample Size"
    ]
  },
  {
    "objectID": "w03_ppl/topic01_ppls_intro.html",
    "href": "w03_ppl/topic01_ppls_intro.html",
    "title": "What is a PPL?",
    "section": "",
    "text": "What is a Probabilistic Programming Language (PPL)?\nHow to use a PPL.\n\n\n\n\nPPLs is the main way you will compute posterior distributions in this course. This week you will write your own! This will give you insight on the strengths and limitation of these approaches.",
    "crumbs": [
      "A first look at PPLs",
      "What is a PPL?"
    ]
  },
  {
    "objectID": "w03_ppl/topic01_ppls_intro.html#outline",
    "href": "w03_ppl/topic01_ppls_intro.html#outline",
    "title": "What is a PPL?",
    "section": "",
    "text": "What is a Probabilistic Programming Language (PPL)?\nHow to use a PPL.\n\n\n\n\nPPLs is the main way you will compute posterior distributions in this course. This week you will write your own! This will give you insight on the strengths and limitation of these approaches.",
    "crumbs": [
      "A first look at PPLs",
      "What is a PPL?"
    ]
  },
  {
    "objectID": "w03_ppl/topic01_ppls_intro.html#ppl-in-a-nutshell",
    "href": "w03_ppl/topic01_ppls_intro.html#ppl-in-a-nutshell",
    "title": "What is a PPL?",
    "section": "PPL in a nutshell",
    "text": "PPL in a nutshell\n\nIt is often easier to do forward sampling than to compute a posterior:\n\nForward sampling: just go down a single path in the decision tree.\nComputing a posterior: need to sum over all paths compatible with observed data.\n\nPPLs allow you to:\n\ncode your model as if you were going to do forward sampling,\nand the PPL will magically figure out how to approximate the posterior!",
    "crumbs": [
      "A first look at PPLs",
      "What is a PPL?"
    ]
  },
  {
    "objectID": "w03_ppl/topic01_ppls_intro.html#ppl-an-example-using-simpple",
    "href": "w03_ppl/topic01_ppls_intro.html#ppl-an-example-using-simpple",
    "title": "What is a PPL?",
    "section": "PPL: an example using simPPLe",
    "text": "PPL: an example using simPPLe\nWe created possibly the simplest possible PPL for this course. Let’s call it simPPLe. The exercise this week will be to understand simPPLe by filling-in a couple of key lines of code.\nFirst I will show you what simPPLe can do.\nLet us start with something we are familiar with: our bag of coin problem…\n\nMathematical description\nRecall our “bag of coins” model can be written as:\n\n\n\n\\[\n\\begin{align*}\nX &\\sim {\\mathrm{Unif}}\\{0, 1, 2\\} \\\\\nY_i | X &\\sim {\\mathrm{Bern}}(X/2)\n\\end{align*}\n\\tag{1}\\]\n\n\nPPL description of the same model\nHere is how to code up the “bag of coin” model in simPPLe for the purpose of computing \\(\\mathbb{P}(X = 1 | Y = (0, 0, 0, 0))\\):\n\n\nex03_ppl.R\n\ncoin_flips = rep(0, 4) # \"dataset\" of four identical coin flips = (0, 0, 0, 0) \n\n# simPPLe's description of our \"bag of coins\" example\nmy_first_probabilistic_program = function() {\n  \n  # Similar to forward sampling, but use 'observe' when the variable is observed\n  coin_index = simulate(DiscreteDistribution(supp = 0:2))\n  prob_heads = coin_index/2\n  for (i in seq_along(coin_flips)) { \n    observe(coin_flips[i], Bern(prob_heads)) \n  }\n  \n  # return the test function g(x, y)\n  return(ifelse(coin_index == 1, 1, 0))\n}\n\nAfter solving this week’s exercise, you will be able to compute this probability as follows:\n\nsource(\"../exercises/ex03_scaffold.R\")\nsource(\"../exercises/ex03_ppl.R\")\nsource(\"../../solutions/sol03_posterior.R\") \n\nposterior(my_first_probabilistic_program, 10000)\n\n[1] 0.05665221\n\n\nCompare this to the solution of question 2.2 in exercise 1.\n\n\nExtension: predicting the next draw\nRecall the painful calculation we did last week to get the predictive.\nHere is how to do it in simPPLe:\n\npredict_next_flip &lt;- function() {\n  coin_index = simulate(DiscreteDistribution(supp = 0:2))\n  prob_heads = coin_index/2\n  for (i in seq_along(coin_flips)) { \n    observe(coin_flips[i], Bern(prob_heads)) \n  }\n  next_flip = simulate(Bern(prob_heads))\n  return(ifelse(next_flip == 0, 1, 0))\n}\n\nposterior(predict_next_flip, 10000)\n\n[1] 0.9695877",
    "crumbs": [
      "A first look at PPLs",
      "What is a PPL?"
    ]
  },
  {
    "objectID": "w03_ppl/topic03_continuous.html",
    "href": "w03_ppl/topic03_continuous.html",
    "title": "Continuous models",
    "section": "",
    "text": "Review of key concepts for continuous random variables.\n\nDensity.\nComputing their expectation.\n\nBayes rule with continuous random variables.\n\n\n\n\nSince the parameters of distributions families are continuous, it natural to use continuous random variables for them in Bayesian statistics. For example it lets us get rid of the artificial discretization \\(K\\) we used in last week’s exercise.\nToday we will approach the calculations with both manual mathematic derivation and PPLs. Later we will increasingly rely only on PPLs. But it is still important to understand both methods.",
    "crumbs": [
      "A first look at PPLs",
      "Continuous models"
    ]
  },
  {
    "objectID": "w03_ppl/topic03_continuous.html#outline",
    "href": "w03_ppl/topic03_continuous.html#outline",
    "title": "Continuous models",
    "section": "",
    "text": "Review of key concepts for continuous random variables.\n\nDensity.\nComputing their expectation.\n\nBayes rule with continuous random variables.\n\n\n\n\nSince the parameters of distributions families are continuous, it natural to use continuous random variables for them in Bayesian statistics. For example it lets us get rid of the artificial discretization \\(K\\) we used in last week’s exercise.\nToday we will approach the calculations with both manual mathematic derivation and PPLs. Later we will increasingly rely only on PPLs. But it is still important to understand both methods.",
    "crumbs": [
      "A first look at PPLs",
      "Continuous models"
    ]
  },
  {
    "objectID": "w03_ppl/topic03_continuous.html#densities",
    "href": "w03_ppl/topic03_continuous.html#densities",
    "title": "Continuous models",
    "section": "Densities",
    "text": "Densities\nDefinition: \\(X\\) has density \\(f(x)\\) if \\[\\mathbb{P}(X \\in A) = \\int_A f(x) \\mathrm{d}x.\\]\nLOTUS: works the same as with discrete models: \\[\\mathbb{E}[g(X)] = \\int f(x) g(x) \\mathrm{d}x.\\]",
    "crumbs": [
      "A first look at PPLs",
      "Continuous models"
    ]
  },
  {
    "objectID": "w03_ppl/topic03_continuous.html#joint-densities",
    "href": "w03_ppl/topic03_continuous.html#joint-densities",
    "title": "Continuous models",
    "section": "Joint densities",
    "text": "Joint densities\nDefinition: \\((X, Y)\\) has joint density \\(f(x, y)\\) if \\[\\mathbb{P}((X, Y) \\in A) = \\int \\int_A f(x, y) \\mathrm{d}x \\mathrm{d}y.\\]",
    "crumbs": [
      "A first look at PPLs",
      "Continuous models"
    ]
  },
  {
    "objectID": "w03_ppl/topic03_continuous.html#key-properties",
    "href": "w03_ppl/topic03_continuous.html#key-properties",
    "title": "Continuous models",
    "section": "Key properties",
    "text": "Key properties\nNote that in much of what follows, everything works the same as with with discrete models except we replace sums by integrals and PMFs \\(p\\) by densities \\(f\\).\nLOTUS: \\[\\mathbb{E}[g(X, Y)] = \\int \\int f(x, y) g(x, y) \\mathrm{d}x \\mathrm{d}y.\\]\nMarginalization: going from a joint density on \\((X, Y)\\) to the density of \\(X\\) only (the latter is called the marginal): \\[f_X(x) = \\int f(x, y) \\mathrm{d}y.\\]\nChain rule: \\[f(x, y) = f_X(x) f_{Y|X}(y|x).\\]\nBayes theorem: \\[f_{X|Y}(x|y) = \\frac{f(x, y)}{f_Y(y)}.\\]",
    "crumbs": [
      "A first look at PPLs",
      "Continuous models"
    ]
  },
  {
    "objectID": "w03_ppl/topic03_continuous.html#example-doomsday-model",
    "href": "w03_ppl/topic03_continuous.html#example-doomsday-model",
    "title": "Continuous models",
    "section": "Example: Doomsday model",
    "text": "Example: Doomsday model\n\nMathematical description\nConsider the following joint density described using chain rule: \\[\\begin{align*}\nX &\\sim {\\mathrm{Unif}}(0, 5) \\\\\nY | X &\\sim {\\mathrm{Unif}}(0, X).\n\\end{align*}\\]\n\n\nInterpretation 1\n\nI have a measuring tape, but you do not know how long is it.\n\nLength of tape: \\(X\\).\nLet’s say we think it’s less than 5m.\n\nI go in a separate room, unroll it fully, and pick a number at random from the tape.\n\nRandom point on tape: \\(Y\\)\n\n\nGoal: from the \\(Y\\), trying to guess the full length of the tape.\n\n\nInterpretation 2\n\n\\(X\\) is the total number of humans to ever live, future and past (in trillion).\n\\(Y\\) is the number of humans that were born before present (from archeological records, ~0.06 trillion).\n\n\n\n\nGoal: Can we guess (probabilistically) how many more human there will ever be in total?\nThis is known as the Doomsday argument\n\n\nComputations on the Doomsday model\nJoint density: which of these is the joint density?\n\n\n\n\n\n\nClick for choices\n\n\n\n\n\n\n\\(\\frac{\\mathbb{1}[y &lt; x &lt; 5]}{y - 5} \\frac{\\mathbb{1}[0 &lt; y &lt; x]}{x}\\)\n\\(\\frac{\\mathbb{1}[0 &lt; y &lt; 5]}{5} \\frac{\\mathbb{1}[0 &lt; x &lt; y]}{y}\\)\n\\(\\frac{\\mathbb{1}[0 &lt; x &lt; 5]}{5} \\frac{\\mathbb{1}[0 &lt; y &lt; x]}{x}\\)\n\\(\\frac{\\mathbb{1}[0 &lt; x &lt; 5]}{5} \\frac{\\mathbb{1}[0 &lt; y &lt; 5]}{5}\\)\nNone of the above\n\n\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nWe have \\[f_X(x) = \\frac{\\mathbb{1}[0 &lt; x &lt; 5]}{5}\\] and \\[f_{Y|X}(y|x) =  \\frac{\\mathbb{1}[0 &lt; y &lt; x]}{x},\\] therefore \\[f(x, y) = \\frac{\\mathbb{1}[0 &lt; x &lt; 5]}{5} \\frac{\\mathbb{1}[0 &lt; y &lt; x]}{x}.\\]\n\n\n\nMarginal density: compute the marginal \\(f_Y(y)\\)\n\n\n\n\n\n\nClick for choices\n\n\n\n\n\n\n\\(\\frac{1}{5} (\\log 5 - \\log y)\\)\n\\(\\log 5 - \\log y\\)\n\\(\\frac{2}{y^2} - \\frac{2}{25}\\)\n\\(\\frac{1}{5} \\left( \\frac{2}{y^2} - \\frac{2}{25} \\right)\\)\nNone of the above\n\n\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\\[\\begin{align*}\nf_Y(y) &= \\frac{1}{5} \\int \\frac{ \\mathbb{1}[0 &lt; x &lt; 5] \\mathbb{1}[0 &lt; y &lt; x]}{x} \\mathrm{d}x \\\\\n&= \\frac{1}{5} \\int_y^5 \\frac{\\mathrm{d}x}{x} \\\\\n&= \\frac{1}{5} (\\log 5 - \\log y).\n\\end{align*}\\]\n\n\n\nPosterior density: from our marginal and Bayes’ theorem, we obtain \\[f_{X|Y}(x|y) = \\frac{\\mathbb{1}[0&lt;x&lt;5] \\mathbb{1}[0 &lt; y &lt; x]}{x (\\log 5 - \\log y)}.\\]\nPosterior expectation: compute \\(\\mathbb{E}[X|Y = 0.06]\\)\n\n\n\n\n\n\nClick for choices\n\n\n\n\n\n\n\\(1.5\\)\n\\(\\approx 1.23\\)\n\\(\\approx 1.117\\)\n\\(\\approx 0.9714\\)\nNone of the above\n\n\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\\[\\begin{align*}\nf_{X|Y}(x|y) &= \\int x f_{X|Y}(x|y) \\mathrm{d}x \\\\\n&= \\frac{1}{\\log 5 - \\log y} \\int_y^5 \\mathrm{d}x \\\\\n&= \\frac{1}{\\log 5 - \\log y} (5 - y) \\approx 1.117. \\\\\n\\end{align*}\\]\n\n\n\n\n\n\n\n\n\nComparison with simPPLe\n\n\n\n\n\nCompare this to the simPPLe approximation\n\nsource(\"../exercises/ex03_scaffold.R\")\nsource(\"../../solutions/sol03_posterior.R\")\n\ndoomsday_model &lt;- function() {\n  x = simulate(Unif(0, 5))\n  observe(0.06, Unif(0, x))\n  return(x)\n}\n\nposterior(doomsday_model, 10000)\n\n[1] 1.125607",
    "crumbs": [
      "A first look at PPLs",
      "Continuous models"
    ]
  },
  {
    "objectID": "w02_discrete_bayes/topic03_posteriors.html",
    "href": "w02_discrete_bayes/topic03_posteriors.html",
    "title": "Posterior distributions",
    "section": "",
    "text": "Notion of posterior distribution.\nExamples\nHow to use a posterior?\n\n\n\n\nThe posterior distribution appears in the second step of the Bayesian Recipe and is therefore encountered (at least implicitly) in all full Bayesian problems.",
    "crumbs": [
      "Bayes on a discrete model",
      "Posterior distributions"
    ]
  },
  {
    "objectID": "w02_discrete_bayes/topic03_posteriors.html#outline",
    "href": "w02_discrete_bayes/topic03_posteriors.html#outline",
    "title": "Posterior distributions",
    "section": "",
    "text": "Notion of posterior distribution.\nExamples\nHow to use a posterior?\n\n\n\n\nThe posterior distribution appears in the second step of the Bayesian Recipe and is therefore encountered (at least implicitly) in all full Bayesian problems.",
    "crumbs": [
      "Bayes on a discrete model",
      "Posterior distributions"
    ]
  },
  {
    "objectID": "w02_discrete_bayes/topic03_posteriors.html#definition",
    "href": "w02_discrete_bayes/topic03_posteriors.html#definition",
    "title": "Posterior distributions",
    "section": "Definition",
    "text": "Definition\nThe conditional PMF of the unknowns \\(X\\) given the observation \\(Y\\) is the called the posterior PMF.1\n\nNotation\nWe use \\(\\pi(x) = \\mathbb{P}(X = x | Y = y)\\) for the posterior PMF.\nRecall:\n\\[\\pi(x) = \\frac{\\gamma(x)}{Z},\\]\nwhere \\(\\gamma(x) = \\mathbb{P}(X = x, Y = y)\\) is the un-normalized posterior and \\(Z = \\mathbb{P}(Y = y)\\) is the normalization constant.",
    "crumbs": [
      "Bayes on a discrete model",
      "Posterior distributions"
    ]
  },
  {
    "objectID": "w02_discrete_bayes/topic03_posteriors.html#examples",
    "href": "w02_discrete_bayes/topic03_posteriors.html#examples",
    "title": "Posterior distributions",
    "section": "Examples",
    "text": "Examples\n\nExample 1: coins in a bag\nVisualization of the prior PMF and how it arises from the decision tree and random variable \\(X\\) (showing 3 flips instead of 4):\n\n\nRecall that the probability of each path is the product of the edge labels.\n\nVisualization of the posterior PMF and how it arises from the decision tree and random variable \\(X\\):\n\n\nRecall we zero out the contribution of the paths not compatible with the observation (Heads, Heads, Heads).\nThis gives a list of numbers that do not sum to one, so we renormalize them.\n\n\n\nExample 2: rocket insurance\nYou will construct prior and posterior PMFs in question 2 of this week’s exercises.",
    "crumbs": [
      "Bayes on a discrete model",
      "Posterior distributions"
    ]
  },
  {
    "objectID": "w02_discrete_bayes/topic03_posteriors.html#what-to-do-with-a-posterior",
    "href": "w02_discrete_bayes/topic03_posteriors.html#what-to-do-with-a-posterior",
    "title": "Posterior distributions",
    "section": "What to do with a posterior?",
    "text": "What to do with a posterior?\n\nShow a visualiation (posterior PMF).\nCompute a summary of the PMF:\n\npoint estimate: single “best guess”, or\na credible region: a set of “guesses”.\n\nMore generally: decision theory / Step 3 of the Bayesian Recipe.",
    "crumbs": [
      "Bayes on a discrete model",
      "Posterior distributions"
    ]
  },
  {
    "objectID": "w02_discrete_bayes/topic03_posteriors.html#footnotes",
    "href": "w02_discrete_bayes/topic03_posteriors.html#footnotes",
    "title": "Posterior distributions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nif the unknown quantity is continuous, the posterior will be expressed using a density. A term that captures both the continuous and discrete case is “distribution” i.e. “posterior distribution”.↩︎",
    "crumbs": [
      "Bayes on a discrete model",
      "Posterior distributions"
    ]
  },
  {
    "objectID": "w02_discrete_bayes/topic08_criticism.html",
    "href": "w02_discrete_bayes/topic08_criticism.html",
    "title": "Intro to model criticism",
    "section": "",
    "text": "What is a model mis-specification?\nWhat are the consequences?\nWhat to do about it.\n\n\n\n\nModel mis-specification (the model being “too wrong”) can lead to serious problems in all types of statistical models, but Bayesian models are often more seriously affected. As a result it is important to detect serious cases of mis-specification and to address them. This is just an introduction, we will go in more depth once we have introduced Bayesian regression models.",
    "crumbs": [
      "Bayes on a discrete model",
      "Intro to model criticism"
    ]
  },
  {
    "objectID": "w02_discrete_bayes/topic08_criticism.html#outline",
    "href": "w02_discrete_bayes/topic08_criticism.html#outline",
    "title": "Intro to model criticism",
    "section": "",
    "text": "What is a model mis-specification?\nWhat are the consequences?\nWhat to do about it.\n\n\n\n\nModel mis-specification (the model being “too wrong”) can lead to serious problems in all types of statistical models, but Bayesian models are often more seriously affected. As a result it is important to detect serious cases of mis-specification and to address them. This is just an introduction, we will go in more depth once we have introduced Bayesian regression models.",
    "crumbs": [
      "Bayes on a discrete model",
      "Intro to model criticism"
    ]
  },
  {
    "objectID": "w02_discrete_bayes/topic08_criticism.html#definitions",
    "href": "w02_discrete_bayes/topic08_criticism.html#definitions",
    "title": "Intro to model criticism",
    "section": "Definitions",
    "text": "Definitions\nModel mis-specification: when the model is “too wrong” (in the context of the famous quote by George Box).\nModel criticism: the task of trying to find mis-specification.\n\nIt can be done by reasoning and discussion with experts.\nIt can be done using data (goodness-of-fit)\n\n\nExample\nRecall our notation: \\(\\rho\\) prior PMF, \\(\\pi\\) posterior PMF.\n\nSuppose we put zero prior mass to having a fair dice. What happens on the posterior?\n\n\n\nIf we put prior mass of zero to some realization \\(x\\), say \\(\\rho(x) = 0\\) then the posterior probability on \\(x\\) will always be zero, \\(\\pi(x) = 0\\), no matter how many observations we get\n\nThis could be disastrous! (e.g. ignoring extreme scenarios can have extreme consequences)\n\nPrinciple to avoid this is known as Cromwell’s rule (Oliver Cromwell, 1650): “I beseech you, in the bowels of Christ, think it possible that you may be mistaken.”\n\nDiscussion: can you identify other issues with the model from Exercise 2?",
    "crumbs": [
      "Bayes on a discrete model",
      "Intro to model criticism"
    ]
  },
  {
    "objectID": "w02_discrete_bayes/topic08_criticism.html#how-to-correct-mis-specification",
    "href": "w02_discrete_bayes/topic08_criticism.html#how-to-correct-mis-specification",
    "title": "Intro to model criticism",
    "section": "How to correct mis-specification",
    "text": "How to correct mis-specification\n\nImprove the model!\nThen iterate with more criticism and improve again if needed.\nIterative model improvement is part of the current “best practice” in Bayesian analysis.",
    "crumbs": [
      "Bayes on a discrete model",
      "Intro to model criticism"
    ]
  },
  {
    "objectID": "w02_discrete_bayes/topic06_decision.html",
    "href": "w02_discrete_bayes/topic06_decision.html",
    "title": "Decision theory",
    "section": "",
    "text": "Decision theory: making decision in the face of uncertainty.\n\nFirst, without observed data.\nSecond, incorporating observed data.\n\nReview of property of expectation useful for decision theory calculations.\n\n\n\n\nWe talked a lot about the “Bayesian recipe.” Here we formalize the last step of that recipe: using a posterior and a loss function to make a decision.\nLater, we will see that the point estimates and credible sets covered this week are actually special cases of the decision theoretic framework (i.e. they emerge when specific loss functions are selected).",
    "crumbs": [
      "Bayes on a discrete model",
      "Decision theory"
    ]
  },
  {
    "objectID": "w02_discrete_bayes/topic06_decision.html#outline",
    "href": "w02_discrete_bayes/topic06_decision.html#outline",
    "title": "Decision theory",
    "section": "",
    "text": "Decision theory: making decision in the face of uncertainty.\n\nFirst, without observed data.\nSecond, incorporating observed data.\n\nReview of property of expectation useful for decision theory calculations.\n\n\n\n\nWe talked a lot about the “Bayesian recipe.” Here we formalize the last step of that recipe: using a posterior and a loss function to make a decision.\nLater, we will see that the point estimates and credible sets covered this week are actually special cases of the decision theoretic framework (i.e. they emerge when specific loss functions are selected).",
    "crumbs": [
      "Bayes on a discrete model",
      "Decision theory"
    ]
  },
  {
    "objectID": "w02_discrete_bayes/topic06_decision.html#decision-theory-without-data",
    "href": "w02_discrete_bayes/topic06_decision.html#decision-theory-without-data",
    "title": "Decision theory",
    "section": "Decision theory without data",
    "text": "Decision theory without data\nYour friend proposes the following game (a good model for lottery!):\n\nShe flips a standard coin:\n\nif it’s a “heads”: you give her $10\nif it’s a “tails”: she gives you $9.\n\nYour decision: to play or not to play?\n\n\nMathematical formulation\n\nSet of actions: \\(A = \\{0, 1\\}\\) (\\(1 =\\) play, \\(0 =\\) do not play).\nProbability model: \\(X \\sim {\\mathrm{Bern}}(1/2)\\).\nLoss function:\n\n\\(L(a, x)\\): how much do I lose if I pick action \\(a \\in A\\) and encounter realization \\(X = x\\)?\n\nMathematical solution: minimize over \\(a \\in A\\) the expected loss \\[L(a) = \\mathbb{E}[L(a, X)].\\]\nRecall, by LOTUS: \\[\\mathbb{E}[L(a, X)] = \\sum_x L(a, x) p_{X}(x).\\]\n\n\n\nExample\nCompute \\(\\operatorname{arg\\,min}L(a)\\) and \\(\\min L(a)\\).\n\n\n\n\n\n\nClick for choices\n\n\n\n\n\n\n\\(1\\) and \\(1\\)\n\\(1\\) and \\(-1/2\\)\n\\(1\\) and \\(1/2\\)\n\\(0\\) and \\(0\\)\nNone of the above\n\n\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nThe loss function is given by:\n\n\n\n\nDo not play (\\(a = 0\\))\nPlay (\\(a = 1\\))\n\n\n\n\nHeads (\\(x = 1\\))\n\\(0\\)\n\\(10\\)\n\n\nTails (\\(x = 0\\))\n\\(0\\)\n\\(-9\\)\n\n\n\nSo we have \\(L(0) = 0\\) and \\(L(1) = 10/2 + (-9)/2 = 1/2\\).\nTherefore, the action that minimizes \\(L(a)\\) is \\(a = 0\\) (i.e., \\(a = 0\\) is the “argmin”) and gives value \\(L(0) = 0\\).\n\n\n\nIt is useful and important to answer this easy question in excruciating mathematical details. This will prepare us for more complex decision scenarios.\nLet us start by writing the loss function mathematically using indicator function defined as: \\[\\mathbb{1}[\\text{some condition}] = \\left\\{\n\\begin{array}{ll}\n1 & \\text{if the condition is true,} \\\\\n0 & \\text{if the condition is false.}\n\\end{array}\n\\right.\\] This gives us: \\[L(a, x) = \\mathbb{1}[a = 0] \\cdot (\\$0) + \\mathbb{1}[a = 1, x = 0] \\cdot (-\\$9) +  \\mathbb{1}[a = 1, x = 1] \\cdot (\\$10).\\]\nLet us simplify the expected loss function, i.e. \\[\\mathbb{E}[L(a, X)] = \\mathbb{E}[\\mathbb{1}[a = 0] \\cdot (\\$0) + \\mathbb{1}[a = 1, X = 0] \\cdot (-\\$9) +  \\mathbb{1}[a = 1, X = 1] \\cdot (\\$10)].\\] o do that simplification, first use linearity of expectation which you will recall from probability theory tells us that for any random variables \\(X_1\\) and \\(X_2\\), we have \\(\\mathbb{E}[X_1 + c X_2] = \\mathbb{E}[X_1] + c \\mathbb{E}[X_2]\\) for any constant \\(c\\).\nIn our context, using linearity with \\(c=1\\) allows us to simplify the above to: \\[\\underbrace{\\mathbb{E}[\\mathbb{1}[a = 0] \\cdot (\\$0)]}_0 + \\mathbb{E}[\\mathbb{1}[a = 1, X = 0] \\cdot (-\\$9)] + \\mathbb{E}[\\mathbb{1}[a = 1, X = 1] \\cdot (\\$10)].\\]\nTrick 1: To deal with each remaining term, use that: \\(\\mathbb{1}[a = 1, X = 1] = \\mathbb{1}[a = 1]\\mathbb{1}[X = 1]\\) (check!), and then use linearity with \\(c = \\$ 10 \\cdot \\mathbb{1}[a = 1]\\) to get: \\[\\mathbb{E}[\\mathbb{1}[a = 1, X = 1] \\cdot (\\$10)] =  \\$ 10 \\cdot \\mathbb{1}[a = 1] \\mathbb{E}[\\mathbb{1}[X = 1]].\\]\nLOTUS: Finally, we have to compute \\(\\mathbb{E}[\\mathbb{1}[X = 1]]\\). From our review of the Law of the Unconscious Statistician, this is done as follows: \\[\\mathbb{E}[\\mathbb{1}[X = 1]] = \\sum_x \\mathbb{1}[x = 1] p_X(x) = 0 + 1 \\cdot p_X(1) = 1/2.\\]\nTrick 2: you will often take expectation of indicator function, you will always find that \\(\\mathbb{E}[\\mathbb{1}[\\text{an event}]] = \\mathbb{P}(\\text{an event})\\).\nPutting it all together: \\[L(a) = \\mathbb{E}[L(a, X)] =  -\\$ 9 \\cdot \\mathbb{1}[a = 1] (1/2) + \\$ 10 \\cdot \\mathbb{1}[a = 1] (1/2) = \\$0.5 \\cdot \\mathbb{1}[a = 1].\\]",
    "crumbs": [
      "Bayes on a discrete model",
      "Decision theory"
    ]
  },
  {
    "objectID": "w02_discrete_bayes/topic06_decision.html#decision-theory-with-data",
    "href": "w02_discrete_bayes/topic06_decision.html#decision-theory-with-data",
    "title": "Decision theory",
    "section": "Decision theory with data",
    "text": "Decision theory with data\n\nWe now consider the case where we have some data \\(Y\\) that give us information about \\(X\\).\nThe actions \\(A\\) and loss \\(L(a, x)\\) are the same as before.\n\nHow to modify the last section to take into account the data \\(Y\\)?\n\nDo everything the same, except:\nuse the conditional PMF \\(p_{X|Y}\\) instead of the PMF \\(p_X\\).\n\n\nDefinition: the Bayes estimator\nMathematically: use an action \\(a \\in A\\) minimizing the posterior expected loss, i.e. \\[\\operatorname{arg\\,min}\\mathbb{E}[L(a, X) | Y = y].\\] where: \\[\\mathbb{E}[L(a, X) | Y = y] = \\sum_x L(a, x) p_{X|Y}(x|y).\\]\n\n\nExample\nYou will apply the Bayes estimator in question 3 of this week’s exercises.",
    "crumbs": [
      "Bayes on a discrete model",
      "Decision theory"
    ]
  },
  {
    "objectID": "w02_discrete_bayes/topic04_point.html",
    "href": "w02_discrete_bayes/topic04_point.html",
    "title": "Point estimates",
    "section": "",
    "text": "Common point estimates:\n\nPosterior mean.\nPosterior mode.\n\n\n\n\n\nIt is often necessary to summarize the posterior distribution with a single “best guess”, even though as we will see this hides important information namely our uncertainty about that guess.",
    "crumbs": [
      "Bayes on a discrete model",
      "Point estimates"
    ]
  },
  {
    "objectID": "w02_discrete_bayes/topic04_point.html#outline",
    "href": "w02_discrete_bayes/topic04_point.html#outline",
    "title": "Point estimates",
    "section": "",
    "text": "Common point estimates:\n\nPosterior mean.\nPosterior mode.\n\n\n\n\n\nIt is often necessary to summarize the posterior distribution with a single “best guess”, even though as we will see this hides important information namely our uncertainty about that guess.",
    "crumbs": [
      "Bayes on a discrete model",
      "Point estimates"
    ]
  },
  {
    "objectID": "w02_discrete_bayes/topic04_point.html#definitions",
    "href": "w02_discrete_bayes/topic04_point.html#definitions",
    "title": "Point estimates",
    "section": "Definitions",
    "text": "Definitions\n\nLet \\(\\pi(x) = \\mathbb{P}(X = x | Y = y)\\) denote a posterior PMF.\nPoint estimate: Instead of plotting the full information in \\(\\pi\\), we can report a “location” summary such as the mean of the posterior \\(\\pi\\).\n\n\nPosterior mean\nRecall the mean is computed from a PMF via \\[\\sum_x x\\; \\pi(x),\\] where the sum is over \\(\\{x : \\pi(x) &gt; 0 \\}\\).\nNotation: the posterior mean is denoted \\(\\mathbb{E}[X | Y = y] = \\sum x\\ \\pi(x)\\).\n\n\nPosterior mode\nThe mode is the location of the “tallest stick” in the PMF.\nNotation: \\(\\operatorname{arg\\,max}\\pi(x),\\) i.e. the point that achieves the maximum of \\(\\pi\\).\nIn the Bayesian context, the mode of a posterior PMF is also known as the Maximum A Posteriori (MAP) estimator.",
    "crumbs": [
      "Bayes on a discrete model",
      "Point estimates"
    ]
  },
  {
    "objectID": "w02_discrete_bayes/topic04_point.html#examples",
    "href": "w02_discrete_bayes/topic04_point.html#examples",
    "title": "Point estimates",
    "section": "Examples",
    "text": "Examples\n\nExample 1\nIn the following, we will compute point estimates based on \\(\\pi(x) = \\mathbb{P}(X = x | Y = (1, 1))\\) in the bag of coin running example (notice we observe only two coin flips here).\n\nImagine a bag with 3 coins each with a different probability parameter \\(p\\)\nCoin \\(i\\in \\{0, 1, 2\\}\\) has bias \\(i/2\\)—in other words:\n\nFirst coin: bias is \\(0/2 = 0\\) (i.e. both sides are “heads”, \\(p = 0\\))\nSecond coin: bias is \\(1/2 = 0.5\\) (i.e. standard coin, \\(p = 1/2\\))\nThird coin: bias is \\(2/2 = 1\\) (i.e. both sides are “tails”, \\(p = 1\\))\n\n\n\n\n\n\nConsider the following two steps sampling process\n\nStep 1: pick one of the three coins, but do not look at it!\nStep 2: flip the coin 2 times\n\nMathematically, this probability model can be written as follows: \\[\n\\begin{align*}\nX &\\sim {\\mathrm{Unif}}\\{0, 1, 2\\} \\\\\nY_i | X &\\sim {\\mathrm{Bern}}(X/2)\n\\end{align*}\n\\tag{1}\\]\n\n\nQuestion: compute the posterior mean of \\(\\pi\\).\n\n\n\n\n\n\nClick for choices\n\n\n\n\n\n\n0.5\n1.8\n2.25\n3.5\nNone of the above\n\n\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nFirst, compute the unnormalized posterior \\(\\gamma \\propto \\pi\\): \\[\\gamma = (\\gamma(0), \\gamma(1), \\gamma(2)) = (1/3) (0^2, (1/2)^2, 1^2),\\] then normalize: \\[\\pi = \\gamma / Z =  (0, 1/5, 4/5).\\] Finally, compute the conditional expectation: \\[\\mathbb{E}[X | Y = (1, 1)] = \\sum x \\; \\pi(x) = (0, 1, 2) \\cdot (0, 1/5, 4/5) = 9/5 = 1.8.\\]\nCommon mistake: forgetting normalization step: \\[\\sum x \\; \\pi(x) \\neq \\sum x \\; \\gamma(x).\\]\n\nThis “common mistake” highlights that we really need \\(Z\\) to compute posterior expectations using the exact, exhaustive approach (i.e. the method we are using here).\nWhen we talk more about Monte Carlo methods, we will see that these methods allow us to approximate expecations without having to compute \\(Z\\)!\n\n\n\n\nQuestion: compute the posterior mode of \\(\\pi\\).\n\n\n\n\n\n\nClick for choices\n\n\n\n\n\n\n0\n1\n2\n4/5\nNone of the above\n\n\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nSince the highest value of \\(\\pi\\) is achieved at \\(x = 2\\), the answer is \\(2\\).\n\n\n\n\n\nExample 2\nYou will practice computing the posterior mean/mode in question 2 of the exercises.",
    "crumbs": [
      "Bayes on a discrete model",
      "Point estimates"
    ]
  },
  {
    "objectID": "challenges/ch05.html",
    "href": "challenges/ch05.html",
    "title": "Challenge questions",
    "section": "",
    "text": "Not for grades!\n\n\n\nThese are not essential for learning the material and can be skipped without affecting your grade. If you successfully solve one set of problem, a week of participation activity will be waived (it does not have to be the same week you submit the challenge question). Submit your answer at any time. I will not post solutions for the challenge questions.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nPage under construction: information on this page may change.",
    "crumbs": [
      "Some theory",
      "Challenge"
    ]
  },
  {
    "objectID": "challenges/ch04.html",
    "href": "challenges/ch04.html",
    "title": "Challenge questions",
    "section": "",
    "text": "Not for grades!\n\n\n\nThese are not essential for learning the material and can be skipped without affecting your grade. If you successfully solve one set of problem, a week of participation activity will be waived (it does not have to be the same week you submit the challenge question). Submit your answer at any time. I will not post solutions for the challenge questions.",
    "crumbs": [
      "The joy of probabilistic modelling",
      "Challenge"
    ]
  },
  {
    "objectID": "challenges/ch04.html#q.1-modelling",
    "href": "challenges/ch04.html#q.1-modelling",
    "title": "Challenge questions",
    "section": "Q.1 Modelling",
    "text": "Q.1 Modelling\nDesign a Bayesian model to infer the total number of copies in circulation. Motivate all choices you make.",
    "crumbs": [
      "The joy of probabilistic modelling",
      "Challenge"
    ]
  },
  {
    "objectID": "challenges/ch04.html#q.2-posterior-computation",
    "href": "challenges/ch04.html#q.2-posterior-computation",
    "title": "Challenge questions",
    "section": "Q.2 Posterior computation",
    "text": "Q.2 Posterior computation\nApproximate the posterior distribution and produce:\n\nA posterior PMF.\nA point estimate.\nA 95% highest probability set.",
    "crumbs": [
      "The joy of probabilistic modelling",
      "Challenge"
    ]
  },
  {
    "objectID": "challenges/ch03.html",
    "href": "challenges/ch03.html",
    "title": "Challenge questions",
    "section": "",
    "text": "Not for grades!\n\n\n\nThese are not essential for learning the material and can be skipped without affecting your grade. If you successfully solve one set of problem, a week of participation activity will be waived (it does not have to be the same week you submit the challenge question). Submit your answer at any time. I will not post solutions for the challenge questions.\n\n\nRecall the decision tree visualization we introduced in the first week.\nCreate a variant of simPPLe which draws decision trees automatically.\nSpecifically, your code should include a function decision_tree which takes as input a simPPLe probabilistic program, and output a mermaid string which can be rendered in your browser as a diagram.\nDemonstrate your code on two examples: one in class, and one not from the class.",
    "crumbs": [
      "A first look at PPLs",
      "Challenge"
    ]
  },
  {
    "objectID": "challenges/ch09.html",
    "href": "challenges/ch09.html",
    "title": "Challenge questions",
    "section": "",
    "text": "Not for grades!\n\n\n\nThese are not essential for learning the material and can be skipped without affecting your grade. If you successfully solve one set of problem, a week of participation activity will be waived (it does not have to be the same week you submit the challenge question). Submit your answer at any time. I will not post solutions for the challenge questions.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nPage under construction: information on this page may change.",
    "crumbs": [
      "MCMC Hacking",
      "Challenge"
    ]
  },
  {
    "objectID": "drafts/stan_test.html",
    "href": "drafts/stan_test.html",
    "title": "Test Stan",
    "section": "",
    "text": "suppressPackageStartupMessages(require(rstan))\n\n\nparameters {\n  real w;\n}\nmodel {\n  w ~ std_normal();\n}\n\n\nfit\n\nInference for Stan model: anon_model.\n1 chains, each with iter=10000; warmup=5000; thin=1; \npost-warmup draws per chain=5000, total post-warmup draws=5000.\n\n      mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nw     0.00    0.02 0.98 -1.92 -0.67  0.01  0.66  1.96  1671    1\nlp__ -0.48    0.01 0.68 -2.38 -0.62 -0.22 -0.05  0.00  2457    1\n\nSamples were drawn using NUTS(diag_e) at Thu Feb 29 15:00:14 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\n\nfit = sampling(\n  test,         \n  data = list(), \n  show_messages = FALSE,\n  open_progress = FALSE, \n  chains = 1,\n  iter = 10000\n)\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 3e-06 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.013 seconds (Warm-up)\nChain 1:                0.013 seconds (Sampling)\nChain 1:                0.026 seconds (Total)\nChain 1:"
  },
  {
    "objectID": "drafts/ex03.html",
    "href": "drafts/ex03.html",
    "title": "Exercise 3: Universal probabilistic inference via importance sampling",
    "section": "",
    "text": "Caution\n\n\n\nPage under construction: information on this page may change."
  },
  {
    "objectID": "drafts/ex03.html#goals",
    "href": "drafts/ex03.html#goals",
    "title": "Exercise 3: Universal probabilistic inference via importance sampling",
    "section": "Goals",
    "text": "Goals\n\nUnderstanding and implementing importance sampling\nIntroduction to probabilistic programming"
  },
  {
    "objectID": "w01_discrete_inference/topic11_monte_carlo.html",
    "href": "w01_discrete_inference/topic11_monte_carlo.html",
    "title": "Simple Monte Carlo",
    "section": "",
    "text": "Simple Monte Carlo method\nTheoretical guarantee from the Law of Large Numbers\n\n\n\n\nSimple Monte Carlo is the foundation for more complex Monte Carlo methods used by Bayesian practioners (e.g. Importance Sampling and Markov chain Monte Carlo (MCMC))",
    "crumbs": [
      "Probability essentials",
      "Simple Monte Carlo"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic11_monte_carlo.html#outline",
    "href": "w01_discrete_inference/topic11_monte_carlo.html#outline",
    "title": "Simple Monte Carlo",
    "section": "",
    "text": "Simple Monte Carlo method\nTheoretical guarantee from the Law of Large Numbers\n\n\n\n\nSimple Monte Carlo is the foundation for more complex Monte Carlo methods used by Bayesian practioners (e.g. Importance Sampling and Markov chain Monte Carlo (MCMC))",
    "crumbs": [
      "Probability essentials",
      "Simple Monte Carlo"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic11_monte_carlo.html#approximation-of-expectations-using-forward-simulation",
    "href": "w01_discrete_inference/topic11_monte_carlo.html#approximation-of-expectations-using-forward-simulation",
    "title": "Simple Monte Carlo",
    "section": "Approximation of expectations using forward simulation",
    "text": "Approximation of expectations using forward simulation\n\nAs before, we want to compute an expectation \\(\\mathbb{E}[g(X, Y)]\\)\nImagine a very large decision tree, but where most branches have very low probability and only few have large probability\nIn this case, instead of computing the exact expectation by iterating over each of the leaves as before, we will approximate expectations using forward simulation (a method know as simple Monte Carlo)\nThis is done as follows:\n\nCall your forward simulator \\(M\\) times.\n\nDenote the output at iteration \\(m \\in \\{1, 2, \\dots M\\}\\) by: \\[(X^{(m)}, Y^{(m)}) \\sim p_{X, Y}(\\cdot)\\]\nCompute \\(g\\) on each, call each of the \\(M\\) outputs \\(G^{(m)}\\) \\[G^{(m)}= g(X^{(m)}, Y^{(m)}).\\]\n\nReturn the average \\[\\hat G_M = \\frac{1}{M} \\sum_{m=1}^M G^{(m)}.\\]\n\n\nIntuitively, the output \\(\\hat G_M\\) has the nice property: \\[\\hat G_M \\approx \\mathbb{E}[g(X, Y)]. \\tag{1}\\]",
    "crumbs": [
      "Probability essentials",
      "Simple Monte Carlo"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic11_monte_carlo.html#example-1",
    "href": "w01_discrete_inference/topic11_monte_carlo.html#example-1",
    "title": "Simple Monte Carlo",
    "section": "Example 1",
    "text": "Example 1\nQuestion 1.2 in the exercise.",
    "crumbs": [
      "Probability essentials",
      "Simple Monte Carlo"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic11_monte_carlo.html#example-2",
    "href": "w01_discrete_inference/topic11_monte_carlo.html#example-2",
    "title": "Simple Monte Carlo",
    "section": "Example 2",
    "text": "Example 2\nConsider the following model:\n\\[\n\\begin{align*}\nX &\\sim {\\mathrm{Unif}}\\{1, 2, 3, 4\\} \\\\\nY | X &\\sim {\\mathrm{Unif}}\\{1, \\dots, X\\}.\n\\end{align*}\n\\]\n\n\n\nInterpretation: you roll a D&D d4 dice (blue one on the image), then pick an integer uniformly between 1 and the number on the dice.\nLet us use Monte Carlo to estimate \\(\\mathbb{E}[X^Y]\\):\n\nrequire(extraDistr)\n\nLoading required package: extraDistr\n\nset.seed(4)\n\n# Recall our forward simulation code from earlier:\nforward_simulate_roll_and_pick &lt;- function() {\n  x &lt;- rdunif(1, min=1, max=4) # 1 uniform between 1-6, i.e. a dice\n  y &lt;- rdunif(1, min=1, max=x)\n  c(x, y) # return a vector with these two realizations\n}\n\nsum &lt;- 0.0\nn_iterations &lt;- 10000\nfor (iteration in 1:n_iterations) {\n  sample &lt;- forward_simulate_roll_and_pick()\n  sum &lt;- sum + sample[1]^sample[2]\n}\nprint(sum/n_iterations)\n\n[1] 25.7831",
    "crumbs": [
      "Probability essentials",
      "Simple Monte Carlo"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic11_monte_carlo.html#sec-lln",
    "href": "w01_discrete_inference/topic11_monte_carlo.html#sec-lln",
    "title": "Simple Monte Carlo",
    "section": "Guarantees from the Law of Large Numbers",
    "text": "Guarantees from the Law of Large Numbers\nQuestion: How can we make \\(\\approx\\) more formal in Equation 1?\nProposition (Law of Large Numbers, LLN): if \\(Z_1, Z_2, \\dots\\) are i.i.d. random variables with \\(\\mathbb{E}|Z_i| &lt; \\infty\\), then1 \\[ \\frac{1}{M} \\sum_{m=1}^M Z_m \\to \\mathbb{E}[Z_1].\\]\nPicking \\(Z_m = G^{(m)}\\) we arrive to the following formalization of \\(\\approx\\): for any approximation error tolerance, we can find a number of iterations \\(M\\) large enough such that we will be within that error tolerance with high probability after \\(M\\) iterations.",
    "crumbs": [
      "Probability essentials",
      "Simple Monte Carlo"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic11_monte_carlo.html#footnotes",
    "href": "w01_discrete_inference/topic11_monte_carlo.html#footnotes",
    "title": "Simple Monte Carlo",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHere “\\(\\to\\)” denotes a suitable notion of convergence of random variables. In STAT 302 you may have seen LLN with “convergence in probability”, but this can be strengthen to “convergence almost sure.” The difference between these notions of convergence will not matter in this course.↩︎",
    "crumbs": [
      "Probability essentials",
      "Simple Monte Carlo"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic10_expectations.html",
    "href": "w01_discrete_inference/topic10_expectations.html",
    "title": "Expectations",
    "section": "",
    "text": "Expectation for discrete random models\nLaw of the Unconscious Statistician\n\n\n\n\nExpectation is the main tool to translate a posterior distribution into the various outputs of Bayesian inference (point estimate, credible intervals, prediction, action).",
    "crumbs": [
      "Probability essentials",
      "Expectations"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic10_expectations.html#outline",
    "href": "w01_discrete_inference/topic10_expectations.html#outline",
    "title": "Expectations",
    "section": "",
    "text": "Expectation for discrete random models\nLaw of the Unconscious Statistician\n\n\n\n\nExpectation is the main tool to translate a posterior distribution into the various outputs of Bayesian inference (point estimate, credible intervals, prediction, action).",
    "crumbs": [
      "Probability essentials",
      "Expectations"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic10_expectations.html#expectation-of-a-single-random-variable",
    "href": "w01_discrete_inference/topic10_expectations.html#expectation-of-a-single-random-variable",
    "title": "Expectations",
    "section": "Expectation of a single random variable",
    "text": "Expectation of a single random variable\nRecall: \\[\\mathbb{E}[X] = \\sum_x x p_X(x),\\] where the sum is over the point masses of \\(X\\), i.e. \\(\\{x : p_X(x) &gt; 0\\}\\).\nExample: compute \\(\\mathbb{E}[X]\\) if \\(X \\sim {\\mathrm{Bern}}(p)\\), with \\(p = 0.8\\).\n\n\n\n\n\n\n\nflowchart TD\nS -- 0.2 --&gt; S__and__X_false[\"X=false\"]\nS -- 0.8 --&gt; S__and__X_true[\"X=true\"]",
    "crumbs": [
      "Probability essentials",
      "Expectations"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic10_expectations.html#law-of-the-unconscious-statistician",
    "href": "w01_discrete_inference/topic10_expectations.html#law-of-the-unconscious-statistician",
    "title": "Expectations",
    "section": "Law of the Unconscious Statistician",
    "text": "Law of the Unconscious Statistician\nProposition: if \\(g\\) is some function, \\[\\mathbb{E}[g(X)] = \\sum_x g(x) p_X(x).\\]\nExample: compute \\(\\mathbb{E}[X^2]\\) if \\(X \\sim {\\mathrm{Bern}}(p)\\), and hence \\(\\operatorname{Var}[X] = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2\\).\nQuestion: Compute \\(\\mathbb{E}[1/(X+1)]\\), where \\(X \\sim {\\mathrm{Bern}}(1/3)\\).\n\n\n\n\n\n\nClick for choices\n\n\n\n\n\n\n3/5\n3/4\n5/6\n1/3\nNone of the above\n\n\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nUse \\(g(x) = 1/(x+1)\\) in LOTUS:\n\\[\n\\mathbb{E}[1/(X+1)] = \\sum_{x\\in\\{0, 1\\}} g(x) p(x) = \\frac{2}{3} \\frac{1}{0+1} + \\frac{1}{3}\\frac{1}{1+1} = \\frac{5}{6}.\n\\]",
    "crumbs": [
      "Probability essentials",
      "Expectations"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic10_expectations.html#expectation-of-a-function-of-several-random-variables",
    "href": "w01_discrete_inference/topic10_expectations.html#expectation-of-a-function-of-several-random-variables",
    "title": "Expectations",
    "section": "Expectation of a function of several random variables",
    "text": "Expectation of a function of several random variables\nLet us go back to our running example:\n\nImagine a bag with 3 coins each with a different probability parameter \\(p\\)\nCoin \\(i\\in \\{0, 1, 2\\}\\) has bias \\(i/2\\)—in other words:\n\nFirst coin: bias is \\(0/2 = 0\\) (i.e. both sides are “heads”, \\(p = 0\\))\nSecond coin: bias is \\(1/2 = 0.5\\) (i.e. standard coin, \\(p = 1/2\\))\nThird coin: bias is \\(2/2 = 1\\) (i.e. both sides are “tails”, \\(p = 1\\))\n\n\n\n\n\n\nConsider the following two steps sampling process\n\nStep 1: pick one of the three coins, but do not look at it!\nStep 2: flip the coin 4 times\n\nMathematically, this probability model can be written as follows: \\[\n\\begin{align*}\nX &\\sim {\\mathrm{Unif}}\\{0, 1, 2\\} \\\\\nY_i | X &\\sim {\\mathrm{Bern}}(X/2)\n\\end{align*}\n\\tag{1}\\]\n\nExample: computing \\(\\mathbb{E}[X (Y_1+1)]\\) (similar to what you will be doing in the exercise in question 1.1)\nNote: this is of the form \\(\\mathbb{E}[g(\\dots)]\\), so we can use the Law of the Unconscious Statistician.\nHow to do it:\n\nfirst, identify \\(g\\), here it is \\(g(x, y_1, \\dots, y_4) = x(y_1+1)\\) (in the exercise it is slightly different)\ndenote by \\(p\\) the joint PMF of all the random variables in the model\ncompute the expectation using \\[\\mathbb{E}[g(X, Y_1, \\dots, Y_4)] = \\sum_x \\sum_{y_1} \\sum_{y_2} \\dots \\sum_{y_4} g(x, y_1, \\dots, y_4)  p(x, y_1, y_2, y_3, y_4).\\]\nEach sum runs over the point mass of its PMF as before, e.g. \\(x \\in \\{0, 1, 2\\}\\).\nRecall: \\(p(x, y_1, y_2, y_3, y_4)\\) can be computed using the chain rule.\n\nRecall the decision tree, how to visualize the above equation?\n\n\n\n\n\n\nflowchart TD\nS__and__X_0 -- 1.0 --&gt; S__and__X_0__and__Y1_false[\"Y1=false\"]\nS__and__X_2__and__Y1_true -- 1.0 --&gt; S__and__X_2__and__Y1_true__and__Y2_true[\"Y2=true\"]\nS -- 0.33 --&gt; S__and__X_0[\"X=0\"]\nS__and__X_1__and__Y1_true__and__Y2_true__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_true__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_true__and__Y2_false__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_true__and__Y4_false[\"Y4=false\"]\nS__and__X_0__and__Y1_false__and__Y2_false__and__Y3_false -- 1.0 --&gt; S__and__X_0__and__Y1_false__and__Y2_false__and__Y3_false__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_false__and__Y2_false__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_false__and__Y4_true[\"Y4=true\"]\nS -- 0.33 --&gt; S__and__X_1[\"X=1\"]\nS__and__X_2__and__Y1_true__and__Y2_true__and__Y3_true -- 1.0 --&gt; S__and__X_2__and__Y1_true__and__Y2_true__and__Y3_true__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_true__and__Y2_false__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_false__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_false__and__Y2_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_false[\"Y3=false\"]\nS__and__X_1__and__Y1_true__and__Y2_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_false[\"Y3=false\"]\nS__and__X_1__and__Y1_false__and__Y2_false__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_false__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_true__and__Y2_true__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_false__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_true__and__Y2_false__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_true__and__Y4_true[\"Y4=true\"]\nS__and__X_1 -- 0.5 --&gt; S__and__X_1__and__Y1_false[\"Y1=false\"]\nS__and__X_1__and__Y1_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false[\"Y2=false\"]\nS__and__X_1__and__Y1_false__and__Y2_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_true[\"Y3=true\"]\nS__and__X_1__and__Y1_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true[\"Y2=true\"]\nS__and__X_0__and__Y1_false -- 1.0 --&gt; S__and__X_0__and__Y1_false__and__Y2_false[\"Y2=false\"]\nS__and__X_1__and__Y1_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false[\"Y2=false\"]\nS__and__X_1__and__Y1_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true[\"Y2=true\"]\nS__and__X_2 -- 1.0 --&gt; S__and__X_2__and__Y1_true[\"Y1=true\"]\nS__and__X_1__and__Y1_false__and__Y2_true__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_true__and__Y4_false[\"Y4=false\"]\nS__and__X_1 -- 0.5 --&gt; S__and__X_1__and__Y1_true[\"Y1=true\"]\nS -- 0.33 --&gt; S__and__X_2[\"X=2\"]\nS__and__X_1__and__Y1_true__and__Y2_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_false[\"Y3=false\"]\nS__and__X_2__and__Y1_true__and__Y2_true -- 1.0 --&gt; S__and__X_2__and__Y1_true__and__Y2_true__and__Y3_true[\"Y3=true\"]\nS__and__X_1__and__Y1_false__and__Y2_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_true[\"Y3=true\"]\nS__and__X_1__and__Y1_false__and__Y2_true__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_false__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_true__and__Y2_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_true[\"Y3=true\"]\nS__and__X_1__and__Y1_true__and__Y2_true__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_true__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_false__and__Y2_true__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_false__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_true__and__Y2_true__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_false__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_false__and__Y2_false__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_true__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_true__and__Y2_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_true[\"Y3=true\"]\nS__and__X_1__and__Y1_true__and__Y2_false__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_false__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_false__and__Y2_false__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_true__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_false__and__Y2_true__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_true__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_false__and__Y2_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_false[\"Y3=false\"]\nS__and__X_0__and__Y1_false__and__Y2_false -- 1.0 --&gt; S__and__X_0__and__Y1_false__and__Y2_false__and__Y3_false[\"Y3=false\"]\n\n\n\n\n\n\n\nQuestion: Assuming the path are summed left to right in the above diagram, what is the last term in the LOTUS’ iterated sum? Use the convention ‘false’ = 0, ‘true’ = 1.\n\n\n\n\n\n\nClick for choices\n\n\n\n\n\n\n4/3\n3/4\n4\n1/3\nNone of the above.\n\n\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nThe last term, \\(g(x, y_1, y_2, y_3, y_4) p(x, y_1, y_2, y_3, y_4)\\), for \\(x = 2, y_1 = y_2 = y_3 = y_4 = 1\\) is the product of: \\[\ng(2, 1, 1, 1, 1) = 2(1 + 1) = 4,\n\\] and, via chain rule, \\[\np(x, y_1, y_2, y_3, y_4) = \\frac{1}{3} \\times 1 \\times 1 \\times 1 \\times 1 = 1/3\n\\]",
    "crumbs": [
      "Probability essentials",
      "Expectations"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic01_outcomes.html",
    "href": "w01_discrete_inference/topic01_outcomes.html",
    "title": "Sample space, outcomes, events",
    "section": "",
    "text": "Review of basic probability theory concepts: outcome, event, sample space\nIntuition from the Bayesian perspective\n\n\n\n\n\nOne definition of Bayesian inference: applying probability theory to statistical inference problems\n\nTherefore, it is critical to understand probability to learn Bayesian inference\nThis week, we will help you “reload in memory” some of the most important bits of probability theory used in this course",
    "crumbs": [
      "Probability essentials",
      "Sample space, outcomes, events"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic01_outcomes.html#outline",
    "href": "w01_discrete_inference/topic01_outcomes.html#outline",
    "title": "Sample space, outcomes, events",
    "section": "",
    "text": "Review of basic probability theory concepts: outcome, event, sample space\nIntuition from the Bayesian perspective\n\n\n\n\n\nOne definition of Bayesian inference: applying probability theory to statistical inference problems\n\nTherefore, it is critical to understand probability to learn Bayesian inference\nThis week, we will help you “reload in memory” some of the most important bits of probability theory used in this course",
    "crumbs": [
      "Probability essentials",
      "Sample space, outcomes, events"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic01_outcomes.html#definitions",
    "href": "w01_discrete_inference/topic01_outcomes.html#definitions",
    "title": "Sample space, outcomes, events",
    "section": "Definitions",
    "text": "Definitions\n\nSample space, denoted \\(S\\), a set.\n\nExample: \\(S = \\{1, 2, 3, 4\\}\\) (see Figure).\n\nEach element \\(s\\) of \\(S\\) is called an outcome, \\(s \\in S\\).\n\nExample: each of the 4 points.\n\nA set of outcomes \\(E \\subset S\\) is called an event.\n\nExample: \\(E = \\{s \\in S : s \\text{ is odd}\\}\\) (red in the Figure).",
    "crumbs": [
      "Probability essentials",
      "Sample space, outcomes, events"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic01_outcomes.html#intuition-bayesian-view",
    "href": "w01_discrete_inference/topic01_outcomes.html#intuition-bayesian-view",
    "title": "Sample space, outcomes, events",
    "section": "Intuition: Bayesian view",
    "text": "Intuition: Bayesian view\n\nIn Bayesian statistics, an outcome will describe the state of the world.\nWe do not know which outcome is the true state of the world.\nWe observe partial information on the state of the world/outcome.\nWe rule out the outcomes that are not consistent with the observation…\n…but there will be several outcomes left!\n\nWe will deal with this situation using probability theory.",
    "crumbs": [
      "Probability essentials",
      "Sample space, outcomes, events"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic01_outcomes.html#intuition-randomized-algorithms",
    "href": "w01_discrete_inference/topic01_outcomes.html#intuition-randomized-algorithms",
    "title": "Sample space, outcomes, events",
    "section": "Intuition: randomized algorithms",
    "text": "Intuition: randomized algorithms\n\nAn algorithm is “randomized” if it has access to virtual dices/coins.\nIn practice this is done using pseudorandom number generators.\nIn this context an outcome is a random seed, i.e. the initialization of the pseudorandom number generator.",
    "crumbs": [
      "Probability essentials",
      "Sample space, outcomes, events"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic03_random_variables.html",
    "href": "w01_discrete_inference/topic03_random_variables.html",
    "title": "Random variables",
    "section": "",
    "text": "Random variable as mathematical objects.\nNotation convention for observation/latent\n\n\n\n\nRandom variables are used as building blocks for two key uses in Bayesian stats: modelling “knowns” (observations) and “unknowns” (latent variables/parameters/prediction).",
    "crumbs": [
      "Probability essentials",
      "Random variables"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic03_random_variables.html#outline",
    "href": "w01_discrete_inference/topic03_random_variables.html#outline",
    "title": "Random variables",
    "section": "",
    "text": "Random variable as mathematical objects.\nNotation convention for observation/latent\n\n\n\n\nRandom variables are used as building blocks for two key uses in Bayesian stats: modelling “knowns” (observations) and “unknowns” (latent variables/parameters/prediction).",
    "crumbs": [
      "Probability essentials",
      "Random variables"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic03_random_variables.html#definition",
    "href": "w01_discrete_inference/topic03_random_variables.html#definition",
    "title": "Random variables",
    "section": "Definition",
    "text": "Definition\nA (real) random variable is a function from a sample space \\(S\\) to the reals, \\(X : S \\to \\mathbb{R}\\).\nExample:\n\nContinuing the example with \\(S = \\{1, 2, 3, 4\\}\\).\nConsider \\(X(s) = 1\\) if \\(s\\) is odd, and \\(X(s) = 0\\) otherwise.",
    "crumbs": [
      "Probability essentials",
      "Random variables"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic03_random_variables.html#probabilists-notation",
    "href": "w01_discrete_inference/topic03_random_variables.html#probabilists-notation",
    "title": "Random variables",
    "section": "Probabilist’s notation",
    "text": "Probabilist’s notation\n\nLet \\(X\\) denote a random variable.\nThe notation \\((X = 1)\\) or \\((X \\in E)\\) is invalid in set theory.\nTherefore, probabilists “gave it a meaning” as follows:\n\n\\[(X = 1) = \\{s : X(s) = 1\\}.\\]\nExample: Consider \\(X(s) = 1\\) if \\(s\\) is odd, and \\(X(s) = 0\\) otherwise. Then \\((X = 1)\\) corresponds to the red circle.",
    "crumbs": [
      "Probability essentials",
      "Random variables"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic03_random_variables.html#sec-conventions-probability-vs-bayesian",
    "href": "w01_discrete_inference/topic03_random_variables.html#sec-conventions-probability-vs-bayesian",
    "title": "Random variables",
    "section": "Conventions: probability vs Bayesian",
    "text": "Conventions: probability vs Bayesian\nProbability convention:\n\nRandom variables are denoted with capitals in probability theory\nThe same letter in small cap is used for a dummy variable holding the output of the random variable.\n\nNote: “A dummy variable holding the output of the random variable” is called a realization.\nExample: \\(X\\) for the random variable and \\(x\\) for its realization.\n\nWe will start off using this convention in the first few weeks.\n\nBayesian statistics convention:\n\nOften the capitalization convention is not used in the Bayesian statistics literature.\nHence we will eventually drop the probability theory capitalization convention.",
    "crumbs": [
      "Probability essentials",
      "Random variables"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic03_random_variables.html#more-conventions",
    "href": "w01_discrete_inference/topic03_random_variables.html#more-conventions",
    "title": "Random variables",
    "section": "More conventions",
    "text": "More conventions\n\n\\(X\\): unobserved random variable (synonym of “unobserved”: latent)\n\\(Y\\): observed random variable\n\nMore precisely:\n\n\\(Y\\) is the “mechanism of observation”..\nwhereas the actual observation is a realization \\(y\\) of \\(Y\\).",
    "crumbs": [
      "Probability essentials",
      "Random variables"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic03_random_variables.html#extension",
    "href": "w01_discrete_inference/topic03_random_variables.html#extension",
    "title": "Random variables",
    "section": "Extension",
    "text": "Extension\nA random vector is a function from a sample space to \\(\\mathbb{R}^n\\).\nExample in Bayesian statistics: the vector \\((X, Y)\\) containing both the unobserved and observed quantities.",
    "crumbs": [
      "Probability essentials",
      "Random variables"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic06_forward_sampling.html",
    "href": "w01_discrete_inference/topic06_forward_sampling.html",
    "title": "Forward sampling",
    "section": "",
    "text": "Notion of forward sampling (also known as forward simulation)\nHow to do it in practice\n\nUseful functions\nGraphical models\n\n\n\n\n\n\nSampling is the main way Bayesian inference is performed nowadays.\nWe introduce here the simplest flavour of sampling, forward sampling.\nBayesian inference mostly uses a more complicated type of sampling called posterior sampling which we will cover later.\nBut forward sampling is still helpful to help debug Bayesian inference software as we will see soon.",
    "crumbs": [
      "Probability essentials",
      "Forward sampling"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic06_forward_sampling.html#outline",
    "href": "w01_discrete_inference/topic06_forward_sampling.html#outline",
    "title": "Forward sampling",
    "section": "",
    "text": "Notion of forward sampling (also known as forward simulation)\nHow to do it in practice\n\nUseful functions\nGraphical models\n\n\n\n\n\n\nSampling is the main way Bayesian inference is performed nowadays.\nWe introduce here the simplest flavour of sampling, forward sampling.\nBayesian inference mostly uses a more complicated type of sampling called posterior sampling which we will cover later.\nBut forward sampling is still helpful to help debug Bayesian inference software as we will see soon.",
    "crumbs": [
      "Probability essentials",
      "Forward sampling"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic06_forward_sampling.html#forward-sampling-as-depth-first-traversal",
    "href": "w01_discrete_inference/topic06_forward_sampling.html#forward-sampling-as-depth-first-traversal",
    "title": "Forward sampling",
    "section": "Forward sampling as depth-first traversal",
    "text": "Forward sampling as depth-first traversal\nRecall our recurring bag sampling example, with its corresponding decision tree:\n\n\n\n\n\n\nflowchart TD\nS__and__X_0 -- 1.0 --&gt; S__and__X_0__and__Y1_false[\"Y1=false\"]\nS__and__X_2__and__Y1_true -- 1.0 --&gt; S__and__X_2__and__Y1_true__and__Y2_true[\"Y2=true\"]\nS -- 0.33 --&gt; S__and__X_0[\"X=0\"]\nS__and__X_1__and__Y1_true__and__Y2_true__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_true__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_true__and__Y2_false__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_true__and__Y4_false[\"Y4=false\"]\nS__and__X_0__and__Y1_false__and__Y2_false__and__Y3_false -- 1.0 --&gt; S__and__X_0__and__Y1_false__and__Y2_false__and__Y3_false__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_false__and__Y2_false__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_false__and__Y4_true[\"Y4=true\"]\nS -- 0.33 --&gt; S__and__X_1[\"X=1\"]\nS__and__X_2__and__Y1_true__and__Y2_true__and__Y3_true -- 1.0 --&gt; S__and__X_2__and__Y1_true__and__Y2_true__and__Y3_true__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_true__and__Y2_false__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_false__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_false__and__Y2_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_false[\"Y3=false\"]\nS__and__X_1__and__Y1_true__and__Y2_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_false[\"Y3=false\"]\nS__and__X_1__and__Y1_false__and__Y2_false__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_false__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_true__and__Y2_true__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_false__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_true__and__Y2_false__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_true__and__Y4_true[\"Y4=true\"]\nS__and__X_1 -- 0.5 --&gt; S__and__X_1__and__Y1_false[\"Y1=false\"]\nS__and__X_1__and__Y1_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false[\"Y2=false\"]\nS__and__X_1__and__Y1_false__and__Y2_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_true[\"Y3=true\"]\nS__and__X_1__and__Y1_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true[\"Y2=true\"]\nS__and__X_0__and__Y1_false -- 1.0 --&gt; S__and__X_0__and__Y1_false__and__Y2_false[\"Y2=false\"]\nS__and__X_1__and__Y1_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false[\"Y2=false\"]\nS__and__X_1__and__Y1_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true[\"Y2=true\"]\nS__and__X_2 -- 1.0 --&gt; S__and__X_2__and__Y1_true[\"Y1=true\"]\nS__and__X_1__and__Y1_false__and__Y2_true__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_true__and__Y4_false[\"Y4=false\"]\nS__and__X_1 -- 0.5 --&gt; S__and__X_1__and__Y1_true[\"Y1=true\"]\nS -- 0.33 --&gt; S__and__X_2[\"X=2\"]\nS__and__X_1__and__Y1_true__and__Y2_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_false[\"Y3=false\"]\nS__and__X_2__and__Y1_true__and__Y2_true -- 1.0 --&gt; S__and__X_2__and__Y1_true__and__Y2_true__and__Y3_true[\"Y3=true\"]\nS__and__X_1__and__Y1_false__and__Y2_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_true[\"Y3=true\"]\nS__and__X_1__and__Y1_false__and__Y2_true__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_false__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_true__and__Y2_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_true[\"Y3=true\"]\nS__and__X_1__and__Y1_true__and__Y2_true__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_true__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_false__and__Y2_true__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_false__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_true__and__Y2_true__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_false__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_false__and__Y2_false__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_true__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_true__and__Y2_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_true[\"Y3=true\"]\nS__and__X_1__and__Y1_true__and__Y2_false__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_false__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_false__and__Y2_false__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_true__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_false__and__Y2_true__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_true__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_false__and__Y2_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_false[\"Y3=false\"]\nS__and__X_0__and__Y1_false__and__Y2_false -- 1.0 --&gt; S__and__X_0__and__Y1_false__and__Y2_false__and__Y3_false[\"Y3=false\"]\n\n\n\n\n\n\n\n\nForward simulation is a type of tree traversal. I.e. moving from node to node in the tree.\nForward simulation is a recursive process initialized at the root of the decision tree (labelled \\(S\\)).\n\nWhen we are a node \\(v\\) in the tree, we pick one of \\(v\\)’s children at random.\n\nMore precisely, we use methods discussed in the previous section on simulation from PMFs\n\nWe recurse until we reach a leaf.\n\nFrom this leaf we obtain an outcome and hence a realization for all random variables, both “observed” and “unobserved.”",
    "crumbs": [
      "Probability essentials",
      "Forward sampling"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic06_forward_sampling.html#sec-model",
    "href": "w01_discrete_inference/topic06_forward_sampling.html#sec-model",
    "title": "Forward sampling",
    "section": "Forward sampling as specifying a model",
    "text": "Forward sampling as specifying a model\nWe have encountered that notation earlier:\n\\[\n\\begin{align*}\nX &\\sim {\\mathrm{Unif}}\\{0, 1, 2\\} \\\\\nY_i | X &\\sim {\\mathrm{Bern}}(X/2).\n\\end{align*}\n\\]\n\nThis notation is a recipe providing all the information required to perform forward sampling.\n\nSpecifically, the PMF to use at each recursion step.\nIn continuous models, it will be the same idea except that we will have a probability density instead of a PMF.",
    "crumbs": [
      "Probability essentials",
      "Forward sampling"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic06_forward_sampling.html#example-1",
    "href": "w01_discrete_inference/topic06_forward_sampling.html#example-1",
    "title": "Forward sampling",
    "section": "Example 1",
    "text": "Example 1\nYou will practice forward sampling in Ex1, Q1.1.2",
    "crumbs": [
      "Probability essentials",
      "Forward sampling"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic06_forward_sampling.html#example-2",
    "href": "w01_discrete_inference/topic06_forward_sampling.html#example-2",
    "title": "Forward sampling",
    "section": "Example 2",
    "text": "Example 2\nConsider the following model:\n\\[\n\\begin{align*}\nX &\\sim {\\mathrm{Unif}}\\{1, 2, 3, 4\\} \\\\\nY | X &\\sim {\\mathrm{Unif}}\\{1, \\dots, X\\}.\n\\end{align*}\n\\]\n\n\n\nInterpretation: you roll a D&D d4 dice (blue one on the image), then pick an integer uniformly between 1 and the number on the dice.\nExample of forward simulation code:\n\nrequire(extraDistr)\n\nLoading required package: extraDistr\n\nset.seed(4)\n\nforward_simulate_roll_and_pick &lt;- function() {\n  x &lt;- rdunif(1, min=1, max=4) # 1 uniform between 1-6, i.e. a dice\n  y &lt;- rdunif(1, min=1, max=x)\n  c(x, y) # return a vector with these two realizations\n}\n\nforward_simulate_roll_and_pick()\n\n[1] 3 1\n\nforward_simulate_roll_and_pick()\n\n[1] 2 1\n\nforward_simulate_roll_and_pick()\n\n[1] 4 2",
    "crumbs": [
      "Probability essentials",
      "Forward sampling"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic09_bayes.html",
    "href": "w01_discrete_inference/topic09_bayes.html",
    "title": "Bayes rule",
    "section": "",
    "text": "Bayes rule for discrete models\nVisual intuition\n\n\n\n\nFirst example of computing a posterior distribution, a key concept in Bayesian statistics.",
    "crumbs": [
      "Probability essentials",
      "Bayes rule"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic09_bayes.html#outline",
    "href": "w01_discrete_inference/topic09_bayes.html#outline",
    "title": "Bayes rule",
    "section": "",
    "text": "Bayes rule for discrete models\nVisual intuition\n\n\n\n\nFirst example of computing a posterior distribution, a key concept in Bayesian statistics.",
    "crumbs": [
      "Probability essentials",
      "Bayes rule"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic09_bayes.html#running-example",
    "href": "w01_discrete_inference/topic09_bayes.html#running-example",
    "title": "Bayes rule",
    "section": "Running example",
    "text": "Running example\n\nImagine a bag with 3 coins each with a different probability parameter \\(p\\)\nCoin \\(i\\in \\{0, 1, 2\\}\\) has bias \\(i/2\\)—in other words:\n\nFirst coin: bias is \\(0/2 = 0\\) (i.e. both sides are “heads”, \\(p = 0\\))\nSecond coin: bias is \\(1/2 = 0.5\\) (i.e. standard coin, \\(p = 1/2\\))\nThird coin: bias is \\(2/2 = 1\\) (i.e. both sides are “tails”, \\(p = 1\\))\n\n\n\n\n\n\nConsider the following two steps sampling process\n\nStep 1: pick one of the three coins, but do not look at it!\nStep 2: flip the coin 4 times\n\nMathematically, this probability model can be written as follows: \\[\n\\begin{align*}\nX &\\sim {\\mathrm{Unif}}\\{0, 1, 2\\} \\\\\nY_i | X &\\sim {\\mathrm{Bern}}(X/2)\n\\end{align*}\n\\tag{1}\\]\n\nConsider the second question in the first exercise:\nSuppose now that you observe the outcome of the 4 coin flips, but not the type of coin that was picked. Say you observe: “heads”, “heads”, “heads”, “heads” = [0, 0, 0, 0]. Given that observation, what is the probability that you picked the standard coin (i.e., the one with \\(p = 1/2\\))?",
    "crumbs": [
      "Probability essentials",
      "Bayes rule"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic09_bayes.html#strategy",
    "href": "w01_discrete_inference/topic09_bayes.html#strategy",
    "title": "Bayes rule",
    "section": "Strategy",
    "text": "Strategy\nDenote the observation by \\(y_{1:4} = (0, 0, 0, 0)\\). In the rest of the argument we will always fix \\(y\\) to that value.\n\nAttack the more general problem \\(\\pi(x) = \\mathbb{P}(X = x | Y_{1:4} = y_{1:4})\\) for all hypotheses \\(x \\in \\{0, 1, 2\\}\\) instead of just the requested \\(x = 1\\) (corresponding to the “standard coin”).\nBy definition of conditioning: \\[\\pi(x) = \\frac{\\mathbb{P}(X = x, Y_{1:4} = y_{1:4})}{\\mathbb{P}(Y_{1:4} = y_{1:4})}.\\] Let us call the numerator \\[\\gamma(x) = \\mathbb{P}(X = x, Y_{1:4} = y_{1:4}),\\] and the denominator, \\[Z = \\mathbb{P}(Y_{1:4} = y_{1:4}).\\]\nStart by computing \\(\\gamma(x)\\) for all \\(x\\). (using chain rule)\nNote \\(Z = \\gamma(0) + \\gamma(1) + \\gamma(2)\\) (why?).",
    "crumbs": [
      "Probability essentials",
      "Bayes rule"
    ]
  },
  {
    "objectID": "w08_mcmc1/topic06_hands_on.html",
    "href": "w08_mcmc1/topic06_hands_on.html",
    "title": "Stan: hands on",
    "section": "",
    "text": "We revisit the time-varying Ariane 1 rocket failure probability model to practice the Stan syntax introduced this week.\n\n\n\n\nsuppressPackageStartupMessages(require(rstan))\nsuppressPackageStartupMessages(require(ggplot2))\nsuppressPackageStartupMessages(require(dplyr))\n\nset.seed(1)\n\ndf = read.csv(url(\"https://raw.githubusercontent.com/UBC-Stat-ML/web447/main/data/launches.csv\")) %&gt;% filter(LV.Type == \"Ariane 1\")\nsuccess_indicators = df$Suc_bin\nrmarkdown::paged_table(df)\n\n\n  \n\n\n\n\nplot(success_indicators, xlab = \"Launch index i\")",
    "crumbs": [
      "Intro to MCMC",
      "Stan: hands on"
    ]
  },
  {
    "objectID": "w08_mcmc1/topic06_hands_on.html#outline",
    "href": "w08_mcmc1/topic06_hands_on.html#outline",
    "title": "Stan: hands on",
    "section": "",
    "text": "We revisit the time-varying Ariane 1 rocket failure probability model to practice the Stan syntax introduced this week.\n\n\n\n\nsuppressPackageStartupMessages(require(rstan))\nsuppressPackageStartupMessages(require(ggplot2))\nsuppressPackageStartupMessages(require(dplyr))\n\nset.seed(1)\n\ndf = read.csv(url(\"https://raw.githubusercontent.com/UBC-Stat-ML/web447/main/data/launches.csv\")) %&gt;% filter(LV.Type == \"Ariane 1\")\nsuccess_indicators = df$Suc_bin\nrmarkdown::paged_table(df)\n\n\n  \n\n\n\n\nplot(success_indicators, xlab = \"Launch index i\")",
    "crumbs": [
      "Intro to MCMC",
      "Stan: hands on"
    ]
  },
  {
    "objectID": "w08_mcmc1/topic06_hands_on.html#model",
    "href": "w08_mcmc1/topic06_hands_on.html#model",
    "title": "Stan: hands on",
    "section": "Model",
    "text": "Model\nRecall the model we discussed previously (with the difference that we use a SD of 10 now)\n\\[\\begin{align*}\n\\text{slope} &\\sim \\mathcal{N}(0, 10) \\\\\n\\text{intercept} &\\sim \\mathcal{N}(0, 10) \\\\\n\\theta(i) &= \\text{logistic}(\\text{slope} \\cdot i + \\text{intercept}) \\\\\ny_i &\\sim {\\mathrm{Bern}}(\\theta(i))\n\\end{align*}\\]\n…which you also implemented in simPPLe as part of exercise 4:\n\nlogistic_regression = function() {\n  intercept = simulate(Norm(0, 10))\n  slope     = simulate(Norm(0, 10))\n  for (i in seq_along(success_indicators)){\n    success_probability = plogis(intercept + i*slope)\n    observe(success_indicators[i], Bern(success_probability))\n  }\n  return(c(intercept, slope))\n}",
    "crumbs": [
      "Intro to MCMC",
      "Stan: hands on"
    ]
  },
  {
    "objectID": "w08_mcmc1/topic06_hands_on.html#translation-into-stan",
    "href": "w08_mcmc1/topic06_hands_on.html#translation-into-stan",
    "title": "Stan: hands on",
    "section": "Translation into Stan",
    "text": "Translation into Stan\nQuestion 1: use the template below to translate the above model into Stan. Set the seed to 1, run 10000 MCMC iterations, and report the posterior mean of the slope parameter.\ndata { \n  int N; \n  array[N] int y; \n}\n\nparameters { \n  real slope;\n  real intercept;\n}\n\nmodel {\n  // complete this\n}",
    "crumbs": [
      "Intro to MCMC",
      "Stan: hands on"
    ]
  },
  {
    "objectID": "w08_mcmc1/topic06_hands_on.html#using-the-posterior",
    "href": "w08_mcmc1/topic06_hands_on.html#using-the-posterior",
    "title": "Stan: hands on",
    "section": "Using the posterior",
    "text": "Using the posterior\nQuestion 2: compute a 95% credible interval on the slope parameter.\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the output of print on the fit object.\n\n\n\nQuestion 3: translate “your updated belief that the Ariane 1 rockets were improving” into a mathematical expression.\nQuestion 4: estimate the numerical value of the expression in the last question.",
    "crumbs": [
      "Intro to MCMC",
      "Stan: hands on"
    ]
  },
  {
    "objectID": "w08_mcmc1/topic03_more_complex_example.html",
    "href": "w08_mcmc1/topic03_more_complex_example.html",
    "title": "Stan: going further",
    "section": "",
    "text": "More complex types in Stan (vectors, constraints)\nLoops and vectorization\nFeeding complex data into Stan\n\n\n\n\nWe review here the constructs needed to write more complex models such as Q2 in the current exercise, more similar exercises coming in the next weeks, and the second quiz.",
    "crumbs": [
      "Intro to MCMC",
      "Stan: going further"
    ]
  },
  {
    "objectID": "w08_mcmc1/topic03_more_complex_example.html#outline",
    "href": "w08_mcmc1/topic03_more_complex_example.html#outline",
    "title": "Stan: going further",
    "section": "",
    "text": "More complex types in Stan (vectors, constraints)\nLoops and vectorization\nFeeding complex data into Stan\n\n\n\n\nWe review here the constructs needed to write more complex models such as Q2 in the current exercise, more similar exercises coming in the next weeks, and the second quiz.",
    "crumbs": [
      "Intro to MCMC",
      "Stan: going further"
    ]
  },
  {
    "objectID": "w08_mcmc1/topic03_more_complex_example.html#example",
    "href": "w08_mcmc1/topic03_more_complex_example.html#example",
    "title": "Stan: going further",
    "section": "Example",
    "text": "Example\nThe data below consists, for each day in December 2021, of the percentage of sequenced COVID-19 samples that are of the Omicron variant.\n\nsuppressPackageStartupMessages(require(rstan))\nsuppressPackageStartupMessages(require(ggplot2))\nsuppressPackageStartupMessages(require(dplyr))\n\n# data from: https://data.chhs.ca.gov/dataset/covid-19-variant-data\ndf = read.csv(\"../data/covid19_variants.csv\")\ndf$date = as.Date(df$date,format=\"%Y-%m-%d\")\n\ndf = df %&gt;% filter(date &gt; \"2021-12-01\" & date &lt; \"2021-12-31\") %&gt;% filter(variant_name == \"Omicron\")\n\ndf %&gt;% ggplot(aes(x = date, y = percentage)) + geom_point() + ylim(0, 100) + theme_minimal()",
    "crumbs": [
      "Intro to MCMC",
      "Stan: going further"
    ]
  },
  {
    "objectID": "w08_mcmc1/topic03_more_complex_example.html#model-mathematical-notation",
    "href": "w08_mcmc1/topic03_more_complex_example.html#model-mathematical-notation",
    "title": "Stan: going further",
    "section": "Model (mathematical notation)",
    "text": "Model (mathematical notation)\nRecall one possible model for this data: for \\(i \\in \\{1, 2, \\dots, N\\}\\), \\(N\\) being the number of days in December (\\(N=31\\)),1\n\\[\\begin{align*}\n\\theta_0 &\\sim {\\mathrm{Exp}}(1) \\\\\n\\theta_1 &\\sim \\mathcal{N}(0, \\sigma = 1000) \\\\\n\\theta_2 &\\sim \\mathcal{N}(0, \\sigma = 1000) \\\\\n\\mu_i &= \\text{logistic}(\\theta_1 (i/N) + \\theta_2) \\\\\ny_i &\\sim {\\mathrm{Beta}}(\\mu_i, \\theta_0) \\\\\n\\end{align*}\\]",
    "crumbs": [
      "Intro to MCMC",
      "Stan: going further"
    ]
  },
  {
    "objectID": "w08_mcmc1/topic03_more_complex_example.html#model-in-stan",
    "href": "w08_mcmc1/topic03_more_complex_example.html#model-in-stan",
    "title": "Stan: going further",
    "section": "Model (in Stan)",
    "text": "Model (in Stan)\nLet us translate the mathematical notation into Stan:\n\n\n// comments in Stan use '//' not '#'\n\ndata { \n  // Here `N` is the number of days considered. \n  int N; \n  \n  // meaning: `y` is a vector of length `N` where each entry is between zero and one.\n  vector&lt;lower=0, upper=1&gt;[N] y; \n}\n\n// As before, we declare in `parameters` the types of the unobserved (latent) random variables. \nparameters { \n  real&lt;lower=0&gt; concentration; \n  real slope;\n  real intercept;\n}\n\nmodel {\n  concentration ~ exponential(1);\n  slope ~ normal(0, 1000);\n  intercept ~ normal(0, 1000);\n  \n  for (i in 1:N) { // Stan is 1-indexed\n    // inv_logit is Stan's name for the logistic function\n1    y[i] ~ beta_proportion(\n              inv_logit(intercept + slope * (i/N)), \n              concentration); \n  }\n}\n\n\n1\n\nHere, beta_proportion corresponds to the mean-concentration parameterization we discussed in the lecture on hierarchical models.",
    "crumbs": [
      "Intro to MCMC",
      "Stan: going further"
    ]
  },
  {
    "objectID": "w08_mcmc1/topic03_more_complex_example.html#improving-our-model-with-transformed-parameters",
    "href": "w08_mcmc1/topic03_more_complex_example.html#improving-our-model-with-transformed-parameters",
    "title": "Stan: going further",
    "section": "Improving our model with transformed parameters",
    "text": "Improving our model with transformed parameters\n\nThe code in the last section will not print out \\(\\mu_i\\)\nThe code below shows how to fix this\n\n\\(\\mu_i\\) is handled differently than \\(\\theta_k\\), \\(y_i\\)…\n… because \\(\\mu_i\\) is defined with \\(=\\) instead of \\(\\sim\\)\n\n\n\n\ndata { \n  int N; \n  vector&lt;lower=0, upper=1&gt;[N] y; \n}\n\nparameters { \n  real&lt;lower=0&gt; concentration; \n  real slope;\n  real intercept;\n}\n\n1transformed parameters {\n  // linspaced_vector(N,0,1) creates a vector \n  //    of N equispace points between 0 and 1\n  //    (we normalize the dates to be between zero and one)\n  // functions in Stan are typically vectorized, \n  //    this is the case for example with inv_logit\n  vector[N] mu = \n    inv_logit(intercept + slope*linspaced_vector(N,0,1));\n}\n\n2model {\n  concentration ~ exponential(1);\n  slope ~ normal(0, 1000);\n  intercept ~ normal(0, 1000);\n  \n  // Another example of vectorization---the line below will produce the same \n  // output as the loop in the previous version, but slightly faster:\n  y ~ beta_proportion(mu, concentration);\n}\n\n\n1\n\nWhenever in the mathematical notation a latent random variable is defined using an equality (“\\(=\\)”), input it in the transformed parameters block.\n\n2\n\nWhenever in the mathematical notation a latent random variable is defined using a distribution statement (“\\(\\sim\\)”), input it in the model block.",
    "crumbs": [
      "Intro to MCMC",
      "Stan: going further"
    ]
  },
  {
    "objectID": "w08_mcmc1/topic03_more_complex_example.html#running-mcmc",
    "href": "w08_mcmc1/topic03_more_complex_example.html#running-mcmc",
    "title": "Stan: going further",
    "section": "Running MCMC",
    "text": "Running MCMC\nRunning MCMC on the model is done as before:2\n\ndata_without_zeros = pmax(pmin(df$percentage/100,0.999),0.001)\n\nfit = sampling(\n  omicron, \n1  data = list(\n          y = data_without_zeros, \n          N = length(data_without_zeros)\n        ), \n2  chains = 1,\n  iter = 10000       \n)\n\n\n1\n\nEach variable in Stan’s data block needs to be fed a value when called from R.\n\n2\n\nThe chains options can be used to simulate several independent MCMC chains, more precisely, this is available after calling the function options(mc.cores = parallel::detectCores()).\n\n\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 2.8e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.28 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.141 seconds (Warm-up)\nChain 1:                0.165 seconds (Sampling)\nChain 1:                0.306 seconds (Total)\nChain 1:",
    "crumbs": [
      "Intro to MCMC",
      "Stan: going further"
    ]
  },
  {
    "objectID": "w08_mcmc1/topic03_more_complex_example.html#posterior-visualization",
    "href": "w08_mcmc1/topic03_more_complex_example.html#posterior-visualization",
    "title": "Stan: going further",
    "section": "Posterior visualization",
    "text": "Posterior visualization\nWe can extract samples as follows:\n\nsamples = extract(fit)$mu\ndata = df$percentage\nn_samples = nrow(samples)\n\nand plot the posterior distribution over the beta’s mean functions:\n\nxs = 1:length(data) / length(data)\nplot(xs, data/100, \n      xlab = \"Fraction of the month of December, 2021\", \n      ylab = \"Omicron fraction\")\n\nfor (i in 1:n_samples) {\n  lines(xs, samples[i,], col = rgb(red = 0, green = 0, blue = 0, alpha = 0.01))\n}",
    "crumbs": [
      "Intro to MCMC",
      "Stan: going further"
    ]
  },
  {
    "objectID": "w08_mcmc1/topic03_more_complex_example.html#model-criticism",
    "href": "w08_mcmc1/topic03_more_complex_example.html#model-criticism",
    "title": "Stan: going further",
    "section": "Model criticism",
    "text": "Model criticism\nWhat are potential weakness(es) of the model?",
    "crumbs": [
      "Intro to MCMC",
      "Stan: going further"
    ]
  },
  {
    "objectID": "w08_mcmc1/topic03_more_complex_example.html#footnotes",
    "href": "w08_mcmc1/topic03_more_complex_example.html#footnotes",
    "title": "Stan: going further",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn the model below, notice we normalize the day by \\(N\\) to put it in the interval \\([0, 1]\\). This is not strictly necessary, but often, bringing covariates in a nice range can make sampling faster when it improves the condition number of the sampling problem.↩︎\nnote we make sure no datapoints have value zero or one as under certain parameter values of the beta distribution, this can lead to a zero density point which Stan is not able to handle.↩︎",
    "crumbs": [
      "Intro to MCMC",
      "Stan: going further"
    ]
  },
  {
    "objectID": "w08_mcmc1/topic02_stan_basics.html",
    "href": "w08_mcmc1/topic02_stan_basics.html",
    "title": "Stan basics",
    "section": "",
    "text": "Anatomy of a simple Stan program.\nInterpreting the output of Stan.\n\n\n\n\nWe will write many Stan models in the coming weeks, so we will cover here the key concepts needed to write in Stan. Similarities with simPPLe will help in this process.",
    "crumbs": [
      "Intro to MCMC",
      "Stan basics"
    ]
  },
  {
    "objectID": "w08_mcmc1/topic02_stan_basics.html#outline",
    "href": "w08_mcmc1/topic02_stan_basics.html#outline",
    "title": "Stan basics",
    "section": "",
    "text": "Anatomy of a simple Stan program.\nInterpreting the output of Stan.\n\n\n\n\nWe will write many Stan models in the coming weeks, so we will cover here the key concepts needed to write in Stan. Similarities with simPPLe will help in this process.",
    "crumbs": [
      "Intro to MCMC",
      "Stan basics"
    ]
  },
  {
    "objectID": "w08_mcmc1/topic02_stan_basics.html#example",
    "href": "w08_mcmc1/topic02_stan_basics.html#example",
    "title": "Stan basics",
    "section": "Example",
    "text": "Example\n\nReview: code in simPPLe and analytic answer\nLet us revisit the Doomsday model, which we wrote in simPPLe as follows:\n\n\n\n\nsource(\"../../solutions/simple.R\")\n\ndoomsday_model &lt;- function() {\n  x = simulate(Unif(0, 5))\n  observe(0.06, Unif(0, x))\n  return(x)\n}\n\nposterior(doomsday_model, 100000)\n\n[1] 1.103447\n\n\nRecall the analytic answer that we computed in clicker questions was: \\(\\approx 1.117\\).\n\n\nCode in Stan\nToday we will re-write this model in Stan instead of simPPLe. The main difference is that:\n\nboth simulate and observe will be denoted by ~ in Stan,\n\nto differentiate between observed and latent, Stan instead uses variable declarations (as data or parameters).\n\nFirst, load rstan:\n\nsuppressPackageStartupMessages(require(rstan))\n\nThen, as you did to test your install put the following code into a .stan file:\n\n1data {\n  real y; \n}\n\n2parameters {\n  real&lt;lower=0, upper=5&gt; x;\n}\n\nmodel {\n3  x ~ uniform(0, 5);\n4  y ~ uniform(0, x);\n}\n\n\n1\n\nVariables introduced in a data block will be treated as observed.\n\n2\n\nVariables introduced in a parameters block will be treated as latent, i.e., unobserved.\n\n\n3\n\nSince x is declared latent (i.e., inside a parameters block), Stan will know to treat this as the counterpart of simPPLe’s x = simulate(Unif(0, 5)).\n\n4\n\nSince y is declared observed (i.e., inside a data block), Stan will know to treat this as the counterpart of simPPLe’s observe(0.06, Unif(0, x)).\n\n\n\n\nQuestion: How does Stan gets the actual observed value \\(y = 0.06\\)?\nAnswer: When Stan gets called in R:1\n\n\nfit = sampling(\n  doomsday,         \n1  data = list(y = 0.06),\n  chains = 1,\n2  iter = 100000\n)\n\n\n1\n\nPass in the values taken by the observed random variables.\n\n2\n\nThe number of samples to compute (equivalent to \\(M\\) in our Monte Carlo notation)\n\n\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 4e-06 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:     1 / 100000 [  0%]  (Warmup)\nChain 1: Iteration: 10000 / 100000 [ 10%]  (Warmup)\nChain 1: Iteration: 20000 / 100000 [ 20%]  (Warmup)\nChain 1: Iteration: 30000 / 100000 [ 30%]  (Warmup)\nChain 1: Iteration: 40000 / 100000 [ 40%]  (Warmup)\nChain 1: Iteration: 50000 / 100000 [ 50%]  (Warmup)\nChain 1: Iteration: 50001 / 100000 [ 50%]  (Sampling)\nChain 1: Iteration: 60000 / 100000 [ 60%]  (Sampling)\nChain 1: Iteration: 70000 / 100000 [ 70%]  (Sampling)\nChain 1: Iteration: 80000 / 100000 [ 80%]  (Sampling)\nChain 1: Iteration: 90000 / 100000 [ 90%]  (Sampling)\nChain 1: Iteration: 100000 / 100000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.253 seconds (Warm-up)\nChain 1:                0.157 seconds (Sampling)\nChain 1:                0.41 seconds (Total)\nChain 1: \n\n\nWe can now compare the approximation provided by Stan:\n\nfit\n\nInference for Stan model: anon_model.\n1 chains, each with iter=1e+05; warmup=50000; thin=1; \npost-warmup draws per chain=50000, total post-warmup draws=50000.\n\n      mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nx     1.12    0.01 1.25  0.07  0.19  0.55  1.68  4.45 10585    1\nlp__ -0.37    0.01 0.63 -2.21 -0.41 -0.12 -0.04 -0.01  9987    1\n\nSamples were drawn using NUTS(diag_e) at Tue Mar  5 11:45:15 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).",
    "crumbs": [
      "Intro to MCMC",
      "Stan basics"
    ]
  },
  {
    "objectID": "w08_mcmc1/topic02_stan_basics.html#tips-and-tricks",
    "href": "w08_mcmc1/topic02_stan_basics.html#tips-and-tricks",
    "title": "Stan basics",
    "section": "Tips and tricks",
    "text": "Tips and tricks\n\nNotice statements in Stan have to end in semicolumns, ;\nTo find out Stan’s name for distributions, see Stan’s documentation,\n\ne.g., this is the documentation for uniform.",
    "crumbs": [
      "Intro to MCMC",
      "Stan basics"
    ]
  },
  {
    "objectID": "w08_mcmc1/topic02_stan_basics.html#interpreting-the-output",
    "href": "w08_mcmc1/topic02_stan_basics.html#interpreting-the-output",
    "title": "Stan basics",
    "section": "Interpreting the output",
    "text": "Interpreting the output\n\nJust as simple Monte Carlo and SNIS, the output of Stan/MCMC is a list of samples\nIn contrast to SNIS, the samples are equally weighted,\n\nin other words, for MCMC we have \\(W^{(m)}= 1/M\\),\ni.e., their structure is the same as samples from simple Monte Carlo!\n\n\nTo access the samples from a “fit” object when calling Stan, use:\n\nsamples = extract(fit)\n\nhead(samples$x)\n\n[1] 0.7224132 3.7228678 2.2768091 1.0077302 4.9788992 0.6378141\n\n\nAs a sanity check, let’s make sure all the samples are greater than the observed lower bound of \\(0.06\\):\n\nmin(samples$x)\n\n[1] 0.06000474\n\n\nQuestion: denote the samples output from Stan by \\(X^{(1)}, X^{(2)}, \\dots, X^{(M)}\\). What formula should you use to approximate \\(\\mathbb{E}[g(X) | Y = y]\\) based on the Stan output?\n\nPlotting histograms\nSince the samples are equally weighted, this makes it simpler to create plots, e.g., for a histogram:\n\nhist(samples$x)\n\n\n\n\n\n\n\n\nYou can also use the bayesplot package which will be handy to create more complex plots from MCMC runs:\n\nsuppressPackageStartupMessages(require(bayesplot))\nmcmc_hist(fit, pars = c(\"x\"))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.",
    "crumbs": [
      "Intro to MCMC",
      "Stan basics"
    ]
  },
  {
    "objectID": "w08_mcmc1/topic02_stan_basics.html#footnotes",
    "href": "w08_mcmc1/topic02_stan_basics.html#footnotes",
    "title": "Stan basics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nwhen calling Stan from an R script, use stan(file = ...), but when calling Stan from R Markdown (as we do here), use sampling(name, ...), where the first argument, name matches the code block argument output.var = \"name\" (see the source of this page for an example).↩︎",
    "crumbs": [
      "Intro to MCMC",
      "Stan basics"
    ]
  },
  {
    "objectID": "w04_glms/topic03_normal.html",
    "href": "w04_glms/topic03_normal.html",
    "title": "Normal distributions",
    "section": "",
    "text": "Quick review of the normal distribution.\nDifferent parameterizations.\n\n\n\n\nThe normal distribution is often used in Bayesian analysis when one needs a prior over an unknown \\(x\\) such that \\(x \\in (-\\infty, \\infty) = \\mathbb{R}\\).",
    "crumbs": [
      "The joy of probabilistic modelling",
      "Normal distributions"
    ]
  },
  {
    "objectID": "w04_glms/topic03_normal.html#outline",
    "href": "w04_glms/topic03_normal.html#outline",
    "title": "Normal distributions",
    "section": "",
    "text": "Quick review of the normal distribution.\nDifferent parameterizations.\n\n\n\n\nThe normal distribution is often used in Bayesian analysis when one needs a prior over an unknown \\(x\\) such that \\(x \\in (-\\infty, \\infty) = \\mathbb{R}\\).",
    "crumbs": [
      "The joy of probabilistic modelling",
      "Normal distributions"
    ]
  },
  {
    "objectID": "w04_glms/topic03_normal.html#examples-of-normal-densities",
    "href": "w04_glms/topic03_normal.html#examples-of-normal-densities",
    "title": "Normal distributions",
    "section": "Examples of normal densities",
    "text": "Examples of normal densities",
    "crumbs": [
      "The joy of probabilistic modelling",
      "Normal distributions"
    ]
  },
  {
    "objectID": "w04_glms/topic03_normal.html#parameterizations",
    "href": "w04_glms/topic03_normal.html#parameterizations",
    "title": "Normal distributions",
    "section": "Parameterizations",
    "text": "Parameterizations\n\nThere are different conventions to measure the spread.\n\nStandard deviation \\(\\sigma\\).\nVariance, \\(\\sigma^2\\).\nPrecision, \\(\\tau = 1/\\sigma^2\\).\n\nKeep that in mind as different languages will use different conventions!\nStandard deviation is the most intuitive:\n\nit is the width of the bell,\nthe only one that has the same units as \\(x\\) (e.g. if \\(x\\) is in meters, so is \\(\\sigma\\)).",
    "crumbs": [
      "The joy of probabilistic modelling",
      "Normal distributions"
    ]
  },
  {
    "objectID": "w04_glms/topic05_regression.html",
    "href": "w04_glms/topic05_regression.html",
    "title": "(Normal) regression",
    "section": "",
    "text": "A second example of a Bayesian model based on a linear model, this time with a normal likelihood.\nPriors on the positive real line.\nHomoscedasticity and heteroscedasticity.\nMarginal posterior.\n\n\n\n\nTogether with classification (previous page), regression is the other major statistical task frequently encountered.\nWe see here that the same approach as we took for classification can be easily modified to do regression.\nThis provides us with a second example of Bayesian GLMs.",
    "crumbs": [
      "The joy of probabilistic modelling",
      "(Normal) regression"
    ]
  },
  {
    "objectID": "w04_glms/topic05_regression.html#outline",
    "href": "w04_glms/topic05_regression.html#outline",
    "title": "(Normal) regression",
    "section": "",
    "text": "A second example of a Bayesian model based on a linear model, this time with a normal likelihood.\nPriors on the positive real line.\nHomoscedasticity and heteroscedasticity.\nMarginal posterior.\n\n\n\n\nTogether with classification (previous page), regression is the other major statistical task frequently encountered.\nWe see here that the same approach as we took for classification can be easily modified to do regression.\nThis provides us with a second example of Bayesian GLMs.",
    "crumbs": [
      "The joy of probabilistic modelling",
      "(Normal) regression"
    ]
  },
  {
    "objectID": "w04_glms/topic05_regression.html#example",
    "href": "w04_glms/topic05_regression.html#example",
    "title": "(Normal) regression",
    "section": "Example",
    "text": "Example\n\nIn the early 20th century, astronomers made the startling observation that pretty much all galaxies are moving away from ours. Why?\nWe now know this is because the universe is expanding.\nHere is a metaphor to help understand this:\n\nImagine ants on an inflating balloon.\nYou are one of the ants…\n…and you notice that all the others ants are moving away from you…\n\n…and the further the neighbor ant, the faster it looks like it is moving away from you.\n\n\nIn 1929 the astronomer Edwin Hubble published a paper1 on the relationship between distance and velocity of galaxies relative to us.\nIt is now called Hubble’s law.\nThe estimated slope of the relationship, known as Hubble’s constant, leads to an estimate of the age of the universe.\n\n\n\n\n\nData\n\nWe will estimate Hubble’s constant using data from the original data used by Edwin Hubble (in CSV).\nTo make the numbers less extreme in the following, I will divide the velocities by 1000\n\nInstructions:\n\ndownload the CSV\ncopy paste the code below and run it on your computer (after changing the path “../data” to the path on your computer)\n\n\nsuppressPackageStartupMessages(require(\"dplyr\"))\ndf = read.csv(\"../data/hubble-1.csv\") %&gt;%\n    rename(distance = R..Mpc.) %&gt;%\n    rename(velocity = v..km.sec.)\ndf$velocity = df$velocity/1000\nrmarkdown::paged_table(df)\n\n\n  \n\n\n\n\nHere is some EDA on that dataset:\n\n\nplot(df$distance, df$velocity, xlab = \"distance\", ylab = \"velocity\")",
    "crumbs": [
      "The joy of probabilistic modelling",
      "(Normal) regression"
    ]
  },
  {
    "objectID": "w04_glms/topic05_regression.html#building-a-bayesian-regression-model",
    "href": "w04_glms/topic05_regression.html#building-a-bayesian-regression-model",
    "title": "(Normal) regression",
    "section": "Building a Bayesian regression model",
    "text": "Building a Bayesian regression model\nGoal:\n\ndesigning a model containing a “slope” parameter,\nfrom which we will compute \\(\\mathbb{E}[\\text{slope} | \\text{data}]\\).\n\nTo achieve our goal, we will complete the gap in the following code:\nsource(\"../../solutions/simple.R\")\n\nregression = function() {\n  # priors will be defined here\n  # ...\n  for (i in 1:nrow(df)) { \n    distance = df[i, \"distance\"]\n    velocity = df[i, \"velocity\"]\n    # likelihood will be defined here\n  }\n  return(slope)\n}\n\nposterior(regression, 1000)\n\nRecall: to build a model, start with the observations.\nSpecifically, let us build a model for the observed velocity first.\n\nQuestion: what would be a reasonable likelihood for the velocity?\n\nvelocity \\(\\sim\\) Exponential(…)\nvelocity \\(\\sim\\) Uniform(…)\nvelocity \\(\\sim\\) t(…)\nvelocity \\(\\sim\\) Normal(…)\nNone of the above.\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nWe have that velocity \\(\\in \\mathbb{R}\\), so the exponential (\\(\\in (0, \\infty)\\)) and the uniform (\\(\\in [a, b]\\)) are not great choices.\nBoth the normal and the t-distribution are reasonable.\nThe t-distribution can have fatter tails, so if the data has outliers this may be preferable but we will stick with the simpler normal choice today.\nNote: galaxies can recede from us faster than the speed of light!.\n\n\n\n\n\n\n\n\n\nClick for next step\n\n\n\n\n\nQuestion: What should we use as the mean parameter of the normal likelihood?\nUse the fact that the scientific question is “to find a linear relationship of the velocity as a function of the distance.”\n\nvelocity \\(\\sim\\) Normal(0, …)\nvelocity \\(\\sim\\) Normal(distance, …)\nvelocity \\(\\sim\\) Normal(distance * slope, …)\nvelocity \\(\\sim\\) Normal(distance * slope + intercept, …)\nNone of the above.\n\n\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nHere the best answer is:\nvelocity \\(\\sim\\) Normal(distance * slope, …).\nWe do not need an intercept, because we know that by definition, our velocity relative to ourself is zero.\n\n\n\n\n\n\n\n\n\nClick for next step\n\n\n\n\n\nQuestion: What should we use as the standard deviation parameter of the normal likelihood?\n\nvelocity \\(\\sim\\) Normal(…, some constant)\nvelocity \\(\\sim\\) Normal(…, some random variable)\nvelocity \\(\\sim\\) Normal(…, some random variable * distance)\nNone of the above.\n\n\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nUsing a constant standard deviation is not ideal as the model will be sensitive to that choice.\nThe other choices are a priori reasonable and more domain knowledge would be needed to pick one. The choice:\nvelocity \\(\\sim\\) Normal(…, some random variable * distance)\nmeans that we expect that the variability of the galaxy velocities increase for galaxies that are further away. This could happen for example if the variability is dominated by measurement error for a measurement technique where errors increase linearly with distance.\nThe choice:\nvelocity \\(\\sim\\) Normal(…, some random variable)\nwould be more reasonable if the measurement error is negligible and the variability comes from other phenomena affecting the velocity of galaxies which are not expected to behave differently for close vs far galaxies (in our metaphor, think of ants walking in random directions at the same time as the balloon is expanding).\nThese are examples of heteroscedasticity (variability changes with input variable) and homoscedasticity (variability is invariant to input).\nLet us pick an homoscedasticity model. To summarize what we have so far:\nvelocity \\(\\sim\\) Normal(distance * slope, sd).\n\n\n\n\n\n\n\n\n\nClick for next step\n\n\n\n\n\nQuestion: Pick priors for the “slope” and “sd” parameters\n\nslope \\(\\sim\\) Normal(…), sd \\(\\sim\\) Normal(…)\nslope \\(\\sim\\) Normal(…), sd \\(\\sim\\) Exponential(…)\nslope \\(\\sim\\) Exponential(…), sd \\(\\sim\\) Exponential(…)\nNone of the above.\n\n\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nWe do not want a normal for the standard deviation (sd) parameter, because negative sd’s are not allowed.\nUsing an exponential for the slope would violate Cromwell’s rule.\nHence the best choice here is:\nslope \\(\\sim\\) Normal(…), sd \\(\\sim\\) Exponential(…).\nNote: for implementation purpose, you will need to use specific values for the prior’s parameters, let us agree on these ones:\n\\[\\begin{align*}\n  \\text{slope} &\\sim \\text{Normal}(0, 1) \\\\\n  \\text{sd} &\\sim \\text{Exponential}(10).\n\\end{align*}\\]\n\n\n\nQuestion: complete the code and approximate \\(\\mathbb{E}[\\text{slope} | \\text{data}]\\).\n\n\n\n\n\n\n\n\nClick for choices\n\n\n\n\n\nPick the closest:\n\n\\(\\approx 0.1\\)\n\\(\\approx 0.5\\)\n\\(\\approx 1\\)\n\\(\\approx 2\\)\n\n\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nsource(\"../../solutions/simple.R\")\nsource(\"../blocks/simple_utils.R\")\n\nregression = function() {\n  slope = simulate(Norm(0, 1))\n  sd = simulate(Exp(10))\n  for (i in 1:nrow(df)) { \n    distance = df[i, \"distance\"]\n    velocity = df[i, \"velocity\"]\n    observe(velocity, Norm(distance * slope, sd))\n  }\n  c(slope, sd)\n}\n\nposterior(regression, 1000)\n\n[1] 0.4312079 0.2373624",
    "crumbs": [
      "The joy of probabilistic modelling",
      "(Normal) regression"
    ]
  },
  {
    "objectID": "w04_glms/topic05_regression.html#visualization-of-the-posterior-distribution",
    "href": "w04_glms/topic05_regression.html#visualization-of-the-posterior-distribution",
    "title": "(Normal) regression",
    "section": "Visualization of the posterior distribution",
    "text": "Visualization of the posterior distribution\n\nsource(\"../blocks/simple_utils.R\")\nsource(\"../../solutions/simple.R\")\n\nposterior = posterior_particles(regression, 10000)\nweighted_scatter_plot(posterior, plot_options = list(xlab=\"slope parameter\", ylab=\"sd parameter\"))\n\n\n\n\n\n\n\n\n\nplot(df$distance, df$velocity, xlab = \"distance\", ylab = \"velocity\")\n\nxs = seq(0, 2, 0.01)\nsamples = posterior$samples \nnorm_weights = posterior$weights / sum(posterior$weights)\n\nfor (i in 1:nrow(samples)) {\n  slope     = samples[i, 1]\n  pr = norm_weights[i]\n  lines(xs, slope * xs, col = rgb(red = 0, green = 0, blue = 0, alpha = pr*20))\n}\n\n\n\n\n\n\n\n\nHere since the scientific question only concerns one parameter, the slope, it is useful to look at the marginal posterior distribution.\n\nIn mathematical expressions, obtained by marginalization: \\[p(x_1 | y) = \\int p(x_1, x_2 | y) \\mathrm{d}x_2.\\]\nIn Monte Carlo methods, much simpler:\n\neach sample is a pair \\(x^{(m)}= (x_1^{(m)}, x_2^{(m)})\\)\nif you are interested in \\(x_1\\) only, just ignore \\(x_2\\)!\n\n\nHere is an example of creating a histogram for the marginal posterior:\n\nlibrary(\"ggplot2\")\nposterior.as.df &lt;- data.frame(slopes = samples[,1], norm_weights)\nggplot(posterior.as.df, aes(x = slopes, weight = norm_weights)) + \n  geom_histogram(binwidth = 0.02) + \n  xlim(0.2, 0.6) +\n  xlab(\"slope parameter\") + \n  ylab(\"probability\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nQuestion: if you were to use an integral instead of Monte Carlo to create this histogram, what region of integration in the scatter plot above (the first plot in this subsection) would be used to compute the height of one histogram bin?\n\n\n\n\n\n\nClick for choices\n\n\n\n\n\n\n“—”\n“|”\n“◤”\n“◢”\nNone of the above\n\n\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nThe answer is “|”.",
    "crumbs": [
      "The joy of probabilistic modelling",
      "(Normal) regression"
    ]
  },
  {
    "objectID": "w04_glms/topic05_regression.html#model-criticism",
    "href": "w04_glms/topic05_regression.html#model-criticism",
    "title": "(Normal) regression",
    "section": "Model criticism",
    "text": "Model criticism\nThe answer we obtain is close to Edwin Hubble’s original estimate.\nThe modern estimate based on state-of-the-art measurements (space telescopes, advanced statistical models) gives an estimate that is about 10x smaller.\nDiscussion: What went wrong?\n\n\n\n\n\n\nClick for discussion\n\n\n\n\n\nIn summary: it is very hard to estimate distance of distant objects.\nNot only the model explored today lacks an error model on the distances, but moreover, Hubble’s distance estimates had systematic errors (i.e. were all offset by a constant), hence any model on this dataset (Bayesian or otherwise) would have likely lead to misleading results.\nIn fact, even with modern data and statistical methods, systematic errors are still suspected, this is known as the “Hubble tension.”",
    "crumbs": [
      "The joy of probabilistic modelling",
      "(Normal) regression"
    ]
  },
  {
    "objectID": "w04_glms/topic05_regression.html#references",
    "href": "w04_glms/topic05_regression.html#references",
    "title": "(Normal) regression",
    "section": "References",
    "text": "References\n\nThis example is inspired by William Welch’s Stat 305 lecture notes, where the same dataset is analyzed using MLE.\nLearn more about the still on-going research on resolving the “Hubble tension”.",
    "crumbs": [
      "The joy of probabilistic modelling",
      "(Normal) regression"
    ]
  },
  {
    "objectID": "w04_glms/topic05_regression.html#footnotes",
    "href": "w04_glms/topic05_regression.html#footnotes",
    "title": "(Normal) regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nProceedings of the National Academy of Sciences, Vol. 15, pp. 168–173.↩︎",
    "crumbs": [
      "The joy of probabilistic modelling",
      "(Normal) regression"
    ]
  },
  {
    "objectID": "w04_glms/topic01_simPPLe.html",
    "href": "w04_glms/topic01_simPPLe.html",
    "title": "simPPLe",
    "section": "",
    "text": "Last week’s Q2 exercise solution.\n\n\n\n\nWe will use simPPLe this week to do probabilistic modelling.",
    "crumbs": [
      "The joy of probabilistic modelling",
      "simPPLe"
    ]
  },
  {
    "objectID": "w04_glms/topic01_simPPLe.html#outline",
    "href": "w04_glms/topic01_simPPLe.html#outline",
    "title": "simPPLe",
    "section": "",
    "text": "Last week’s Q2 exercise solution.\n\n\n\n\nWe will use simPPLe this week to do probabilistic modelling.",
    "crumbs": [
      "The joy of probabilistic modelling",
      "simPPLe"
    ]
  },
  {
    "objectID": "w04_glms/topic01_simPPLe.html#background",
    "href": "w04_glms/topic01_simPPLe.html#background",
    "title": "simPPLe",
    "section": "Background",
    "text": "Background\nIf you missed Tuesday’s lecture last week, make sure to read the introduction to PPLs.",
    "crumbs": [
      "The joy of probabilistic modelling",
      "simPPLe"
    ]
  },
  {
    "objectID": "w04_glms/topic01_simPPLe.html#setting-up-simpple-on-your-computer",
    "href": "w04_glms/topic01_simPPLe.html#setting-up-simpple-on-your-computer",
    "title": "simPPLe",
    "section": "Setting up simPPLe on your computer",
    "text": "Setting up simPPLe on your computer\n\nLet us combine the “scaffold” files and the answer of last week’s Q2 into one file. Copy paste the code below into a file called simple.R:\n\n\n\nsimple.R\n\n###### solution\n\nposterior = function(ppl_function, number_of_iterations) {\n  numerator = 0.0\n  denominator = 0.0\n  for (i in 1:number_of_iterations) {\n    weight &lt;&lt;- 1.0\n    g_i = ppl_function()\n    numerator = numerator + weight * g_i\n    denominator = denominator + weight\n  }\n  return(numerator/denominator)\n}\n\n###### contents of the scaffold\n\nsuppressPackageStartupMessages(library(distr))\n\n## Utilities to make the distr library a bit nicer to use\n\np &lt;- function(distribution, realization) {\n  d(distribution)(realization) # return the PMF or density \n}\n\nBern = function(probability_to_get_one) {\n  DiscreteDistribution(supp = 0:1, prob = c(1-probability_to_get_one, probability_to_get_one))\n}\n\n## Key functions called by simPPLe programs\n\n# Use simulate(distribution) for unobserved random variables\nsimulate &lt;- function(distribution) {\n  r(distribution)(1) # sample once from the given distribution\n}\n\n# Use observe(realization, distribution) for observed random variables\nobserve = function(realization, distribution) {\n  # `&lt;&lt;-` lets us modify variables that live in the global scope from inside a function\n  weight &lt;&lt;- weight * p(distribution, realization) \n}\n\n\nThis way, you can now load simPPLe by loading simple.R into your session (note: in the code below replace ../../solutions/ by the path to the file you just created)\n\nsource(\"../../solutions/simple.R\")\n\n# define your model as a function, e.g. my_function\n\n# call: posterior(my_function, 100) to approximate posterior E[my_function | observations]",
    "crumbs": [
      "The joy of probabilistic modelling",
      "simPPLe"
    ]
  },
  {
    "objectID": "w04_glms/topic01_simPPLe.html#testing",
    "href": "w04_glms/topic01_simPPLe.html#testing",
    "title": "simPPLe",
    "section": "Testing",
    "text": "Testing\nI will start this Tuesday’s lecture with the following simPPLe test (a continuous version of the rocket model): (note: in the code below replace ../../solutions/ by the path to the file you just created)\nsource(\"../../solutions/simple.R\")\n\nset.seed(1)\n\ndata = rep(0, 4) \n\n# simPPLe's description of our \"bag of coins\" example\nbeta_binomial = function() {\n  \n  # Similar to forward sampling, but use 'observe' when the variable is observed\n  p = simulate(Beta(1, 1))\n  for (i in seq_along(data)) { \n    observe(data[i], Bern(p)) \n  }\n  \n  # return the test function, here the parameter p\n  return(p)\n}\n\nposterior(beta_binomial, 100)\n\nWhat is the output?\n\n\n\n\n\n\nClick for choices\n\n\n\n\n\n\n\\(0.0010394\\)\n\\(0.0194394\\)\n\\(0.0826293\\)\n\\(0.1728016\\)\nNone of the above\n\n\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\\(0.1728016\\).",
    "crumbs": [
      "The joy of probabilistic modelling",
      "simPPLe"
    ]
  },
  {
    "objectID": "w06_hierarchical/topic02_gm.html",
    "href": "w06_hierarchical/topic02_gm.html",
    "title": "Graphical models",
    "section": "",
    "text": "Directed graphical models.\n\n\n\n\nGraphical models help understanding complex models such as hierarchical ones",
    "crumbs": [
      "Hierarchical models",
      "Graphical models"
    ]
  },
  {
    "objectID": "w06_hierarchical/topic02_gm.html#outline",
    "href": "w06_hierarchical/topic02_gm.html#outline",
    "title": "Graphical models",
    "section": "",
    "text": "Directed graphical models.\n\n\n\n\nGraphical models help understanding complex models such as hierarchical ones",
    "crumbs": [
      "Hierarchical models",
      "Graphical models"
    ]
  },
  {
    "objectID": "w06_hierarchical/topic02_gm.html#directed-graphical-model",
    "href": "w06_hierarchical/topic02_gm.html#directed-graphical-model",
    "title": "Graphical models",
    "section": "Directed graphical model",
    "text": "Directed graphical model\nA graphical model is a discrete graph showing the dependencies involved in a forward sampling description of a model. Specifically:\nDefinition: a directed graphical model is a discrete graph such that:\n\nthe set of vertices is a set of random variables,\nthere is an edge \\(X_1 \\to X_2\\) if the forward sampling step producing \\(X_2\\) requires knowing \\(X_1\\).",
    "crumbs": [
      "Hierarchical models",
      "Graphical models"
    ]
  },
  {
    "objectID": "w06_hierarchical/topic02_gm.html#examples",
    "href": "w06_hierarchical/topic02_gm.html#examples",
    "title": "Graphical models",
    "section": "Examples",
    "text": "Examples\n\nExchangeable observations\nMany simples models can be written as:\n\n\n\n\\[\\begin{align*}\n\\theta &\\sim \\text{SomePrior} \\\\\ny_i | \\theta &\\sim \\text{SomeLikelihood}(\\theta).\n\\end{align*}\\]\nMost of the models we have encountered so far have this form.\n\n\nHidden Markov model\nTime series are often modelled as follows:\n\n\n\n\nthere is an unobserved state \\(x_t\\) changing at each time step \\(t \\in \\{1, 2, \\dots\\}\\)\n\ne.g. true location of a plane,\n\nwe have noisy observations \\(y_t\\) from this true position,\n\ne.g. information from a radar.\n\n\nSimplest HMM example:\n\\[\\begin{align*}\nx_i | x_{i-1} &\\sim \\mathcal{N}(x_{i-1}, \\sigma_1) \\\\\ny_i | x_i &\\sim \\mathcal{N}(x_{i}, \\sigma_2),\n\\end{align*}\\]\nwhere \\(\\sigma_1\\) controls the amount of variability in the dynamics, and \\(\\sigma_2\\), in the measurement.",
    "crumbs": [
      "Hierarchical models",
      "Graphical models"
    ]
  },
  {
    "objectID": "w06_hierarchical/topic02_gm.html#conditioning",
    "href": "w06_hierarchical/topic02_gm.html#conditioning",
    "title": "Graphical models",
    "section": "Conditioning",
    "text": "Conditioning\nShaded nodes in a directed graphical models indicate what we condition on (i.e. what we observe).",
    "crumbs": [
      "Hierarchical models",
      "Graphical models"
    ]
  },
  {
    "objectID": "w06_hierarchical/topic02_gm.html#plates",
    "href": "w06_hierarchical/topic02_gm.html#plates",
    "title": "Graphical models",
    "section": "Plates",
    "text": "Plates\n\n\n\nPlate: When there are too many vertices to draw in a graphical model, use a square (“plate”) to indicate a group of nodes that are repeated several times. In other words, a graphical “for loop.”",
    "crumbs": [
      "Hierarchical models",
      "Graphical models"
    ]
  },
  {
    "objectID": "w06_hierarchical/topic02_gm.html#further-pointers-on-graphical-models",
    "href": "w06_hierarchical/topic02_gm.html#further-pointers-on-graphical-models",
    "title": "Graphical models",
    "section": "Further pointers on graphical models",
    "text": "Further pointers on graphical models\nDirected graphical models are also useful to identify conditional independence relationships (recall, \\(A\\) is conditionally independent of \\(B\\) given \\(C\\) if \\(\\Pr(A|C) \\Pr(B|C) = \\Pr(A \\cap B | C)\\)).\nIf you are curious, watch this video to see how this is done, or read the original paper.",
    "crumbs": [
      "Hierarchical models",
      "Graphical models"
    ]
  },
  {
    "objectID": "blocks/stan_templates/from_notebook/notebook.html",
    "href": "blocks/stan_templates/from_notebook/notebook.html",
    "title": "Running Stan from notebook",
    "section": "",
    "text": "Quarto is an open source notebook platform integrated with RStudio: download from here.\n(These instructions should also work for Rmarkdown, just change the extension to .Rmd instead of .qmd)."
  },
  {
    "objectID": "blocks/stan_templates/from_notebook/notebook.html#install-quarto",
    "href": "blocks/stan_templates/from_notebook/notebook.html#install-quarto",
    "title": "Running Stan from notebook",
    "section": "",
    "text": "Quarto is an open source notebook platform integrated with RStudio: download from here.\n(These instructions should also work for Rmarkdown, just change the extension to .Rmd instead of .qmd)."
  },
  {
    "objectID": "blocks/stan_templates/from_notebook/notebook.html#import-required-packages",
    "href": "blocks/stan_templates/from_notebook/notebook.html#import-required-packages",
    "title": "Running Stan from notebook",
    "section": "Import required packages",
    "text": "Import required packages\n\nsuppressPackageStartupMessages(require(rstan))"
  },
  {
    "objectID": "blocks/stan_templates/from_notebook/notebook.html#write-the-stan-model-in-a-stan-cell",
    "href": "blocks/stan_templates/from_notebook/notebook.html#write-the-stan-model-in-a-stan-cell",
    "title": "Running Stan from notebook",
    "section": "Write the Stan model in a Stan cell",
    "text": "Write the Stan model in a Stan cell\n\ndata {\n  int&lt;lower=0&gt; n;         // number of trials\n  int&lt;lower=0,upper=n&gt; k; // number of successes\n}\n\nparameters {\n  real&lt;lower=0,upper=1&gt; p;\n}\n\nmodel {\n  // prior\n  p ~ beta(1,1);\n\n  // likelihood\n  k ~ binomial(n, p);\n}"
  },
  {
    "objectID": "blocks/stan_templates/from_notebook/notebook.html#call-stan-from-a-r-cell",
    "href": "blocks/stan_templates/from_notebook/notebook.html#call-stan-from-a-r-cell",
    "title": "Running Stan from notebook",
    "section": "Call Stan from a R cell",
    "text": "Call Stan from a R cell\n\nfit = sampling(\n  betabinomial,\n  seed = 1,\n  data = list(n=3, k=3),        # named list of data\n  iter = 1000                   # number of samples to draw\n)"
  },
  {
    "objectID": "blocks/stan_templates/from_notebook/notebook.html#summarize-the-output",
    "href": "blocks/stan_templates/from_notebook/notebook.html#summarize-the-output",
    "title": "Running Stan from notebook",
    "section": "Summarize the output",
    "text": "Summarize the output\n\nfit\n\nInference for Stan model: anon_model.\n4 chains, each with iter=1000; warmup=500; thin=1; \npost-warmup draws per chain=500, total post-warmup draws=2000.\n\n      mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\np     0.79    0.01 0.16  0.40  0.69  0.83  0.92  0.99   730 1.00\nlp__ -3.04    0.03 0.72 -4.92 -3.24 -2.76 -2.56 -2.50   785 1.01\n\nSamples were drawn using NUTS(diag_e) at Thu Mar  7 12:54:49 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1)."
  },
  {
    "objectID": "w06_hierarchical/topic03_hierarchy.html",
    "href": "w06_hierarchical/topic03_hierarchy.html",
    "title": "Intro to hierarchical models",
    "section": "",
    "text": "Hierarchical models.\nIntuition why they decrease prior sensitivity.\n\n\n\n\nHierarchical models, a crowning achievement of Bayesian inference, can be used to decrease prior sensitivity even when the dataset of interest is of limited size.",
    "crumbs": [
      "Hierarchical models",
      "Intro to hierarchical models"
    ]
  },
  {
    "objectID": "w06_hierarchical/topic03_hierarchy.html#outline",
    "href": "w06_hierarchical/topic03_hierarchy.html#outline",
    "title": "Intro to hierarchical models",
    "section": "",
    "text": "Hierarchical models.\nIntuition why they decrease prior sensitivity.\n\n\n\n\nHierarchical models, a crowning achievement of Bayesian inference, can be used to decrease prior sensitivity even when the dataset of interest is of limited size.",
    "crumbs": [
      "Hierarchical models",
      "Intro to hierarchical models"
    ]
  },
  {
    "objectID": "w06_hierarchical/topic03_hierarchy.html#example",
    "href": "w06_hierarchical/topic03_hierarchy.html#example",
    "title": "Intro to hierarchical models",
    "section": "Example",
    "text": "Example\n\n\n\nExample: predicting the probability of failure of the next Delta 7925H launch (so far, we observed this type of rocket has been launched 3 times, with 0 failed launches).\nKey idea: use “side data” to inform the prior…\n\nFor example: success/failure launch data from other other types of rockets.\nCan we use the following rocket launch dataset to inform prediction for a single rocket type of interest?\n\n\nsuppressPackageStartupMessages(require(\"dplyr\"))\ndf = read.csv(\"../data/failure_counts.csv\") \nrmarkdown::paged_table(df)",
    "crumbs": [
      "Hierarchical models",
      "Intro to hierarchical models"
    ]
  },
  {
    "objectID": "w06_hierarchical/topic03_hierarchy.html#how-to-use-side-data-two-suboptimal-approaches",
    "href": "w06_hierarchical/topic03_hierarchy.html#how-to-use-side-data-two-suboptimal-approaches",
    "title": "Intro to hierarchical models",
    "section": "How to use “side data”? Two suboptimal approaches",
    "text": "How to use “side data”? Two suboptimal approaches\nFor pedagogy, we will first cover two simple heuristics to incorporate “side data” and describe their limitations.\nThen we will cover hierarchical models in the next section, showing how they address the limitations of simpler methods.\n\nFirst try (do not use this one!)\n\nMerge all the data into one type?\nI.e. just sum the columns in the data:\n\n\nsum(df$numberOfLaunches)\n\n[1] 5667\n\nsum(df$numberOfFailures)\n\n[1] 431\n\n\nWhy merging everything is a bad idea?\n\n\nTowards an improved way to use side data\n\nBackground: “mean–pseudo-sample-size” reparameterization of the Beta distribution.\n\nA reparametrization is a different labelling of a family such that you can go back and forth between the two labellings.\nConsider \\[\\alpha  = \\mu s, \\;\\; \\beta = (1 - \\mu) s\\] where \\(\\mu \\in (0, 1)\\), \\(s &gt; 0\\).\n\nInterpretation:\n\n\\(\\mu\\): mean of the Beta\n\\(s\\): measure of “peakiness” of the density, higher \\(s\\) corresponds to more peaked; roughly, \\(s \\approx\\) number of data points that would make the posterior peaked like that.\n\nWhy we did this reparameterization?\n\n\\(\\to\\) it should now be more intuitive how we can use “side data” to inform \\(\\mu\\)…\n\n\n\n\nSecond try (still suboptimal, but not as bad)\n\nEstimate a failure probability \\(\\hat p_i\\) for each type of rocket \\(i\\).\nFit a distribution \\(p(\\mu)\\) on those \\(\\{\\hat p_i\\}\\).\nUse this distribution \\(p(\\mu)\\) as the prior on \\(\\mu\\)?\n\n\nsuppressPackageStartupMessages(require(\"ggplot2\"))\nsuppressPackageStartupMessages(require(\"latex2exp\"))\n\nggplot(df, aes(x = numberOfFailures / numberOfLaunches)) +\n  geom_histogram() + \n  xlab(TeX(\"$\\\\hat{p}_i = \\\\textrm{numberOfFailures}_i / \\\\textrm{numberOfLaunches}_i$\")) +\n  geom_rug(alpha = 0.1) + \n  theme_minimal()\n\n\n\n\n\n\n\n\nQuestion: what is the weakness of using a prior fitted on this histogram?",
    "crumbs": [
      "Hierarchical models",
      "Intro to hierarchical models"
    ]
  },
  {
    "objectID": "w06_hierarchical/topic03_hierarchy.html#hierarchical-models-a-better-way-to-use-side-data",
    "href": "w06_hierarchical/topic03_hierarchy.html#hierarchical-models-a-better-way-to-use-side-data",
    "title": "Intro to hierarchical models",
    "section": "Hierarchical models: a better way to use side data",
    "text": "Hierarchical models: a better way to use side data\nSolution: go fully Bayesian!\nRecall: our Bayesian recipe…\n\nConstruct a probability model including\n\nrandom variables for what we will measure/observe\nrandom variables for the unknown quantities\n\nthose we are interested in (“parameters”, “predictions”)\nothers that just help us formulate the problem (“nuisance”, “random effects”).\n\n\nCompute the posterior distribution conditionally on the actual data at hand\nUse the posterior distribution to make a decision\n\nWhat it means here: we model the launcher type’s population parameters (\\(\\mu, s\\)) as random variables. We are not interested in these other launcher types, but they help us inform inference about the type of rocket we are interested in (i.e., “nuisance” parameters).\n\nGraphical model\n\n\n\nMathematical description\n\nShare these two “population parameters” across all launch types \\[p_i | \\mu, s \\sim {\\mathrm{Beta}}(\\mu s, (1 - \\mu) s)\\]\nLikelihood same as before: \\(F_i | p_i \\sim {\\mathrm{Binom}}(n_i, p_i).\\)\nWe still need to put prior on \\(\\mu\\) and \\(s\\)…\n\n…but as we’ll discuss next, you should expect this prior choice to be less sensitive.\nExample: \\(\\mu \\sim {\\mathrm{Beta}}(1,1) = {\\mathrm{Unif}}(0, 1)\\), \\(s \\sim {\\mathrm{Exp}}(1/10000)\\)\n\n\nPS: why we pick such a small value, \\(1/10000\\)?\n\nRecall the parameter of an exponential is a rate which is 1/mean.\nSo a mean of 10000 encodes we put significant mass to values up to \\(O(10000)\\).\nThis is sometimes described as a “vague prior”.\n\n\n\nDecrease prior choice sensitivity\nIt seems we have introduced new problems as now we again have hyper-parameters, namely those for the priors on \\(\\mu\\) and \\(s\\)!\nKey point: yes, but now we are less sensitive to these choices! Why?\nHeuristic: look at the node \\(n\\) connected to the hyper-parameters…\n\nIf most of \\(n\\)’s edges link to hyper-parameters: posterior more sensitive to hyper-parameters.\nIf most of \\(n\\)’s edges link to random variables connected to data: posterior will probably be less sensitive.\n\nBefore going hierarchical: for maiden/early flights we had\n\nNote: \\(p\\) is connected to only a small number of observations.\nAfter going hierarchical:\n\nNote: \\(\\mu, s\\) is connected to a large population of \\(p_i\\)’s, each linked with data.",
    "crumbs": [
      "Hierarchical models",
      "Intro to hierarchical models"
    ]
  },
  {
    "objectID": "w06_hierarchical/topic01_priors.html",
    "href": "w06_hierarchical/topic01_priors.html",
    "title": "Prior choice",
    "section": "",
    "text": "Strategies for constructing priors.\nSituations where choosing the prior matters.\nInformative vs non-informative priors.\n\n\n\n\nSelection of priors is a necessary step in the construction of Bayesian models. Difficulties that can be encountered in prior choice motivate hierarchical models, introduced this week.",
    "crumbs": [
      "Hierarchical models",
      "Prior choice"
    ]
  },
  {
    "objectID": "w06_hierarchical/topic01_priors.html#outline",
    "href": "w06_hierarchical/topic01_priors.html#outline",
    "title": "Prior choice",
    "section": "",
    "text": "Strategies for constructing priors.\nSituations where choosing the prior matters.\nInformative vs non-informative priors.\n\n\n\n\nSelection of priors is a necessary step in the construction of Bayesian models. Difficulties that can be encountered in prior choice motivate hierarchical models, introduced this week.",
    "crumbs": [
      "Hierarchical models",
      "Prior choice"
    ]
  },
  {
    "objectID": "w06_hierarchical/topic01_priors.html#example",
    "href": "w06_hierarchical/topic01_priors.html#example",
    "title": "Prior choice",
    "section": "Example",
    "text": "Example\nConsider the problem of selecting a prior for a parameter defined on \\([0, 1]\\), e.g. \\(p\\) in the basic rocket launch success probability example (iid, continuous): \\[\\begin{align*}\np &\\sim \\;\\text{???} \\\\\ny | p &\\sim {\\mathrm{Binom}}(n, p).\n\\end{align*}\\]",
    "crumbs": [
      "Hierarchical models",
      "Prior choice"
    ]
  },
  {
    "objectID": "w06_hierarchical/topic01_priors.html#general-approach",
    "href": "w06_hierarchical/topic01_priors.html#general-approach",
    "title": "Prior choice",
    "section": "General approach",
    "text": "General approach\nOften, the choice of prior is approached in two stages\n\nFirst, pick a distribution family\n\nin our example: let us pick say \\({\\color{red} {\\mathrm{Beta}}}(\\cdot, \\cdot)\\),\n\na common starting point for distribution with support \\([0, 1]\\)\nbut not the only choice, e.g., an alternative is the Kumaraswamy distribution.\n\n\nThen, pick one member of this family\n\ne.g. Beta\\(({\\color{red} 1}, {\\color{red} 2})\\), but how to pick these “magic” numbers (called hyper-parameters)?",
    "crumbs": [
      "Hierarchical models",
      "Prior choice"
    ]
  },
  {
    "objectID": "w06_hierarchical/topic01_priors.html#when-does-the-choice-of-prior-matter",
    "href": "w06_hierarchical/topic01_priors.html#when-does-the-choice-of-prior-matter",
    "title": "Prior choice",
    "section": "When does the choice of prior matter?",
    "text": "When does the choice of prior matter?\n\nWhen data is large, posterior tends to be less sensitive to specification of the prior\n\nTheoretical reason: the “Bayesian central limit theorem” (Bernstein von-Mises theorem).\nNot always true (e.g. partially identifiable model).\n\nWhen data is small, posterior tends to be more sensitive to specification of the prior\n\nExtreme example:\n\nconsider a rocket maiden flight,\ni.e. there is no observation available yet,\nQuestion: what will be the posterior?",
    "crumbs": [
      "Hierarchical models",
      "Prior choice"
    ]
  },
  {
    "objectID": "w06_hierarchical/topic01_priors.html#strategies-for-constructing-priors",
    "href": "w06_hierarchical/topic01_priors.html#strategies-for-constructing-priors",
    "title": "Prior choice",
    "section": "Strategies for constructing priors",
    "text": "Strategies for constructing priors\n\n“Informative priors”: use expert knowledge to determine the prior (“prior elicitation”)\n\ne.g.: when we build rockets, there is a lot of quality control, so it would be surprising to see very low values for the success probability parameter \\(p\\).\nMany prior elicitation techniques developed, see Petrus et al, 2021 for a recent review,\nbut the state of that literature not completely satisfactory.\n\n“Non-informative priors”: use properties of the likelihood to determine prior\n\nmore advanced, see Robert, Sections 3.5,\nnot automated, case-by-case mathematical derivation often intractable.\nAutomating this into PPLs is an open problem.\n\nToday: side-step these issues thanks to Hierarchical models.\n\nWe will not remove the need for prior choice, instead we will decrease sensitivity of the posterior to these choices.",
    "crumbs": [
      "Hierarchical models",
      "Prior choice"
    ]
  },
  {
    "objectID": "w06_hierarchical/topic01_priors.html#example-continued",
    "href": "w06_hierarchical/topic01_priors.html#example-continued",
    "title": "Prior choice",
    "section": "Example, continued",
    "text": "Example, continued\nSuppose we picked a Beta, so we now have to pick a member of the Beta family.\n\\[\\begin{align*}\np &\\sim {\\mathrm{Beta}}(?, ?) \\\\\ny | p &\\sim {\\mathrm{Binom}}(n, p).\n\\end{align*}\\]\nAgain the numbers to fill in the “?” are known as hyper-parameters, i.e., an hyper-parameter is a parameter of a prior.\nExpert elicitation:\n\nfind a rocket expert,\nask the expert what they think is a reasonable range of values for \\(p\\), e.g. upper and lower quartiles \\(Q_1, Q_3\\).\nUse numerical method to fit \\(\\alpha, \\beta\\) such that a Beta(\\(\\alpha, \\beta\\)) matches the reported quartiles \\(Q_1, Q_3\\).",
    "crumbs": [
      "Hierarchical models",
      "Prior choice"
    ]
  },
  {
    "objectID": "w04_glms/topic06_glms.html",
    "href": "w04_glms/topic06_glms.html",
    "title": "Bayesian GLMs",
    "section": "",
    "text": "We have seen two models that share the same general structure:\n\n“Bernoulli regression” for binary observations\n“Normal regression” for continuous observations.",
    "crumbs": [
      "The joy of probabilistic modelling",
      "Bayesian GLMs"
    ]
  },
  {
    "objectID": "w04_glms/topic06_glms.html#summary-so-far",
    "href": "w04_glms/topic06_glms.html#summary-so-far",
    "title": "Bayesian GLMs",
    "section": "",
    "text": "We have seen two models that share the same general structure:\n\n“Bernoulli regression” for binary observations\n“Normal regression” for continuous observations.",
    "crumbs": [
      "The joy of probabilistic modelling",
      "Bayesian GLMs"
    ]
  },
  {
    "objectID": "w04_glms/topic06_glms.html#generalization-of-these-two-special-cases",
    "href": "w04_glms/topic06_glms.html#generalization-of-these-two-special-cases",
    "title": "Bayesian GLMs",
    "section": "Generalization of these two special cases",
    "text": "Generalization of these two special cases\n\nLook at the data type of the output variable, this will guide the choice of likelihood model\nAre you trying to predict..\n\na real number? us a normal likelihood (or better: fat tail distribution such as t distribution, for robustness to outliers)\na non-negative integer? Replace Bernoulli by Poisson (or Negative Binomial, or other integer valued distribution)\netc\n\nGeneral pattern:\n\n\\[\\text{output} | \\text{inputs}, \\text{parameters} \\sim \\text{SuitableDistribution}(f(\\text{parameters},  \\text{inputs}))\\]\n\n\\(f\\) should map to the range of parameters permitted by your “SuitableDistribution”\nknown in the GLM literature as the inverse of the link function\noften a composition of a linear function with a non-linear “squashing” function if the output is constrained.",
    "crumbs": [
      "The joy of probabilistic modelling",
      "Bayesian GLMs"
    ]
  },
  {
    "objectID": "w04_glms/topic02_bivariate.html",
    "href": "w04_glms/topic02_bivariate.html",
    "title": "Bivariate posteriors",
    "section": "",
    "text": "Posterior distribution on two parameters (bivariate).\n\n\n\n\nThere is typically more than one unknown variables. Going from one unknown to two unknowns is the biggest leap.",
    "crumbs": [
      "The joy of probabilistic modelling",
      "Bivariate posteriors"
    ]
  },
  {
    "objectID": "w04_glms/topic02_bivariate.html#outline",
    "href": "w04_glms/topic02_bivariate.html#outline",
    "title": "Bivariate posteriors",
    "section": "",
    "text": "Posterior distribution on two parameters (bivariate).\n\n\n\n\nThere is typically more than one unknown variables. Going from one unknown to two unknowns is the biggest leap.",
    "crumbs": [
      "The joy of probabilistic modelling",
      "Bivariate posteriors"
    ]
  },
  {
    "objectID": "w04_glms/topic02_bivariate.html#example",
    "href": "w04_glms/topic02_bivariate.html#example",
    "title": "Bivariate posteriors",
    "section": "Example",
    "text": "Example\nWe will revisit the rocket comparison problem from the first week:\n\nWould you rather get strapped to…\n\n“shiny rocket” (option A): 1 success, 0 failures\n“rugged rocket” (option B): 98 successes, 2 failures\n\n\n\n\n\n\nIf you are sick of hearing about rockets, here is an alternative interpretation:\n\nImagine you work for Google and serve website ad banners.\nA client gives you two designs for the ad banner: A and B.\nYou will serve them to web visitors and the objective is to maximize the number of clicks.\n\nSuccess: a user clicked (1).\nFailure: a user did not click (0).\n\nTraining data:\n\nVersion A: 1 success, 0 failures.\nVersion B: 98 successes, 2 failures\n\nYou can only show the ad one more time to a user (client budget almost exhausted).\n\nShould you show A or B?\n\nThis is a basic example of what is known as A/B testing.\n\n\nModel: for \\(j \\in \\{A, B\\}\\), independently,\n\\[\\begin{align*}\np_j &\\sim {\\mathrm{Unif}}(0, 1) \\\\\ny_j | p_j &\\sim {\\mathrm{Binom}}(n_j, p_j),\n\\end{align*}\\]\nwhere:\n\n\\(p_j\\) are the success probabilities\n\\(n_j\\) is the number of times you showed \\(j\\)\n\\(y_j\\) is the number of clicks on \\(j\\).\n\nQuestion:\nHow to mathematically encode the questions:\n\n“Would you rather get strapped to…”\n“Should you show A or B?”\n\n\n\n\n\n\n\nClick for choices\n\n\n\n\n\n\n\\(\\mathbb{P}(p_A = 1)\\)\n\\(\\mathbb{P}(p_A = 1 | Y = y)\\)\n\\(\\mathbb{P}(p_A &gt; p_B)\\)\n\\(\\mathbb{P}(p_A &gt; p_B | Y = y)\\)\nNone of the above\n\n\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\\(\\mathbb{P}(p_A &gt; p_B | Y = y)\\): in words, is the chance of success of A greater than B, given the data?\n\n\n\nModify the code below to approximate that probability.\nsource(\"../../solutions/simple.R\")\nset.seed(1)\n\nbivariate = function() {\n  \n  p_A = simulate(Beta(1, 1))\n  observe(1, Binom(size = 1, prob = p_A)) \n  \n  # add the part of the model describing p_B (prior and likelihood)\n  \n  return # what is the \"test function\"?\n}\nposterior(bivariate, 10000)\n\n\n\n\n\n\nClick for choices\n\n\n\n\n\n\n\\(\\approx 0.1\\)\n\\(\\approx 0.3\\)\n\\(\\approx 0.6\\)\n\\(\\approx 0.9\\)\n\n\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nsource(\"../../solutions/simple.R\")\nset.seed(1)\n\nbivariate = function() {\n  \n  p_A = simulate(Beta(1, 1))\n  observe(1, Binom(size = 1, prob = p_A)) \n  \n  p_B = simulate(Beta(1, 1))\n  observe(98, Binom(size = 100, prob = p_B)) \n  \n  return(ifelse(p_A &gt; p_B, 1, 0))\n}\nposterior(bivariate, 10000)\n\n[1] 0.06909026",
    "crumbs": [
      "The joy of probabilistic modelling",
      "Bivariate posteriors"
    ]
  },
  {
    "objectID": "w04_glms/topic02_bivariate.html#visualization-of-a-bivariate-joint-posterior",
    "href": "w04_glms/topic02_bivariate.html#visualization-of-a-bivariate-joint-posterior",
    "title": "Bivariate posteriors",
    "section": "Visualization of a bivariate joint posterior",
    "text": "Visualization of a bivariate joint posterior\nLet us get more insight on joint posterior distributions (posterior over two variables).\nTo do so, we use visualizations where the x-axis is the first parameter (here \\(p_A\\)) and the y-axis is the second parameter (here \\(p_B\\)).\n\n# This contains useful functions to visualize the output of simPPLe:\nsource(\"../blocks/simple_utils.R\")\nsource(\"../../solutions/simple.R\")\n\nbivariate_pair = function() {\n  \n  p_A = simulate(Beta(1, 1))\n  observe(1, Binom(size = 1, prob = p_A)) \n  \n  p_B = simulate(Beta(1, 1))\n  observe(98, Binom(size = 100, prob = p_B)) \n  \n  # We modify the above to return a vector containing both parameters\n  c(p_A, p_B)\n}\n\n# To compute the plot, we need the list of samples and weights (\"particles\"), i.e. more details compared to posterior()\n# This is what posterior_particles accomplishes\nparticles = posterior_particles(bivariate_pair, 10000)\n\n# Now we use these to create the plot below\nweighted_scatter_plot(particles, plot_options = list(xlab=\"p_A\", ylab=\"p_B\"))\n\nxaxis = seq(0, 1, 0.01)\nlines(xaxis, xaxis)\n\n\n\n\n\n\n\n\n\nEach point is a sample produced by SNIS, \\(x^{(m)}= (p_A^{(m)}, p_B^{(m)})\\).\nWe encode its associated weight \\(w^{(m)}\\) by the transparency (alpha) of the point.\n\nQuestion: What region of integration should you use to compute \\(\\mathbb{P}(p_A &gt; p_B | Y = y)\\)?\n\n\n\n\n\n\nClick for choices\n\n\n\n\n\n\n◤\n◥\n◢\n◣\n\n\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n◢",
    "crumbs": [
      "The joy of probabilistic modelling",
      "Bivariate posteriors"
    ]
  },
  {
    "objectID": "w04_glms/topic04_classification.html",
    "href": "w04_glms/topic04_classification.html",
    "title": "Bernoulli regression",
    "section": "",
    "text": "A first example of a Bayesian model based on a linear model and a Bernoulli likelihood.\nPrior construction via prior predictive distribution.\nApproximation of the posterior using a PPL.\nVisualizing a posterior distribution over functions.\n\n\n\n\n\nThis is our first example of a Bayesian General Linear Model (GLM).\nGLMs are the probably the most common models.",
    "crumbs": [
      "The joy of probabilistic modelling",
      "Bernoulli regression"
    ]
  },
  {
    "objectID": "w04_glms/topic04_classification.html#outline",
    "href": "w04_glms/topic04_classification.html#outline",
    "title": "Bernoulli regression",
    "section": "",
    "text": "A first example of a Bayesian model based on a linear model and a Bernoulli likelihood.\nPrior construction via prior predictive distribution.\nApproximation of the posterior using a PPL.\nVisualizing a posterior distribution over functions.\n\n\n\n\n\nThis is our first example of a Bayesian General Linear Model (GLM).\nGLMs are the probably the most common models.",
    "crumbs": [
      "The joy of probabilistic modelling",
      "Bernoulli regression"
    ]
  },
  {
    "objectID": "w04_glms/topic04_classification.html#sec-data",
    "href": "w04_glms/topic04_classification.html#sec-data",
    "title": "Bernoulli regression",
    "section": "Example",
    "text": "Example\n\nThe Ariane 1 is an expandable rocket launched 11 times between 1979 and 1986.\nIt failed 2 times and was successful the 9 other launches.\nSo far our models treat the launches as iid given the success probability \\(p\\).\nCan we do better?\n\n\n\n\n\nsource(\"../../solutions/simple.R\")\nsource(\"../blocks/simple_utils.R\")\nsuppressPackageStartupMessages(require(\"dplyr\"))\nset.seed(1)\n\ndf = read.csv(\"../data/launches.csv\") %&gt;% filter(LV.Type == \"Ariane 1\")\nsuccess_indicators = df$Suc_bin\nrmarkdown::paged_table(df)\n\n\n  \n\n\n\n\nplot(success_indicators, xlab = \"Launch index i\")\n\n\n\n\n\n\n\n\n\nFrom this “Exploratory Data Analysis” (EDA), it seems plausible that the success probability is increasing with time.\nMatches with intuition: after a failure, some corrections are made.",
    "crumbs": [
      "The joy of probabilistic modelling",
      "Bernoulli regression"
    ]
  },
  {
    "objectID": "w04_glms/topic04_classification.html#sec-model",
    "href": "w04_glms/topic04_classification.html#sec-model",
    "title": "Bernoulli regression",
    "section": "Building a better model",
    "text": "Building a better model\n\nEach observation is binary, so the likelihood still has to be Bernoulli.\n\nWe will denote its parameter by \\(\\theta \\in [0, 1]\\).\n\nWhat we will change is the prior.\nOld model: \\(\\theta\\) is shared by all launches (“constant over the launch index” \\(i \\in \\{1, 2, \\dots, 11\\}\\))\nNew model: \\(\\theta\\) changes from one launch to the next.\n\ni.e., \\(\\theta\\) is a function of the index \\(i \\in \\{1, 2, \\dots, 11\\}\\), denoted \\(\\theta(i)\\).\n\nQuestion: what kind of function should we start with?\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nGeneral modelling principle: start with something simple!\nIn our context: start with a linear function.\n\n\n\n\nStructure of the model\n\nWe need to build a prior over linear functions.\nRecall: enough to describe how to forward simulate a dataset.\nForward simulation process:\n\nsimulate an intercept,\nsimulate a slope,\nthis determines \\(\\theta(i)\\) for each \\(i \\in \\{1, 2, \\dots, 11\\}\\).\nSimulate \\(y_i \\sim {\\mathrm{Bern}}(\\theta(i))\\) independently but not identically.\n\nI.e. we have reduced the problem to that of sampling two real numbers.\n\nReasonable prior for a first try: the normal distribution.\n\n\n\n\nPrior (first attempt)\nLet us draw one random linear function:\n\nMath: (not yet final) \\[\\begin{align*}\n\\text{slope} &\\sim \\mathcal{N}(0, 1) \\\\\n\\text{intercept} &\\sim \\mathcal{N}(0, 1) \\\\\n\\theta(i) &= \\text{slope} \\cdot i + \\text{intercept}\n\\end{align*}\\]\nForward simulation code: (not yet final)\n\n\nset.seed(1)\nplot(success_indicators, ylab = \"success probability\", xlab = \"Launch index i\")\nxs = 1:length(success_indicators)\nintercept = simulate(Norm(0, 1))\nslope     = simulate(Norm(0, 1))\n\nlines(intercept + slope * xs)\n\n\n\n\n\n\n\n\nWhat is the problem in the above?\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nThe function \\(\\theta(i)\\) can take values smaller than zero or greater than one.\nThis creates problem when we feed the parameter to the Bernoulli, which expects a number between 0 and 1.\n\n\n\n\nFix:\n\nwe cannot use just a linear function…\n… instead we can compose a linear function with the logistic or sigmoid function.\n\nLogistic function: maps real numbers \\(r \\in (-\\infty, \\infty)\\) into \\((0, 1)\\).\nMath: \\[\\text{logistic}(r) = \\frac{1}{1 + e^{-r}}.\\]\nIn R: plogis(r).\n\n\n\nrs = seq(-5, 5, 0.01)\nplot(rs, plogis(rs), type = 'l', xlab = \"r\", ylab = \"logistic(r)\")\n\n\n\n\n\n\n\n\n\n\nPrior (second, final attempt)\n\nMath: \\[\\begin{align*}\n\\text{slope} &\\sim \\mathcal{N}(0, 1) \\\\\n\\text{intercept} &\\sim \\mathcal{N}(0, 1) \\\\\n\\theta(i) &= \\text{logistic}(\\text{slope} \\cdot i + \\text{intercept})\n\\end{align*}\\]\nForward simulation code:\n\n\nset.seed(1)\nplot(success_indicators, ylab = \"success probability\", xlab = \"Launch index i\")\nxs = 1:length(success_indicators)\nintercept = simulate(Norm(0, 1))\nslope     = simulate(Norm(0, 1))\n\nlines(plogis(intercept + slope * xs))",
    "crumbs": [
      "The joy of probabilistic modelling",
      "Bernoulli regression"
    ]
  },
  {
    "objectID": "w04_glms/topic04_classification.html#prior-predictive",
    "href": "w04_glms/topic04_classification.html#prior-predictive",
    "title": "Bernoulli regression",
    "section": "Prior predictive",
    "text": "Prior predictive\nLet us repeat what we did in the last section 50 times to see several draws from the prior at once (using alpha to make the lines translucent):\n\nset.seed(1)\nplot(success_indicators, ylab = \"success probability\", xlab = \"Launch index i\")\nxs = 1:length(success_indicators)\n\nfor (i in 1:50) {\n  intercept = simulate(Norm(0, 1))\n  slope     = simulate(Norm(0, 1))\n  lines(plogis(intercept + slope * xs), col = rgb(red = 0, green = 0, blue = 0, alpha = 0.5))\n}\n\n\n\n\n\n\n\n\n\nHard to get intuition about a prior by just staring at the mathematical formulas.\nSimulating from the prior can help figuring out if the prior is reasonable or not.\n\nThis is known as the prior predictive.\n\nExploration:\n\nso far I used a mean of zero and standard deviation of 1 for the slope and intercept priors.\nLet us try a prior that is more “vague”, with standard deviation of 10 for both the slope and intercept priors:\n\n\n\nset.seed(1)\nplot(success_indicators, ylab = \"success probability\", xlab = \"Launch index i\")\nxs = 1:length(success_indicators)\n\nfor (i in 1:50) {\n  intercept = simulate(Norm(0, 10))\n  slope     = simulate(Norm(0, 10))\n  lines(plogis(intercept + slope * xs), col = rgb(red = 0, green = 0, blue = 0, alpha = 0.5))\n}",
    "crumbs": [
      "The joy of probabilistic modelling",
      "Bernoulli regression"
    ]
  },
  {
    "objectID": "w04_glms/topic04_classification.html#posterior-distribution",
    "href": "w04_glms/topic04_classification.html#posterior-distribution",
    "title": "Bernoulli regression",
    "section": "Posterior distribution",
    "text": "Posterior distribution\n\nIn this week’s exercise, you will implement the model described above in simPPLe.\nHere is a peak of the posterior distribution you should obtain:\n\n\nsource(\"../../solutions/sol04_logistic_regression.R\")\n\nposterior = posterior_particles(logistic_regression, 1000)\nweighted_scatter_plot(posterior, plot_options = list(xlab=\"intercept parameter\", ylab=\"slope parameter\"))\n\n\n\n\n\n\n\n\n\nAgain, this is a bit hard to interpret.\nLet us plot similarly to what we did with the prior predictive:\n\nFor each sample \\(x^{(m)}= (\\text{intercept}^{(m)}, \\text{slope}^{(m)})\\) with weight \\(w^{(m)}\\),\nDraw the curve \\(\\text{logistic}(\\text{slope}^{(m)}\\cdot i + \\text{intercept}^{(m)})\\)…\nwith alpha value (transparency) proportional to the corresponding weight \\(w^{(m)}\\).\n\n\n\nset.seed(1)\nplot(success_indicators, ylab = \"success probability\", xlab = \"Launch index i\")\nxs = 1:length(success_indicators)\n\nsamples = posterior$samples \nnorm_weights = posterior$weights / sum(posterior$weights)\n\nfor (i in 1:nrow(samples)) {\n  intercept = samples[i, 1]\n  slope     = samples[i, 2]\n  pr = norm_weights[i]\n  lines(plogis(intercept + slope * xs), col = rgb(red = 0, green = 0, blue = 0, alpha = pr*20))\n}",
    "crumbs": [
      "The joy of probabilistic modelling",
      "Bernoulli regression"
    ]
  },
  {
    "objectID": "w04_glms/topic04_classification.html#terminology",
    "href": "w04_glms/topic04_classification.html#terminology",
    "title": "Bernoulli regression",
    "section": "Terminology",
    "text": "Terminology\nThe model we just reviewed is an instance of Bayesian logistic regression, a method for classification.\nSome terminology from classification:\n\noutput variables: instances of which we try to “predict”\n\nalso known as “target”, “label”, “predicted variable”, “regressand”, …\nsometimes observed (“training instances”), sometimes unobserved (“test instances”)\nin our example?\n\ninput variables: what we use as the basis of each prediction\n\nalso known as “independent variables”, “covariates”, “predictor”, “regressors”, “feature”,..\ntypically always observed (both at training and test time)\n\nparameters: auxiliary quantities that encode a function mapping inputs to (information on) output.",
    "crumbs": [
      "The joy of probabilistic modelling",
      "Bernoulli regression"
    ]
  },
  {
    "objectID": "w04_glms/topic04_classification.html#references",
    "href": "w04_glms/topic04_classification.html#references",
    "title": "Bernoulli regression",
    "section": "References",
    "text": "References\n\nDataset source.",
    "crumbs": [
      "The joy of probabilistic modelling",
      "Bernoulli regression"
    ]
  },
  {
    "objectID": "w08_mcmc1/topic01_installing_stan.html",
    "href": "w08_mcmc1/topic01_installing_stan.html",
    "title": "Installing and running Stan",
    "section": "",
    "text": "What is Stan?\nLinks to install.\nPeak of next week’s exercise first question (running Stan on your laptop).\n\n\n\n\nSNIS and simPPLe is very flexible and relatively easy to understand, but it can be very slow as we experienced in the exercise on hierarchical models.\nStan is an alternative way to approximate posterior distributions, with complementary properties:\n\n\n\n\nSNIS\nStan\n\n\n\n\nSpeed\nSlow\nFaster1\n\n\nFlexibility\nVery flexible\nLess flexible2\n\n\nEasy to understand?\nSimple\nMore complex3\n\n\n\nFrom a pedagogical point of view, it is useful to first learn about SNIS, however for real-world models, one has to use Stan, or some other advanced inference method, due to the poor scalability of SNIS.",
    "crumbs": [
      "Intro to MCMC",
      "Installing and running Stan"
    ]
  },
  {
    "objectID": "w08_mcmc1/topic01_installing_stan.html#outline",
    "href": "w08_mcmc1/topic01_installing_stan.html#outline",
    "title": "Installing and running Stan",
    "section": "",
    "text": "What is Stan?\nLinks to install.\nPeak of next week’s exercise first question (running Stan on your laptop).\n\n\n\n\nSNIS and simPPLe is very flexible and relatively easy to understand, but it can be very slow as we experienced in the exercise on hierarchical models.\nStan is an alternative way to approximate posterior distributions, with complementary properties:\n\n\n\n\nSNIS\nStan\n\n\n\n\nSpeed\nSlow\nFaster1\n\n\nFlexibility\nVery flexible\nLess flexible2\n\n\nEasy to understand?\nSimple\nMore complex3\n\n\n\nFrom a pedagogical point of view, it is useful to first learn about SNIS, however for real-world models, one has to use Stan, or some other advanced inference method, due to the poor scalability of SNIS.",
    "crumbs": [
      "Intro to MCMC",
      "Installing and running Stan"
    ]
  },
  {
    "objectID": "w08_mcmc1/topic01_installing_stan.html#what-is-stan",
    "href": "w08_mcmc1/topic01_installing_stan.html#what-is-stan",
    "title": "Installing and running Stan",
    "section": "What is Stan?",
    "text": "What is Stan?\n\nStan is the most popular PPL as of 2024.\nReview the notes on “what is a PPL.”\nStan uses Markov chain Monte Carlo (MCMC) to approximate the posterior distribution.\n\nThink of MCMC has a drop-in replacement for SNIS.\nWe will talk about it in more detail soon.",
    "crumbs": [
      "Intro to MCMC",
      "Installing and running Stan"
    ]
  },
  {
    "objectID": "w08_mcmc1/topic01_installing_stan.html#installing-stan",
    "href": "w08_mcmc1/topic01_installing_stan.html#installing-stan",
    "title": "Installing and running Stan",
    "section": "Installing Stan",
    "text": "Installing Stan\nYou will need Stan installed to complete next week’s exercise. Don’t wait until next week, install it today!\nThere are two main steps to install Stan:\n\nConfiguring a C++ Toolchains\nInstalling RStan\n\nLet us know on Piazza if you encounter any issues! This week, our priority will be resolving Stan installation issues. Next week, our priority will be replying questions about the material.",
    "crumbs": [
      "Intro to MCMC",
      "Installing and running Stan"
    ]
  },
  {
    "objectID": "w08_mcmc1/topic01_installing_stan.html#running",
    "href": "w08_mcmc1/topic01_installing_stan.html#running",
    "title": "Installing and running Stan",
    "section": "Running Stan",
    "text": "Running Stan\nWe present two methods for running Stan in the next two section: either from an R script, or from a notebook.\nTemplate: to quickly get started, download the following templates which you can use as a starting point for either R script or notebook.\n\nFrom a R script\nFirst, copy and paste the following code into a file called beta_binomial.stan:\n\n\nbeta_binomial.stan\n\ndata {\n  int&lt;lower=0&gt; n;         // number of trials\n  int&lt;lower=0,upper=n&gt; k; // number of successes\n}\n\nparameters {\n  real&lt;lower=0,upper=1&gt; p;\n}\n\nmodel {\n  // prior\n  p ~ beta(1,1);\n\n  // likelihood\n  k ~ binomial(n, p);\n}\n\nSecond, run Stan as follows:\nrequire(rstan)\n\nfit = stan(\n  seed = 123,\n  file = \"beta_binomial.stan\",  # Stan program\n  data = list(n=3, k=3),        # named list of data\n  iter = 1000                   # number of samples to draw\n)\nThe first question of next week’s exercise will be to report the posterior median, which can be obtained under the column “50%” of the output of the following R command:\nprint(fit)\n\n\nFrom a notebook\nTo see an example of how to integrate Stan code inside quarto (R markdown would work the same), see the source code used for the next page in the notes, available on github.",
    "crumbs": [
      "Intro to MCMC",
      "Installing and running Stan"
    ]
  },
  {
    "objectID": "w08_mcmc1/topic01_installing_stan.html#footnotes",
    "href": "w08_mcmc1/topic01_installing_stan.html#footnotes",
    "title": "Installing and running Stan",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor example, for the model used in the exercise on hierarchical models, Stan can extract 1000 effective samples in less than a second, whereas doing so with SNIS/simPPLe required several minutes!↩︎\nFor example, Stan does not support latent integer-value random variables, whereas simPPLe does.↩︎\nsimPPLe is a few dozen lines of codes, whereas Stan has millions of lines of code.↩︎",
    "crumbs": [
      "Intro to MCMC",
      "Installing and running Stan"
    ]
  },
  {
    "objectID": "w08_mcmc1/topic04_mh.html",
    "href": "w08_mcmc1/topic04_mh.html",
    "title": "Metropolis-Hastings",
    "section": "",
    "text": "Metropolis-Hastings (MH) algorithm with symmetric proposal\nTrace plots from MH\nHistograms from MH\n\n\n\n\nThe vast majority of MCMC algorithms are based on the Metropolis-Hastings (MH) algorithm.\nLearning MH is the first step towards understanding why MCMC algorithms such as Stan work. Some knowledge of how MCMC works is useful to identify when it fails (“MCMC diagnostic”). There are also situations where you will have to write your own MCMC algorithm (something we will practice more in the last weeks of the course).",
    "crumbs": [
      "Intro to MCMC",
      "Metropolis-Hastings"
    ]
  },
  {
    "objectID": "w08_mcmc1/topic04_mh.html#outline",
    "href": "w08_mcmc1/topic04_mh.html#outline",
    "title": "Metropolis-Hastings",
    "section": "",
    "text": "Metropolis-Hastings (MH) algorithm with symmetric proposal\nTrace plots from MH\nHistograms from MH\n\n\n\n\nThe vast majority of MCMC algorithms are based on the Metropolis-Hastings (MH) algorithm.\nLearning MH is the first step towards understanding why MCMC algorithms such as Stan work. Some knowledge of how MCMC works is useful to identify when it fails (“MCMC diagnostic”). There are also situations where you will have to write your own MCMC algorithm (something we will practice more in the last weeks of the course).",
    "crumbs": [
      "Intro to MCMC",
      "Metropolis-Hastings"
    ]
  },
  {
    "objectID": "w08_mcmc1/topic04_mh.html#goal",
    "href": "w08_mcmc1/topic04_mh.html#goal",
    "title": "Metropolis-Hastings",
    "section": "Goal",
    "text": "Goal\n\nJust as SNIS, our goal is to compute \\(\\mathbb{E}[g(X) | Y = y]\\).\nMH will allow us to approximate this conditional expectation.\n\nHow is this different than the simple Monte Carlo method that we used in Ex1.Q.1.3?\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nWe used simple Monte Carlo to compute an expectation without conditioning on \\(Y = y\\), i.e. \\(\\mathbb{E}[g(X)]\\).\nMH, just like SNIS, allows us to take into account observed data.",
    "crumbs": [
      "Intro to MCMC",
      "Metropolis-Hastings"
    ]
  },
  {
    "objectID": "w08_mcmc1/topic04_mh.html#problem-formulation",
    "href": "w08_mcmc1/topic04_mh.html#problem-formulation",
    "title": "Metropolis-Hastings",
    "section": "Problem formulation",
    "text": "Problem formulation\nMH is used to solve the following type of problems:\n\nInput: You are given an unnormalized distribution \\(\\gamma(x)\\)\n\nExample: the numerator in Bayes rule.\n\nOutput: You want to approximate an expectation…\n\n…under the renormalized target distribution \\(\\pi(x) = \\gamma(x) / Z\\).\nMathematically: MH provides an approximation to \\(\\mathbb{E}_\\pi[g(X)] = \\sum \\pi(x) g(x)\\).\n\n\nTerminology review: \\(g\\) is called a test function. Think about it as specifying a query.\nExamples: of test functions\n\nTo compute \\(\\mathbb{E}[X | Y = y]\\), use \\(g(x) = x\\).\nLet’s say you want to approximate \\(\\mathbb{P}(X = 1 | Y = y)\\)…\n\n…what function \\(g(x)\\) would you take?",
    "crumbs": [
      "Intro to MCMC",
      "Metropolis-Hastings"
    ]
  },
  {
    "objectID": "w08_mcmc1/topic04_mh.html#mhs-extra-ingredient-the-proposal",
    "href": "w08_mcmc1/topic04_mh.html#mhs-extra-ingredient-the-proposal",
    "title": "Metropolis-Hastings",
    "section": "MH’s extra ingredient: the proposal",
    "text": "MH’s extra ingredient: the proposal\nMH requires a proposal \\(q(x' | x)\\):\n\ngiven a current point \\(x\\),\n\\(q(\\cdot|x)\\) is a density or PMF;\nit is used to decide where to go next.\n\nExample: standard normal centered at the current point, \\[q(x' | x) \\propto \\exp\\left(- \\frac{1}{2} (x - x')^2 \\right).\\]\nKey difference compared to SNIS:\n\nthe proposal can depend on the current value \\(x\\).\nWhy this is useful:\n\nOnce we find a “good neighborhood”, i.e., a region where the posterior density is high,\nhaving \\(q(\\cdot|x)\\) depend on current value allows us to stay for a while in that good neighborhood.\n\n\nToday:\n\nwe assume \\(q\\) is symmetric, i.e., \\(q(x | x') = q(x' | x)\\).\nWe will relax this later.",
    "crumbs": [
      "Intro to MCMC",
      "Metropolis-Hastings"
    ]
  },
  {
    "objectID": "w08_mcmc1/topic04_mh.html#symmetric-proposal-mh-algorithm",
    "href": "w08_mcmc1/topic04_mh.html#symmetric-proposal-mh-algorithm",
    "title": "Metropolis-Hastings",
    "section": "Symmetric proposal MH algorithm",
    "text": "Symmetric proposal MH algorithm\nInputs: unnormalized target \\(\\gamma\\), proposal \\(q\\), number of iterations \\(M\\).\nAlgorithm:\n\nInitialize \\(X^{(0)}\\) arbitrarily\nFor \\(m = 1, 2, \\dots, M\\) do:\n\nDenote the proposal at iteration \\(m \\in \\{1, 2, \\dots, M\\}\\) by: \\[\\tilde X^{(m)}\\sim q(\\cdot | X^{(m-1)}).\\]\nCompute the MH ratio: \\[R^{(m)}= \\frac{\\gamma(\\tilde X^{(m)})}{\\gamma(X^{(m-1)})}.\\]\nSample an acceptance Bernoulli: \\[A^{(m)}\\sim {\\mathrm{Bern}}(\\min(1, R^{(m)})).\\]\n\nIf \\(A^{(m)}= 1\\), we accept the proposed sample: \\[X^{(m)}= \\tilde X^{(m)},\\]\nElse, \\(A^{(m)}= 0\\), and we reject the proposed sample and stay at previous position: \\[X^{(m)}= X^{(m-1)}.\\]\n\n\n\nOutput: equally weighted samples \\(X^{(1)}, X^{(2)}, \\dots, X^{(M)}\\).\nDemo: Have a look at this interactive animation showing MH on a two-dimensional target (green arrow shows accepted proposal, red arrow, rejected).",
    "crumbs": [
      "Intro to MCMC",
      "Metropolis-Hastings"
    ]
  },
  {
    "objectID": "w08_mcmc1/topic04_mh.html#intuition",
    "href": "w08_mcmc1/topic04_mh.html#intuition",
    "title": "Metropolis-Hastings",
    "section": "Intuition",
    "text": "Intuition\n\nWhen proposing to a higher point under the posterior density, we have:\n\n\\(\\gamma(\\tilde X^{(m)}) &gt; \\gamma(X^{(m-1)})\\)\nhence \\(R^{(m)}&gt; 1\\),\nso \\(\\min(1, R^{(m)}) = 1\\),\nand \\(A^{(m)}\\) is always one, i.e. always accepted!\n\nWhen proposing to a lower point under the posterior density, we have:\n\n\\(\\gamma(\\tilde X^{(m)}) &lt; \\gamma(X^{(m-1)})\\)\nhence \\(R^{(m)}&lt; 1\\),\nso \\(\\min(1, R^{(m)}) = R^{(m)}\\),\n\\(A^{(m)}\\sim {\\mathrm{Bern}}(\\gamma(\\tilde X^{(m)})/\\gamma(X^{(m-1)}))\\),\ni.e., acceptance probability is a ratio of posterior densities at two points (proposed vs previous).",
    "crumbs": [
      "Intro to MCMC",
      "Metropolis-Hastings"
    ]
  },
  {
    "objectID": "w08_mcmc1/topic04_mh.html#visualization",
    "href": "w08_mcmc1/topic04_mh.html#visualization",
    "title": "Metropolis-Hastings",
    "section": "Visualization",
    "text": "Visualization\nIn the exercise this week, you will implement MH with a standard normal proposal.\nOnce your exercise is complete you will be able to call your function as follows:\n\nsource(\"../exercises/ex07_scaffold.R\")\nsource(\"../../solutions/sol07_mh.R\")\n\nsamples = simple_mh(gamma_beta_binomial, 0.5, 1000)\n\n\nTrace plot\nLet us visualize the first 100 samples:\n\nplot(samples[1:100], \n     xlab = \"MCMC iteration\", \n     ylab = \"p\",\n     type = \"o\", \n     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.2))\n\n\n\n\n\n\n\n\nQuestion: Notice that many points take exactly the same value, why is that?\nTerminology: the above plot (sample as a function of MCMC iteration) is called a trace plot.\nFull trace plot:\n\nplot(samples, \n     xlab = \"MCMC iteration\", \n     ylab = \"p\",\n     type = \"o\", \n     col = rgb(red = 0, green = 0, blue = 0, alpha = 0.2))\n\n\n\n\n\n\n\n\n\n\nHistogram\nAs with Stan (another MCMC algorithm we saw earlier this week), we can use a histogram to summarize the posterior:\n\nhist(samples)\n\n\n\n\n\n\n\n\nTo connect the two pictures, it is useful to visualize the histogram rotated as follows:\n\nsource(\"../blocks/plot_traces_and_hist.R\")\n\nplot_traces_and_hist(samples)",
    "crumbs": [
      "Intro to MCMC",
      "Metropolis-Hastings"
    ]
  },
  {
    "objectID": "w08_mcmc1/topic04_mh.html#further-readings",
    "href": "w08_mcmc1/topic04_mh.html#further-readings",
    "title": "Metropolis-Hastings",
    "section": "Further readings",
    "text": "Further readings\n\nGeyer’s course notes on MCMC, available from the author’s website\nHandbook on MCMC [Available online via UBC Library]\nMore advanced text on the theory [Available online via UBC Library]",
    "crumbs": [
      "Intro to MCMC",
      "Metropolis-Hastings"
    ]
  },
  {
    "objectID": "w08_mcmc1/topic05_mcmc_consistency.html",
    "href": "w08_mcmc1/topic05_mcmc_consistency.html",
    "title": "Consistency of MCMC",
    "section": "",
    "text": "MH’s consistency guarantee.\nNotion of irreducibility.\n\n\n\n\nConsistency of MH is a key property that explains MCMC’s popularity. We will prove the key steps of this theorem later on, today we only state it.",
    "crumbs": [
      "Intro to MCMC",
      "Consistency of MCMC"
    ]
  },
  {
    "objectID": "w08_mcmc1/topic05_mcmc_consistency.html#outline",
    "href": "w08_mcmc1/topic05_mcmc_consistency.html#outline",
    "title": "Consistency of MCMC",
    "section": "",
    "text": "MH’s consistency guarantee.\nNotion of irreducibility.\n\n\n\n\nConsistency of MH is a key property that explains MCMC’s popularity. We will prove the key steps of this theorem later on, today we only state it.",
    "crumbs": [
      "Intro to MCMC",
      "Consistency of MCMC"
    ]
  },
  {
    "objectID": "w08_mcmc1/topic05_mcmc_consistency.html#key-theoretical-guarantee-of-mh",
    "href": "w08_mcmc1/topic05_mcmc_consistency.html#key-theoretical-guarantee-of-mh",
    "title": "Consistency of MCMC",
    "section": "Key theoretical guarantee of MH",
    "text": "Key theoretical guarantee of MH\n\nWe have the same type of result as we encountered in Simple Monte Carlo and SNIS.\nNamely: for any approximation error tolerance, we can find a number of iterations \\(M\\) large enough such that we will be within that error tolerance with high probability after \\(M\\) iterations.\nRecall that the name for the above property is consistency.\nHowever to get consistency with MH we will need one extra assumption…",
    "crumbs": [
      "Intro to MCMC",
      "Consistency of MCMC"
    ]
  },
  {
    "objectID": "w08_mcmc1/topic05_mcmc_consistency.html#additional-assumption",
    "href": "w08_mcmc1/topic05_mcmc_consistency.html#additional-assumption",
    "title": "Consistency of MCMC",
    "section": "Additional assumption",
    "text": "Additional assumption\n\nCompared to SNIS, we need an additional assumption to get consistency.\nInformally: the proposal should permit the algorithm to explore the whole state space.\nTechnical name: irreducibility.\n\nDefinition: (discrete case1) Let \\(\\pi\\) denote the posterior PMF, and \\(X^{(1)}, X^{(2)}, \\dots\\), the random variables produced at each iteration of MH. We say the chain \\(X^{(m)}\\) is irreducible if for any states \\(x, x'\\) with \\(\\pi(x) &gt; 0\\) and \\(\\pi(x') &gt; 0\\), we can get from \\(x\\) to \\(x'\\) with positive probability, i.e., there exists a number of steps \\(m\\) such that \\(\\mathbb{P}(X^{(m)} = x' | X^{(0)} = x) &gt; 0\\).",
    "crumbs": [
      "Intro to MCMC",
      "Consistency of MCMC"
    ]
  },
  {
    "objectID": "w08_mcmc1/topic05_mcmc_consistency.html#consistency-of-mh",
    "href": "w08_mcmc1/topic05_mcmc_consistency.html#consistency-of-mh",
    "title": "Consistency of MCMC",
    "section": "Consistency of MH",
    "text": "Consistency of MH\nProposition: if \\(\\mathbb{E}_\\pi|g(X)| &lt; \\infty\\), and the chain \\(X^{(m)}\\) produced by MH is irreducible, then2 \\[\\frac{1}{M} \\sum_{m=1}^M g(X^{(m)}) \\to \\mathbb{E}_\\pi[g(X)], \\tag{1}\\] as the number of MCMC iterations \\(M\\) goes to infinity.\nNote: the right hand side of Equation 1 does not involve the initial distribution, i.e., the distribution we use to initialize our MCMC algorithm is eventually “forgotten” when the chain is irreducible.",
    "crumbs": [
      "Intro to MCMC",
      "Consistency of MCMC"
    ]
  },
  {
    "objectID": "w08_mcmc1/topic05_mcmc_consistency.html#footnotes",
    "href": "w08_mcmc1/topic05_mcmc_consistency.html#footnotes",
    "title": "Consistency of MCMC",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor the generalization to continuous state spaces, see e.g., Geyer’s notes, page 5. In summary, the difficulty in continuous state space is that the probability of reaching an individual “destination point” \\(x'\\) is zero. The solution is the replace the destinations \\(x'\\) by positive probability events.↩︎\nMore precisely, let \\(E = \\{x : (X^{(m)}) \\text{ initialized at }x, \\frac{1}{M} \\sum_{m=1}^M g(X^{(m)}) \\to \\mathbb{E}_\\pi[g(X)]\\}\\). Then \\(\\pi(E) = 1\\). This is known as Birkhoff’s ergodic theorem (combined here with the additional structure that the sequence is Markovian and irreducible to obtain a deterministic limit). For more information and a proof, see for example Douc et al., 2018, Theorem 5.2.1.↩︎",
    "crumbs": [
      "Intro to MCMC",
      "Consistency of MCMC"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic08_chain.html",
    "href": "w01_discrete_inference/topic08_chain.html",
    "title": "Chain rule",
    "section": "",
    "text": "Mathematical statement\nVisual intuition on a decision tree\nSpecial names for the pieces of chain rule (joint and conditional PMFs)\nConditional independence\n\n\n\n\nThe chain rule allows us to compute a probability that the forward sampling function takes a given path.\nThe chain rule seem innocent but is used heavily in Bayesian statistics. It is also the building block for Bayes rule.",
    "crumbs": [
      "Probability essentials",
      "Chain rule"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic08_chain.html#outline",
    "href": "w01_discrete_inference/topic08_chain.html#outline",
    "title": "Chain rule",
    "section": "",
    "text": "Mathematical statement\nVisual intuition on a decision tree\nSpecial names for the pieces of chain rule (joint and conditional PMFs)\nConditional independence\n\n\n\n\nThe chain rule allows us to compute a probability that the forward sampling function takes a given path.\nThe chain rule seem innocent but is used heavily in Bayesian statistics. It is also the building block for Bayes rule.",
    "crumbs": [
      "Probability essentials",
      "Chain rule"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic08_chain.html#proposition",
    "href": "w01_discrete_inference/topic08_chain.html#proposition",
    "title": "Chain rule",
    "section": "Proposition",
    "text": "Proposition\nIf \\(E_1\\) and \\(E_2\\) are any events, \\[\\mathbb{P}(E_1, E_2) = \\mathbb{P}(E_1) \\mathbb{P}(E_2 | E_1).\\]\nThis is true in any order, i.e. we also have \\(\\mathbb{P}(E_1, E_2) = \\mathbb{P}(E_2) \\mathbb{P}(E_1 | E_2)\\).",
    "crumbs": [
      "Probability essentials",
      "Chain rule"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic08_chain.html#sec-chain-rule",
    "href": "w01_discrete_inference/topic08_chain.html#sec-chain-rule",
    "title": "Chain rule",
    "section": "Generalization",
    "text": "Generalization\nFor any events \\(E_1, E_2, E_3 \\dots\\),\n\\[\\mathbb{P}(E_1, E_2, E_3 \\dots) = \\mathbb{P}(E_1) \\mathbb{P}(E_2 | E_1) \\mathbb{P}(E_3 | E_1, E_2) \\dots.\\]",
    "crumbs": [
      "Probability essentials",
      "Chain rule"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic08_chain.html#visual-intuition",
    "href": "w01_discrete_inference/topic08_chain.html#visual-intuition",
    "title": "Chain rule",
    "section": "Visual intuition",
    "text": "Visual intuition\nChain rule: the probability of a node is the product of the edge labels on the path to the root.\n\n\n\n\n\n\nflowchart TD\nS__and__X_0 -- 1.0 --&gt; S__and__X_0__and__Y1_false[\"Y1=false\"]\nS__and__X_2__and__Y1_true -- 1.0 --&gt; S__and__X_2__and__Y1_true__and__Y2_true[\"Y2=true\"]\nS -- 0.33 --&gt; S__and__X_0[\"X=0\"]\nS__and__X_1__and__Y1_true__and__Y2_true__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_true__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_true__and__Y2_false__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_true__and__Y4_false[\"Y4=false\"]\nS__and__X_0__and__Y1_false__and__Y2_false__and__Y3_false -- 1.0 --&gt; S__and__X_0__and__Y1_false__and__Y2_false__and__Y3_false__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_false__and__Y2_false__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_false__and__Y4_true[\"Y4=true\"]\nS -- 0.33 --&gt; S__and__X_1[\"X=1\"]\nS__and__X_2__and__Y1_true__and__Y2_true__and__Y3_true -- 1.0 --&gt; S__and__X_2__and__Y1_true__and__Y2_true__and__Y3_true__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_true__and__Y2_false__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_false__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_false__and__Y2_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_false[\"Y3=false\"]\nS__and__X_1__and__Y1_true__and__Y2_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_false[\"Y3=false\"]\nS__and__X_1__and__Y1_false__and__Y2_false__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_false__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_true__and__Y2_true__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_false__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_true__and__Y2_false__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_true__and__Y4_true[\"Y4=true\"]\nS__and__X_1 -- 0.5 --&gt; S__and__X_1__and__Y1_false[\"Y1=false\"]\nS__and__X_1__and__Y1_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false[\"Y2=false\"]\nS__and__X_1__and__Y1_false__and__Y2_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_true[\"Y3=true\"]\nS__and__X_1__and__Y1_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true[\"Y2=true\"]\nS__and__X_0__and__Y1_false -- 1.0 --&gt; S__and__X_0__and__Y1_false__and__Y2_false[\"Y2=false\"]\nS__and__X_1__and__Y1_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false[\"Y2=false\"]\nS__and__X_1__and__Y1_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true[\"Y2=true\"]\nS__and__X_2 -- 1.0 --&gt; S__and__X_2__and__Y1_true[\"Y1=true\"]\nS__and__X_1__and__Y1_false__and__Y2_true__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_true__and__Y4_false[\"Y4=false\"]\nS__and__X_1 -- 0.5 --&gt; S__and__X_1__and__Y1_true[\"Y1=true\"]\nS -- 0.33 --&gt; S__and__X_2[\"X=2\"]\nS__and__X_1__and__Y1_true__and__Y2_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_false[\"Y3=false\"]\nS__and__X_2__and__Y1_true__and__Y2_true -- 1.0 --&gt; S__and__X_2__and__Y1_true__and__Y2_true__and__Y3_true[\"Y3=true\"]\nS__and__X_1__and__Y1_false__and__Y2_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_true[\"Y3=true\"]\nS__and__X_1__and__Y1_false__and__Y2_true__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_false__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_true__and__Y2_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_true[\"Y3=true\"]\nS__and__X_1__and__Y1_true__and__Y2_true__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_true__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_false__and__Y2_true__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_false__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_true__and__Y2_true__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_false__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_false__and__Y2_false__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_true__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_true__and__Y2_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_true[\"Y3=true\"]\nS__and__X_1__and__Y1_true__and__Y2_false__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_false__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_false__and__Y2_false__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_true__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_false__and__Y2_true__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_true__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_false__and__Y2_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_false[\"Y3=false\"]\nS__and__X_0__and__Y1_false__and__Y2_false -- 1.0 --&gt; S__and__X_0__and__Y1_false__and__Y2_false__and__Y3_false[\"Y3=false\"]\nstyle S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_true fill:#f9f,stroke:#333,stroke-width:4px",
    "crumbs": [
      "Probability essentials",
      "Chain rule"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic08_chain.html#poll-what-is-the-probability-of-the-node-in-red",
    "href": "w01_discrete_inference/topic08_chain.html#poll-what-is-the-probability-of-the-node-in-red",
    "title": "Chain rule",
    "section": "Poll: what is the probability of the node in red?",
    "text": "Poll: what is the probability of the node in red?\n\n1/2\n1/4\n1/8\n1/24\nNone of the above\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nMultiplying the four edge leading to the node we get: \\(1/2 \\cdot 1/2 \\cdot 1/2 \\cdot 1/3 = 1/24\\).",
    "crumbs": [
      "Probability essentials",
      "Chain rule"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic08_chain.html#poll-what-is-the-event-corresponding-to-that-node",
    "href": "w01_discrete_inference/topic08_chain.html#poll-what-is-the-event-corresponding-to-that-node",
    "title": "Chain rule",
    "section": "Poll: what is the event corresponding to that node?",
    "text": "Poll: what is the event corresponding to that node?\n\n\\((Y_3 = 1)\\)\n\\((Y_3 = 1, Y_2 = 1)\\)\n\\((Y_3 = 1, Y_2 = 1)\\)\n\\((Y_3 = 1, Y_2 = 1, Y_1 = 1)\\)\n\\((Y_3 = 1, Y_2 = 1, Y_1 = 1, X = 1)\\)\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nRecall that the event is the intersection of all node labels to the root, hence the event is \\((Y_3 = 1, Y_2 = 1, Y_1 = 1, X = 1)\\).\nThe calculation we did visually in the previous clicker question is mathematically: \\[\\mathbb{P}(Y_3 = 1, Y_2 = 1, Y_1 = 1, X = 1) = \\mathbb{P}(X = 1) \\mathbb{P}(Y_1 = 1 | X = 1) \\mathbb{P}(Y_2 = 1 | X = 1, Y_1 = 1) \\mathbb{P}(Y_3 = 1 | X = 1, Y_1 = 1, Y_2 = 1).\\]",
    "crumbs": [
      "Probability essentials",
      "Chain rule"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic08_chain.html#joint-pmf",
    "href": "w01_discrete_inference/topic08_chain.html#joint-pmf",
    "title": "Chain rule",
    "section": "Joint PMF",
    "text": "Joint PMF\nWe will often encounter expression of the form of a conjunction (intersection/and) of several variables. A handy notation for that is the joint PMF\nFor example, here is the joint PMF of \\((X, Y_1, Y_2, Y_3)\\):\n\\[p(x, y_1, y_2, y_3) = \\mathbb{P}(X = x, Y_1 = y_1, Y_2 = y_2, Y_3 = y_3).\\]\nSometimes we put the random variables in question as subscript, for example \\(p_{X, Y_1}(x, y)\\) for the joint PMF of \\(X\\) and \\(Y_1\\).",
    "crumbs": [
      "Probability essentials",
      "Chain rule"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic08_chain.html#conditional-pmf",
    "href": "w01_discrete_inference/topic08_chain.html#conditional-pmf",
    "title": "Chain rule",
    "section": "Conditional PMF",
    "text": "Conditional PMF\nSimilarly, here is an example of a conditional PMF: \\[p_{Y_1|X}(y | x) = \\mathbb{P}(Y_1 = y | X = x).\\]",
    "crumbs": [
      "Probability essentials",
      "Chain rule"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic08_chain.html#sec-conditional-independence",
    "href": "w01_discrete_inference/topic08_chain.html#sec-conditional-independence",
    "title": "Chain rule",
    "section": "Conditional independence",
    "text": "Conditional independence\nThe model was specified as:\n\\[\n\\begin{align*}\nX &\\sim {\\mathrm{Unif}}\\{0, 1, 2\\} \\\\\nY_i | X &\\sim {\\mathrm{Bern}}(X/2)\n\\end{align*}\n\\] i.e. with \\(\\mathbb{P}(X = x)\\) and \\(\\mathbb{P}(Y_i = y | X = x)\\) for all \\(x\\) and \\(y\\).\nQuestion: how did we go from \\(\\mathbb{P}(Y_2 = 1 | X = 1, Y_1 = 1)\\) (in our chain rule computation) to \\(\\mathbb{P}(Y_2 = 1 | X = 1)\\) (model specification)?\nDefinition: \\(V\\) and \\(W\\) are conditionally independence given \\(Z\\) if \\[\\mathbb{P}(V = v, W = w | Z = z) = \\mathbb{P}(V = v | Z = z) \\mathbb{P}(W = w | Z = z).\\]\nExercise: show the above definition is equivalent to:\n\\[\\mathbb{P}(V = v | W = w, Z = z) = \\mathbb{P}(V = v | Z = z).\\]",
    "crumbs": [
      "Probability essentials",
      "Chain rule"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic07_conditional.html",
    "href": "w01_discrete_inference/topic07_conditional.html",
    "title": "Conditioning",
    "section": "",
    "text": "Intuition on conditioning\nA conditional probability is a probability.\n\n\n\n\n\nConditioning is the workhorse of Bayesian inference!\n\nUsed to define models (as when we assigned probabilities to edges of a decision tree)\nAnd soon, to gain information on latent variables given observations.",
    "crumbs": [
      "Probability essentials",
      "Conditioning"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic07_conditional.html#outline",
    "href": "w01_discrete_inference/topic07_conditional.html#outline",
    "title": "Conditioning",
    "section": "",
    "text": "Intuition on conditioning\nA conditional probability is a probability.\n\n\n\n\n\nConditioning is the workhorse of Bayesian inference!\n\nUsed to define models (as when we assigned probabilities to edges of a decision tree)\nAnd soon, to gain information on latent variables given observations.",
    "crumbs": [
      "Probability essentials",
      "Conditioning"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic07_conditional.html#conditioning-as-belief-update",
    "href": "w01_discrete_inference/topic07_conditional.html#conditioning-as-belief-update",
    "title": "Conditioning",
    "section": "Conditioning as belief update",
    "text": "Conditioning as belief update\nKey concept: Bayesian methods use probabilities to encode beliefs.\n\nWe will explore this perspective in much more details next week.",
    "crumbs": [
      "Probability essentials",
      "Conditioning"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic07_conditional.html#a-conditional-probability-is-a-probability",
    "href": "w01_discrete_inference/topic07_conditional.html#a-conditional-probability-is-a-probability",
    "title": "Conditioning",
    "section": "A conditional probability is a probability",
    "text": "A conditional probability is a probability\nThe “updated belief” interpretation highlights the fact that we want the result of the conditioning procedure, \\(\\mathbb{P}(\\cdot | E)\\) to be a probability when viewed as a function of the first argument for any fixed \\(E\\).",
    "crumbs": [
      "Probability essentials",
      "Conditioning"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic07_conditional.html#intuition-behind-conditioning",
    "href": "w01_discrete_inference/topic07_conditional.html#intuition-behind-conditioning",
    "title": "Conditioning",
    "section": "Intuition behind conditioning",
    "text": "Intuition behind conditioning\n\n\n\n\n\n\n\nFor a query even \\(A\\), what should be the updated probability?\nWe want to remove from \\(A\\) all the outcomes that are not compatible with the new information \\(E\\). How?\n\nTake the intersection: \\(A \\cap E\\)\nWe also want: \\(\\mathbb{P}(S | E) = 1\\) (last section)\nHow? Renormalize:1 \\[\\mathbb{P}(A | E) = \\frac{\\mathbb{P}(A, E)}{\\mathbb{P}(E)}\\]",
    "crumbs": [
      "Probability essentials",
      "Conditioning"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic07_conditional.html#footnotes",
    "href": "w01_discrete_inference/topic07_conditional.html#footnotes",
    "title": "Conditioning",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn the equation, we use comma to denote intersection, i.e., \\((A \\cap E) = (A, E)\\).↩︎",
    "crumbs": [
      "Probability essentials",
      "Conditioning"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic04_pmfs.html",
    "href": "w01_discrete_inference/topic04_pmfs.html",
    "title": "Probability mass functions",
    "section": "",
    "text": "Probability Mass Function (PMF):\n\ndenoted by \\(p\\) (not to be confused by \\(\\mathbb{P}\\)),\ndefined by: \\[p(x) = \\mathbb{P}(X = x).\\]\nTerminology: \\(\\{x : p(x) &gt; 0\\}\\) is called the support.\nIf there are several random variables, we use a subscript to disambiguate the PMFS, e.g.,\n\n\\(p_X\\) for the PMF of the latent random variable,\n\\(p_Y\\) for the PMF of the observed random variable.",
    "crumbs": [
      "Probability essentials",
      "Probability mass functions"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic04_pmfs.html#definition",
    "href": "w01_discrete_inference/topic04_pmfs.html#definition",
    "title": "Probability mass functions",
    "section": "",
    "text": "Probability Mass Function (PMF):\n\ndenoted by \\(p\\) (not to be confused by \\(\\mathbb{P}\\)),\ndefined by: \\[p(x) = \\mathbb{P}(X = x).\\]\nTerminology: \\(\\{x : p(x) &gt; 0\\}\\) is called the support.\nIf there are several random variables, we use a subscript to disambiguate the PMFS, e.g.,\n\n\\(p_X\\) for the PMF of the latent random variable,\n\\(p_Y\\) for the PMF of the observed random variable.",
    "crumbs": [
      "Probability essentials",
      "Probability mass functions"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic04_pmfs.html#sec-simulate-pmf",
    "href": "w01_discrete_inference/topic04_pmfs.html#sec-simulate-pmf",
    "title": "Probability mass functions",
    "section": "Simulation/sampling from a PMF",
    "text": "Simulation/sampling from a PMF\n\nIn R\n\nrequire(extraDistr)\n\nLoading required package: extraDistr\n\n# 10 coin flips:\nrbern(10, prob=0.5)\n\n [1] 1 1 0 1 1 0 0 0 1 1\n\n# two dice rolls\nrdunif(2, min=1, max=6)\n\n[1] 3 6\n\n\n\n\nHow does it work?\n\nThink of the green lines in the uniform PMF as “sticks” with “labels.”\n\nThe “labels” are the different realization, e.g. 1, 2, 3, …, 6.\n\nPlace the six sticks in the interval \\([0, 1]\\) so that they do not overlap.\nSample a uniform real number in \\([0, 1]\\)\n\nin R: runif(1)\n\nThe uniform falls in exactly one of the sticks.\nReturn the label of that stick.\n\n\n\nMathematical explanation\nThe above “stick-based” algorithm can be implemented using the cumulative distribution function and a generalization of its inverse known as the quantile function.\nCumulative distribution function (CDF): \\(F(x) = \\mathbb{P}(X \\le x).\\)\nQuantile function: \\(Q(u) = \\inf\\{x \\in \\mathbb{R}: u \\le F(x)\\}\\).\nThen the “stick-based” algorithm can be written as:\n\n\\(U \\sim {\\mathrm{Unif}}[0, 1]\\)\nReturn \\(Q(U)\\).",
    "crumbs": [
      "Probability essentials",
      "Probability mass functions"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic04_pmfs.html#plotting-pmfs-in-r",
    "href": "w01_discrete_inference/topic04_pmfs.html#plotting-pmfs-in-r",
    "title": "Probability mass functions",
    "section": "Plotting PMFs in R",
    "text": "Plotting PMFs in R\nHere is an example of how to plot PMFs in R:\n\nrealizations &lt;- 0:20\n\nplot(realizations, dbinom(realizations, size=20, prob=.3), type='h')\n\n\n\n\n\n\n\n\nSome explanations:\n\nHere dbinom is the R function for the PMF of a binomial (R uses the prefix d___ for densities and PMFs).\nWe use the fact that many functions in R such as this one are vectorized, i.e. the normal PMF function takes a single point \\(x\\) and output \\(p(x)\\) where \\(p\\) is the PMF; the vectorized version takes vector \\((x_1, x_2, \\dots)\\) and returns \\((p(x_1), p(x_2), \\dots)\\).\nThe argument type = 'h' instructs the plot function to make the plot “histogram”-like.",
    "crumbs": [
      "Probability essentials",
      "Probability mass functions"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic02_axioms.html",
    "href": "w01_discrete_inference/topic02_axioms.html",
    "title": "Axioms of probability",
    "section": "",
    "text": "Definition: \\(E_1, E_2, \\dots\\) is a partition of \\(E\\) if:\n\nthe \\(E_i\\)’s are disjoint, i.e., \\[E_i \\cap E_j = \\emptyset \\text{ when } i\\neq j,\\]\nand their union is \\(E\\), i.e., \\(\\cup_i E_i = E\\).",
    "crumbs": [
      "Probability essentials",
      "Axioms of probability"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic02_axioms.html#partitions",
    "href": "w01_discrete_inference/topic02_axioms.html#partitions",
    "title": "Axioms of probability",
    "section": "",
    "text": "Definition: \\(E_1, E_2, \\dots\\) is a partition of \\(E\\) if:\n\nthe \\(E_i\\)’s are disjoint, i.e., \\[E_i \\cap E_j = \\emptyset \\text{ when } i\\neq j,\\]\nand their union is \\(E\\), i.e., \\(\\cup_i E_i = E\\).",
    "crumbs": [
      "Probability essentials",
      "Axioms of probability"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic02_axioms.html#sec-axioms",
    "href": "w01_discrete_inference/topic02_axioms.html#sec-axioms",
    "title": "Axioms of probability",
    "section": "Axioms of probability",
    "text": "Axioms of probability\n\nA probability is a function \\(\\mathbb{P}\\) that satisfy the following constraints:\n\n\\(\\mathbb{P}\\) should take events as input and return a number between zero and one: \\[\\mathbb{P}(E) \\in [0, 1].\\]\nAdditivity axiom: if \\(E_1, E_2, \\dots\\) is a partition of \\(E\\), then \\[\\mathbb{P}(E) = \\sum_i \\mathbb{P}(E_i).\\]\n\\(\\mathbb{P}(S) = 1\\)\n\nThanks to the constraints, even if I only specify a few known probabilities I can recover many other ones mathematically/computationally.",
    "crumbs": [
      "Probability essentials",
      "Axioms of probability"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic05_decision_diagrams.html",
    "href": "w01_discrete_inference/topic05_decision_diagrams.html",
    "title": "Decision trees",
    "section": "",
    "text": "Decision trees\nReview of more probability theory concepts, contextualized in decision trees: outcome, event, sample space, partitions, conditional probability\n\n\n\n\nWe will use decision trees to provide visualization for a bunch of complex concepts such as forward simulation, posterior inference, importance sampling, probabilistic programming, etc.",
    "crumbs": [
      "Probability essentials",
      "Decision trees"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic05_decision_diagrams.html#outline",
    "href": "w01_discrete_inference/topic05_decision_diagrams.html#outline",
    "title": "Decision trees",
    "section": "",
    "text": "Decision trees\nReview of more probability theory concepts, contextualized in decision trees: outcome, event, sample space, partitions, conditional probability\n\n\n\n\nWe will use decision trees to provide visualization for a bunch of complex concepts such as forward simulation, posterior inference, importance sampling, probabilistic programming, etc.",
    "crumbs": [
      "Probability essentials",
      "Decision trees"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic05_decision_diagrams.html#running-example",
    "href": "w01_discrete_inference/topic05_decision_diagrams.html#running-example",
    "title": "Decision trees",
    "section": "Running example",
    "text": "Running example\n\nImagine a bag with 3 coins each with a different probability parameter \\(p\\)\nCoin \\(i\\in \\{0, 1, 2\\}\\) has bias \\(i/2\\)—in other words:\n\nFirst coin: bias is \\(0/2 = 0\\) (i.e. both sides are “heads”, \\(p = 0\\))\nSecond coin: bias is \\(1/2 = 0.5\\) (i.e. standard coin, \\(p = 1/2\\))\nThird coin: bias is \\(2/2 = 1\\) (i.e. both sides are “tails”, \\(p = 1\\))\n\n\n\n\n\n\nConsider the following two steps sampling process\n\nStep 1: pick one of the three coins, but do not look at it!\nStep 2: flip the coin 4 times\n\nMathematically, this probability model can be written as follows: \\[\n\\begin{align*}\nX &\\sim {\\mathrm{Unif}}\\{0, 1, 2\\} \\\\\nY_i | X &\\sim {\\mathrm{Bern}}(X/2)\n\\end{align*}\n\\tag{1}\\]",
    "crumbs": [
      "Probability essentials",
      "Decision trees"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic05_decision_diagrams.html#decision-tree",
    "href": "w01_discrete_inference/topic05_decision_diagrams.html#decision-tree",
    "title": "Decision trees",
    "section": "Decision tree",
    "text": "Decision tree\n\nDecision tree: a recursive classification of all possible scenarios\nNodes in the tree are “groups of scenarios” which we call events\nChildren of a node partitions an event into an exhaustive set of sub-cases,\n\ni.e. \\(E_1, E_2, \\dots\\) is a partition of \\(E\\).\n\nIn the decision tree below, we partitioned events until we get events at the leaves each containing a single scenario\n\nWe call one individual scenario an outcome\nWe call the set of all outcomes the sample space, \\(S\\), and put it at the root of decision trees.\n\n\n\n\n\n\n\n\nflowchart TD\nS__and__X_0 -- 1.0 --&gt; S__and__X_0__and__Y1_false[\"Y1=false\"]\nS__and__X_2__and__Y1_true -- 1.0 --&gt; S__and__X_2__and__Y1_true__and__Y2_true[\"Y2=true\"]\nS -- 0.33 --&gt; S__and__X_0[\"X=0\"]\nS__and__X_1__and__Y1_true__and__Y2_true__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_true__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_true__and__Y2_false__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_true__and__Y4_false[\"Y4=false\"]\nS__and__X_0__and__Y1_false__and__Y2_false__and__Y3_false -- 1.0 --&gt; S__and__X_0__and__Y1_false__and__Y2_false__and__Y3_false__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_false__and__Y2_false__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_false__and__Y4_true[\"Y4=true\"]\nS -- 0.33 --&gt; S__and__X_1[\"X=1\"]\nS__and__X_2__and__Y1_true__and__Y2_true__and__Y3_true -- 1.0 --&gt; S__and__X_2__and__Y1_true__and__Y2_true__and__Y3_true__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_true__and__Y2_false__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_false__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_false__and__Y2_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_false[\"Y3=false\"]\nS__and__X_1__and__Y1_true__and__Y2_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_false[\"Y3=false\"]\nS__and__X_1__and__Y1_false__and__Y2_false__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_false__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_true__and__Y2_true__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_false__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_true__and__Y2_false__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_true__and__Y4_true[\"Y4=true\"]\nS__and__X_1 -- 0.5 --&gt; S__and__X_1__and__Y1_false[\"Y1=false\"]\nS__and__X_1__and__Y1_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false[\"Y2=false\"]\nS__and__X_1__and__Y1_false__and__Y2_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_true[\"Y3=true\"]\nS__and__X_1__and__Y1_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true[\"Y2=true\"]\nS__and__X_0__and__Y1_false -- 1.0 --&gt; S__and__X_0__and__Y1_false__and__Y2_false[\"Y2=false\"]\nS__and__X_1__and__Y1_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false[\"Y2=false\"]\nS__and__X_1__and__Y1_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true[\"Y2=true\"]\nS__and__X_2 -- 1.0 --&gt; S__and__X_2__and__Y1_true[\"Y1=true\"]\nS__and__X_1__and__Y1_false__and__Y2_true__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_true__and__Y4_false[\"Y4=false\"]\nS__and__X_1 -- 0.5 --&gt; S__and__X_1__and__Y1_true[\"Y1=true\"]\nS -- 0.33 --&gt; S__and__X_2[\"X=2\"]\nS__and__X_1__and__Y1_true__and__Y2_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_false[\"Y3=false\"]\nS__and__X_2__and__Y1_true__and__Y2_true -- 1.0 --&gt; S__and__X_2__and__Y1_true__and__Y2_true__and__Y3_true[\"Y3=true\"]\nS__and__X_1__and__Y1_false__and__Y2_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_true[\"Y3=true\"]\nS__and__X_1__and__Y1_false__and__Y2_true__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_false__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_true__and__Y2_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_true[\"Y3=true\"]\nS__and__X_1__and__Y1_true__and__Y2_true__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_true__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_false__and__Y2_true__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_false__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_true__and__Y2_true__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_true__and__Y3_false__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_false__and__Y2_false__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_true__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_true__and__Y2_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_true[\"Y3=true\"]\nS__and__X_1__and__Y1_true__and__Y2_false__and__Y3_false -- 0.5 --&gt; S__and__X_1__and__Y1_true__and__Y2_false__and__Y3_false__and__Y4_false[\"Y4=false\"]\nS__and__X_1__and__Y1_false__and__Y2_false__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_true__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_false__and__Y2_true__and__Y3_true -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_true__and__Y3_true__and__Y4_true[\"Y4=true\"]\nS__and__X_1__and__Y1_false__and__Y2_false -- 0.5 --&gt; S__and__X_1__and__Y1_false__and__Y2_false__and__Y3_false[\"Y3=false\"]\nS__and__X_0__and__Y1_false__and__Y2_false -- 1.0 --&gt; S__and__X_0__and__Y1_false__and__Y2_false__and__Y3_false[\"Y3=false\"]",
    "crumbs": [
      "Probability essentials",
      "Decision trees"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic05_decision_diagrams.html#nodes-and-events",
    "href": "w01_discrete_inference/topic05_decision_diagrams.html#nodes-and-events",
    "title": "Decision trees",
    "section": "Nodes and events",
    "text": "Nodes and events\nTo describe the event corresponding to a node \\(v\\) in the tree:\n\ntrace the path from the node \\(v\\) to the root\ntake the intersection of all node labels.\n\nExample: find the node in the above tree corresponding to the event \\((X = 1) \\cap (Y_1 = 0)\\).\nProbability notation review:\n\n\\((X = 1) = \\{s \\in S : X(s) = 1\\}\\)\n\\((X = 1, Y_1 = 0) = (X = 1) \\cap (Y_1 = 0)\\)",
    "crumbs": [
      "Probability essentials",
      "Decision trees"
    ]
  },
  {
    "objectID": "w01_discrete_inference/topic05_decision_diagrams.html#edges-and-conditional-probabilities",
    "href": "w01_discrete_inference/topic05_decision_diagrams.html#edges-and-conditional-probabilities",
    "title": "Decision trees",
    "section": "Edges and conditional probabilities",
    "text": "Edges and conditional probabilities\nWhen there is an edge from events \\(E_1\\) to \\(E_2\\), we annotate it with \\(\\mathbb{P}(E_2 | E_1)\\).\nRecall: conditional probability of \\(E_2\\) given \\(E_1\\)\n\\[\\mathbb{P}(E_2 | E_1) = \\frac{\\mathbb{P}(E_1 \\cap E_2)}{\\mathbb{P}(E_1)}\\]\nExample:\n\ntake the edge from \\(E_1 = (X = 1)\\) to \\(E_2 = (X = 1, Y_1 = 0)\\). \\[\\mathbb{P}(E_2 | E_1) = \\frac{\\mathbb{P}(E_1 \\cap E_2)}{\\mathbb{P}(E_1)} = \\frac{\\mathbb{P}(E_2)}{\\mathbb{P}(E_1)} = \\mathbb{P}(Y_1 = 0 | X = 1)\\]\nTranslating \\(\\mathbb{P}(Y_1 = 0 | X = 1)\\) into words: “the probability that the first flip is ‘heads’ \\((Y_1 = 0)\\) given that you picked the standard coin \\((X = 1)\\).’’\nHence the edge from \\(E_1\\) to \\(E_2\\) is labelled \\(1/2\\).",
    "crumbs": [
      "Probability essentials",
      "Decision trees"
    ]
  },
  {
    "objectID": "drafts/ex02.html",
    "href": "drafts/ex02.html",
    "title": "Exercise 2: Bayesian inference on discrete spaces",
    "section": "",
    "text": "Caution\n\n\n\nPage under construction: information on this page may change."
  },
  {
    "objectID": "drafts/ex02.html#goals",
    "href": "drafts/ex02.html#goals",
    "title": "Exercise 2: Bayesian inference on discrete spaces",
    "section": "Goals",
    "text": "Goals\nA first contact with several Bayesian concepts. We use the same discrete model for all questions.\n\nNotions of priors, likelihood, posterior.\nPoint summary of posterior distribution: mean, median, mode.\nConstructing credible intervals.\nBasic decision theory.\nSequential update of posterior distribution."
  },
  {
    "objectID": "drafts/ariane_stan.html",
    "href": "drafts/ariane_stan.html",
    "title": "Stan: hands on",
    "section": "",
    "text": "We revisit the time-varying Ariane 1 rocket failure probability model to practice the Stan syntax introduced this week.\n\n\n\n\nsuppressPackageStartupMessages(require(rstan))\nsuppressPackageStartupMessages(require(ggplot2))\nsuppressPackageStartupMessages(require(dplyr))\n\nset.seed(1)\n\ndf = read.csv(url(\"https://raw.githubusercontent.com/UBC-Stat-ML/web447/main/data/launches.csv\")) %&gt;% filter(LV.Type == \"Ariane 1\")\nsuccess_indicators = df$Suc_bin\nrmarkdown::paged_table(df)\n\n\n  \n\n\n\n\nplot(success_indicators, xlab = \"Launch index i\")"
  },
  {
    "objectID": "drafts/ariane_stan.html#outline",
    "href": "drafts/ariane_stan.html#outline",
    "title": "Stan: hands on",
    "section": "",
    "text": "We revisit the time-varying Ariane 1 rocket failure probability model to practice the Stan syntax introduced this week.\n\n\n\n\nsuppressPackageStartupMessages(require(rstan))\nsuppressPackageStartupMessages(require(ggplot2))\nsuppressPackageStartupMessages(require(dplyr))\n\nset.seed(1)\n\ndf = read.csv(url(\"https://raw.githubusercontent.com/UBC-Stat-ML/web447/main/data/launches.csv\")) %&gt;% filter(LV.Type == \"Ariane 1\")\nsuccess_indicators = df$Suc_bin\nrmarkdown::paged_table(df)\n\n\n  \n\n\n\n\nplot(success_indicators, xlab = \"Launch index i\")"
  },
  {
    "objectID": "drafts/ariane_stan.html#model",
    "href": "drafts/ariane_stan.html#model",
    "title": "Stan: hands on",
    "section": "Model",
    "text": "Model\nRecall the model we discussed previously:\n\\[\\begin{align*}\n\\text{slope} &\\sim \\mathcal{N}(0, 1) \\\\\n\\text{intercept} &\\sim \\mathcal{N}(0, 1) \\\\\n\\theta(i) &= \\text{logistic}(\\text{slope} \\cdot i + \\text{intercept}) \\\\\ny_i &\\sim {\\mathrm{Bern}}(\\theta(i))\n\\end{align*}\\]\n…which you also implemented in simPPLe as part of exercise 4:\n\nlogistic_regression = function() {\n  intercept = simulate(Norm(0, 1))\n  slope     = simulate(Norm(0, 1))\n  for (i in seq_along(success_indicators)){\n    success_probability = plogis(intercept + i*slope)\n    observe(success_indicators[i], Bern(success_probability))\n  }\n  return(c(intercept, slope))\n}"
  },
  {
    "objectID": "drafts/ariane_stan.html#translation-into-stan",
    "href": "drafts/ariane_stan.html#translation-into-stan",
    "title": "Stan: hands on",
    "section": "Translation into Stan",
    "text": "Translation into Stan\nQuestion 1: use the template below to translate the above model into Stan. Set the seed to 1, run 10000 MCMC iterations, and report the posterior mean of the slope parameter.\n\n\ndata { \n  int N; \n  array[N] int y; \n}\n\nparameters { \n  real slope;\n  real intercept;\n}\n\ntransformed parameters { \n  vector[N] mu = \n    inv_logit(intercept + slope*linspaced_vector(N, 1, N));\n}\n\nmodel {\n  slope ~ normal(0, 10);\n  intercept ~ normal(0, 10);\n  y ~ bernoulli(mu);\n}\n\n\nfit = sampling(\n  logistic, \n  data = list( \n          y = success_indicators, \n          N = length(success_indicators)\n        ), \n  chains = 1,\n  iter = 10000       \n)\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.5e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.046 seconds (Warm-up)\nChain 1:                0.05 seconds (Sampling)\nChain 1:                0.096 seconds (Total)\nChain 1: \n\n\n\nfit\n\nInference for Stan model: anon_model.\n1 chains, each with iter=10000; warmup=5000; thin=1; \npost-warmup draws per chain=5000, total post-warmup draws=5000.\n\n           mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nslope      0.55    0.01 0.41 -0.15  0.27  0.51  0.79  1.47  1133    1\nintercept -0.60    0.05 1.82 -4.15 -1.78 -0.61  0.55  3.20  1180    1\nmu[1]      0.49    0.01 0.27  0.05  0.26  0.48  0.71  0.96  1388    1\nmu[2]      0.59    0.01 0.23  0.14  0.42  0.61  0.78  0.96  1647    1\nmu[3]      0.70    0.00 0.19  0.29  0.58  0.73  0.85  0.97  2265    1\nmu[4]      0.79    0.00 0.15  0.44  0.71  0.82  0.91  0.98  3247    1\nmu[5]      0.85    0.00 0.12  0.56  0.79  0.88  0.94  0.99  3840    1\nmu[6]      0.89    0.00 0.10  0.63  0.84  0.92  0.97  1.00  3473    1\nmu[7]      0.92    0.00 0.09  0.67  0.88  0.95  0.98  1.00  2748    1\nmu[8]      0.94    0.00 0.09  0.69  0.91  0.97  0.99  1.00  2182    1\nmu[9]      0.95    0.00 0.08  0.69  0.93  0.98  1.00  1.00  1816    1\nmu[10]     0.95    0.00 0.09  0.68  0.95  0.99  1.00  1.00  1576    1\nmu[11]     0.96    0.00 0.09  0.67  0.96  0.99  1.00  1.00  1424    1\nlp__      -5.54    0.03 1.12 -8.54 -6.01 -5.21 -4.73 -4.41  1210    1\n\nSamples were drawn using NUTS(diag_e) at Wed Mar  6 21:17:48 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\nWe can extract samples as follows:\n\nsamples = extract(fit)$mu\ndata = success_indicators\nn_samples = nrow(samples)\n\n\nxs = 1:length(data)\nplot(xs, data,\n     xlab = \"Launch index\", \n     ylab = \"Success probability (lines) / indicator (circles)\")\n\nfor (i in 1:n_samples) {\n  lines(xs, samples[i,], col = rgb(red = 0, green = 0, blue = 0, alpha = 0.01))\n}"
  },
  {
    "objectID": "drafts/ariane_stan.html#using-the-posterior",
    "href": "drafts/ariane_stan.html#using-the-posterior",
    "title": "Stan: hands on",
    "section": "Using the posterior",
    "text": "Using the posterior\nQuestion 2: compute a 95% credible interval on the slope parameter.\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse println on the fit object.\n\n\n\nQuestion 3: translate “your updated belief that the Ariane 1 rockets were improving” into a mathematical expression.\nQuestion 4: estimate the numerical value of the expression in the last question."
  },
  {
    "objectID": "drafts/topic06_ess.html",
    "href": "drafts/topic06_ess.html",
    "title": "Effective sample size",
    "section": "",
    "text": "TODO\n\n\n\nTODO"
  },
  {
    "objectID": "drafts/topic06_ess.html#outline",
    "href": "drafts/topic06_ess.html#outline",
    "title": "Effective sample size",
    "section": "",
    "text": "TODO\n\n\n\nTODO"
  },
  {
    "objectID": "drafts/topic06_ess.html#todo",
    "href": "drafts/topic06_ess.html#todo",
    "title": "Effective sample size",
    "section": "TODO",
    "text": "TODO\nTODO: look up bound in Chopin\nTODO: link up with rates plot - intuition of the MCSE\nTODO: more readings on SNIS\n\nChopin\nOwen\nPareto stuffs"
  },
  {
    "objectID": "challenges/ch08.html",
    "href": "challenges/ch08.html",
    "title": "Challenge questions",
    "section": "",
    "text": "Not for grades!\n\n\n\nThese are not essential for learning the material and can be skipped without affecting your grade. If you successfully solve one set of problem, a week of participation activity will be waived (it does not have to be the same week you submit the challenge question). Submit your answer at any time. I will not post solutions for the challenge questions.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nPage under construction: information on this page may change.",
    "crumbs": [
      "Bayesian workflow",
      "Challenge"
    ]
  },
  {
    "objectID": "challenges/ch02.html",
    "href": "challenges/ch02.html",
    "title": "Challenge questions",
    "section": "",
    "text": "Not for grades!\n\n\n\nThese are not essential for learning the material and can be skipped without affecting your grade. If you successfully solve one set of problem, a week of participation activity will be waived (it does not have to be the same week you submit the challenge question). Submit your answer at any time. I will not post solutions for the challenge questions.\n\n\nAfter doing Exercise 2 consider the following problem:\n\nWrite a function that takes as input posterior_probabilities and builds the highest probability set for any \\(\\alpha\\in[0,1]\\).\nReport a \\(75\\%\\) highest probability set, making sure that the posterior probability of the set you return does not fall below \\(75\\%\\).",
    "crumbs": [
      "Bayes on a discrete model",
      "Challenge"
    ]
  },
  {
    "objectID": "challenges/ch06.html",
    "href": "challenges/ch06.html",
    "title": "Challenge questions",
    "section": "",
    "text": "Not for grades!\n\n\n\nThese are not essential for learning the material and can be skipped without affecting your grade. If you successfully solve one set of problem, a week of participation activity will be waived (it does not have to be the same week you submit the challenge question). Submit your answer at any time. I will not post solutions for the challenge questions.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nPage under construction: information on this page may change.",
    "crumbs": [
      "Hierarchical models",
      "Challenge"
    ]
  },
  {
    "objectID": "challenges/ch10.html",
    "href": "challenges/ch10.html",
    "title": "Challenge questions",
    "section": "",
    "text": "Not for grades!\n\n\n\nThese are not essential for learning the material and can be skipped without affecting your grade. If you successfully solve one set of problem, a week of participation activity will be waived (it does not have to be the same week you submit the challenge question). Submit your answer at any time. I will not post solutions for the challenge questions.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nPage under construction: information on this page may change.",
    "crumbs": [
      "Advanced inference",
      "Challenge"
    ]
  },
  {
    "objectID": "w02_discrete_bayes/topic01_bayes_recipe.html",
    "href": "w02_discrete_bayes/topic01_bayes_recipe.html",
    "title": "The Bayesian recipe",
    "section": "",
    "text": "Introduce the Bayesian Recipe (synonym for the Bayes estimator)\nIllustrate it using this week’s running example\n\n\n\n\nThe Bayesian Recipe/Bayes estimator is the guide for all “full Bayesian” statistical analyses. This week we apply it to an example that builds on last week’s review.",
    "crumbs": [
      "Bayes on a discrete model",
      "The Bayesian recipe"
    ]
  },
  {
    "objectID": "w02_discrete_bayes/topic01_bayes_recipe.html#outline",
    "href": "w02_discrete_bayes/topic01_bayes_recipe.html#outline",
    "title": "The Bayesian recipe",
    "section": "",
    "text": "Introduce the Bayesian Recipe (synonym for the Bayes estimator)\nIllustrate it using this week’s running example\n\n\n\n\nThe Bayesian Recipe/Bayes estimator is the guide for all “full Bayesian” statistical analyses. This week we apply it to an example that builds on last week’s review.",
    "crumbs": [
      "Bayes on a discrete model",
      "The Bayesian recipe"
    ]
  },
  {
    "objectID": "w02_discrete_bayes/topic01_bayes_recipe.html#sec-running",
    "href": "w02_discrete_bayes/topic01_bayes_recipe.html#sec-running",
    "title": "The Bayesian recipe",
    "section": "This week’s running example",
    "text": "This week’s running example\n\nYou are consulting for a satellite operator\nThey are about to send a $100M satellite on a Delta 7925H rocket\n\n\n\n\n\nData: as of Jan 2025, Delta 7925H rockets have been launched 3 times, with 0 failed launches \n\nNote: Delta 7925H is not reusable, so each rocket is “copy- built” from the same blueprint\n\nShould you recommend buying a $2M insurance policy?\n\nConvention: use 1 for a success, 0 for a failure.",
    "crumbs": [
      "Bayes on a discrete model",
      "The Bayesian recipe"
    ]
  },
  {
    "objectID": "w02_discrete_bayes/topic01_bayes_recipe.html#the-bayesian-recipe",
    "href": "w02_discrete_bayes/topic01_bayes_recipe.html#the-bayesian-recipe",
    "title": "The Bayesian recipe",
    "section": "The Bayesian recipe",
    "text": "The Bayesian recipe\nThe goal this week is to undersand the 3 steps in the Bayesian recipe:\n\nConstruct a probability model including\n\nrandom variables for what we will measure/observe\nrandom variables for the unknown quantities\n\nthose we are interested in (“parameters”, “predictions”)\nothers that just help us formulate the problem (“nuisance”, “random effects”).\n\n\nCompute the posterior distribution (condition on the data)\nUse the posterior distribution to (decision theory):\n\nmake prediction (point estimate)\nestimate uncertainty (credible intervals)\nmake a decision",
    "crumbs": [
      "Bayes on a discrete model",
      "The Bayesian recipe"
    ]
  },
  {
    "objectID": "w02_discrete_bayes/topic01_bayes_recipe.html#plan",
    "href": "w02_discrete_bayes/topic01_bayes_recipe.html#plan",
    "title": "The Bayesian recipe",
    "section": "Plan",
    "text": "Plan\n\nUnderstanding Step 1 (“construct a probability model”):\n\nwe reviewed probability models last week,\nin fact, the model we use for this week’s problem is the same as last week’s!\nWe will just need to add some Bayesian terminology: prior, likelihood.\n\nUnderstanding Step 2 (“condition on the data”):\n\nwe reviewed conditional probability last week,\nin fact, the conditional probability calculation for this week’s problem is the same as last week’s!\nWe just need to add some Bayesian terminology: posterior distributions.\n\nUnderstanding Step 3: this is where most of the new material will be for this week.",
    "crumbs": [
      "Bayes on a discrete model",
      "The Bayesian recipe"
    ]
  },
  {
    "objectID": "w02_discrete_bayes/topic07_prediction.html",
    "href": "w02_discrete_bayes/topic07_prediction.html",
    "title": "Prediction",
    "section": "",
    "text": "Prediction using decision trees\nExample\n\n\n\n\nOften we do not care so much about “parameters” but instead about predicting future observations.",
    "crumbs": [
      "Bayes on a discrete model",
      "Prediction"
    ]
  },
  {
    "objectID": "w02_discrete_bayes/topic07_prediction.html#outline",
    "href": "w02_discrete_bayes/topic07_prediction.html#outline",
    "title": "Prediction",
    "section": "",
    "text": "Prediction using decision trees\nExample\n\n\n\n\nOften we do not care so much about “parameters” but instead about predicting future observations.",
    "crumbs": [
      "Bayes on a discrete model",
      "Prediction"
    ]
  },
  {
    "objectID": "w02_discrete_bayes/topic07_prediction.html#example-coins-in-a-bag",
    "href": "w02_discrete_bayes/topic07_prediction.html#example-coins-in-a-bag",
    "title": "Prediction",
    "section": "Example: coins in a bag",
    "text": "Example: coins in a bag\nConsider the setup from last week with 3 coins and 3 flips with \\(Y = (1, 1, 1)\\) (in the following, let \\(\\boldsymbol{1}\\) denote a vector of 1’s).\nQuestion: given you have see 3 heads, what is the probability that the next one is also heads?\nMathematically: \\(\\mathbb{P}(Y_4 = 1 | Y_{1:3} = \\boldsymbol{1})\\). This is known as “prediction”.",
    "crumbs": [
      "Bayes on a discrete model",
      "Prediction"
    ]
  },
  {
    "objectID": "w02_discrete_bayes/topic07_prediction.html#general-approach",
    "href": "w02_discrete_bayes/topic07_prediction.html#general-approach",
    "title": "Prediction",
    "section": "General approach",
    "text": "General approach\nKey message: In Bayesian statistics, prediction and parameter estimation are treated in the exact same way!\nIdea: Add \\(Y_4\\) to the unobserved random variables, i.e. set \\(\\tilde X = (X, Y_4)\\).\nThen, to compute \\(\\mathbb{P}(Y_4 = 1 | Y_{1:3} = \\boldsymbol{1})\\) use same techniques as last week (decision tree, chain rule, axioms of probability).",
    "crumbs": [
      "Bayes on a discrete model",
      "Prediction"
    ]
  },
  {
    "objectID": "w02_discrete_bayes/topic07_prediction.html#example-continued",
    "href": "w02_discrete_bayes/topic07_prediction.html#example-continued",
    "title": "Prediction",
    "section": "Example, continued",
    "text": "Example, continued\nUse the following picture to help you computing \\(\\mathbb{P}(Y_4 = 1 | Y_{1:3} = \\boldsymbol{1})\\).\n\nNotation: let \\(\\gamma(i) = \\mathbb{P}(Y_{1:3} = \\boldsymbol{1}, Y_4 = i)\\).\nQuestion: compute \\(\\gamma(0)\\).\n\n\n\n\n\n\nClick for choices\n\n\n\n\n\n\n\\(\\approx 0.02\\)\n\\(\\approx 0.06\\)\n\\(\\approx 0.52\\)\n\\(\\approx 0.94\\)\nNone of the above\n\n\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nThere is only one way to get \\((Y_{1:3} = \\boldsymbol{1}, Y_4 = 0)\\): this has to be the standard coin, i.e., \\[(Y_{1:3} = \\boldsymbol{1}, Y_4 = 0) = (X = 1, Y_{1:3} = \\boldsymbol{1}, Y_4 = 0)\\]\nTo compute the probability of that path we can multiply the edge probabilities (why?): \\[\\gamma(0) = \\mathbb{P}(X = 1, Y_{1:3} = \\boldsymbol{1}, Y_4 = 0) = (1/3) \\times (1/2)^4 = 1/48 \\approx 0.02.\\]\n\n\n\n\nQuestion: compute \\(\\gamma(1)\\).\n\n\n\n\n\n\nClick for choices\n\n\n\n\n\n\n\\(\\approx 0.11\\)\n\\(\\approx 0.35\\)\n\\(\\approx 0.52\\)\n\\(\\approx 0.94\\)\nNone of the above\n\n\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nTwist: two distinct paths are compatible with the event: \\[(Y_{1:4} = \\boldsymbol{1}) = (X = 2, Y_{1:4} = \\boldsymbol{1}) \\cup (X = 1, Y_{1:4} = \\boldsymbol{1}).\\]\nSum the probabilities of the paths leading to the same prediction (why can we do this?).: \\[\\begin{align*}\n\\mathbb{P}(Y_{1:4} = \\boldsymbol{1}) &= \\mathbb{P}(X = 2, Y_{1:4} = \\boldsymbol{1}) + \\mathbb{P}(X = 1, Y_{1:4} = \\boldsymbol{1}) \\\\\n&= 1/48 + 1/3 \\approx 0.35.\n\\end{align*}\\]\n\n\n\n\nQuestion: compute the predictive\n\n\n\n\n\n\nClick for choices\n\n\n\n\n\n\n\\(\\approx 0.94\\)\n\\(\\approx 0.52\\)\n\\(\\approx 0.35\\)\n\\(\\approx 0.11\\)\nNone of the above\n\n\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nLet: \\[\\pi(i) := \\mathbb{P}(Y_4 = i | Y_{1:3} = \\boldsymbol{1}).\\]\nNote: \\[\\pi(i) \\propto \\gamma(i).\\]\nHence: \\[(\\pi(0), \\pi(1)) = \\frac{(\\gamma(0), \\gamma(1))}{\\gamma(0) + \\gamma(1)}.\\] Therefore we get: \\[\\mathbb{P}(Y_4 = 1|Y_{1:3} = \\boldsymbol{1}) = \\frac{\\gamma(1)}{\\gamma(0) + \\gamma(1)} = 17/18 \\approx 0.94.\\]",
    "crumbs": [
      "Bayes on a discrete model",
      "Prediction"
    ]
  },
  {
    "objectID": "w02_discrete_bayes/topic05_credible.html",
    "href": "w02_discrete_bayes/topic05_credible.html",
    "title": "Credible sets",
    "section": "",
    "text": "Nominal coverage.\nCommon credible sets:\n\nQuantiles.\nHighest-density sets.\n\n\n\n\n\nAs we have seen at the beginning one motivation for Bayesian methods is that they allow us to quantify uncertainty in our predictions. Credible sets is one way to convey this uncertainty quantification.",
    "crumbs": [
      "Bayes on a discrete model",
      "Credible sets"
    ]
  },
  {
    "objectID": "w02_discrete_bayes/topic05_credible.html#outline",
    "href": "w02_discrete_bayes/topic05_credible.html#outline",
    "title": "Credible sets",
    "section": "",
    "text": "Nominal coverage.\nCommon credible sets:\n\nQuantiles.\nHighest-density sets.\n\n\n\n\n\nAs we have seen at the beginning one motivation for Bayesian methods is that they allow us to quantify uncertainty in our predictions. Credible sets is one way to convey this uncertainty quantification.",
    "crumbs": [
      "Bayes on a discrete model",
      "Credible sets"
    ]
  },
  {
    "objectID": "w02_discrete_bayes/topic05_credible.html#nominal-coverage",
    "href": "w02_discrete_bayes/topic05_credible.html#nominal-coverage",
    "title": "Credible sets",
    "section": "Nominal coverage",
    "text": "Nominal coverage\nThe “nominal coverage” is the basic property that we use to construct credible sets:\n\nYou are given a level, typically \\(95\\%\\) or \\(90\\%\\), denoted \\(1-\\alpha = 0.95\\) or \\(1-\\alpha = 0.9\\)\n\nIntuition: we want to find a set \\(T\\) such that we are \\(95\\%\\) sure that the true \\(x\\) is in \\(T\\).\n\nIdeally, we would want to find a set \\(T\\) such that \\(\\sum_{x \\in T} p(x) = 0.9\\)\n\nFor discrete distributions, there may not be a solution, but let us ignore that issue for now (suppose each probability is very small and replace \\(=\\) by \\(\\approx\\) up to an error bounded by the largest individual probability)\nFor continuous distributions that will not be an issue\n\nAnother problem: there are many solutions! Some of them might appear weird, e.g. include points of probability zero!\n\nExample: both sets shown in red contain \\(75\\%\\) of the mass.",
    "crumbs": [
      "Bayes on a discrete model",
      "Credible sets"
    ]
  },
  {
    "objectID": "w02_discrete_bayes/topic05_credible.html#quantile-based-credible-interval",
    "href": "w02_discrete_bayes/topic05_credible.html#quantile-based-credible-interval",
    "title": "Credible sets",
    "section": "Quantile-based credible interval",
    "text": "Quantile-based credible interval\nThe simplest way of building credible intervals:\n\nsuppose you want a \\(90\\%\\) credible set\nwe will remove \\(10\\%/2 = 5\\%\\) on each side\nremove sticks from the very left moving to right\n\neach time you remove one, add its length to a counter…\nuntil that counter hits \\(5\\%\\).\n\nthen do the same from very right moving to the left\nthe sticks left form \\(T\\).",
    "crumbs": [
      "Bayes on a discrete model",
      "Credible sets"
    ]
  },
  {
    "objectID": "w02_discrete_bayes/topic05_credible.html#highest-density-set",
    "href": "w02_discrete_bayes/topic05_credible.html#highest-density-set",
    "title": "Credible sets",
    "section": "Highest Density Set",
    "text": "Highest Density Set\n\nHighest Density Set: pick a solution with the least number of points \\(|T|\\)\nTo handle discrete models: \\(\\text{argmin}\\{ |T| : \\sum_{x \\in T} p(x) \\ge 0.9\\}\\).\n\nStill not unique in some corner cases, but good enough\n\n\nExample:\n\nthe left one below is a highest density set\nnote the one on the right has more points in it, \\(5 &lt; 9\\).\n\n\nChallenge: implement a function taking as input a posterior PMF (as a vector), a level, and returns an HDI. Details here.",
    "crumbs": [
      "Bayes on a discrete model",
      "Credible sets"
    ]
  },
  {
    "objectID": "w02_discrete_bayes/topic05_credible.html#credible-interval",
    "href": "w02_discrete_bayes/topic05_credible.html#credible-interval",
    "title": "Credible sets",
    "section": "Credible interval",
    "text": "Credible interval\nWhen we move to continuous random variables, \\(T\\) will often be an interval, i.e. \\(T = [L, R]\\) for some left and right end points \\(L\\) and \\(R\\). In such case, \\(T\\) is called a credible interval.",
    "crumbs": [
      "Bayes on a discrete model",
      "Credible sets"
    ]
  },
  {
    "objectID": "w02_discrete_bayes/topic05_credible.html#highest-density-interval",
    "href": "w02_discrete_bayes/topic05_credible.html#highest-density-interval",
    "title": "Credible sets",
    "section": "Highest Density Interval",
    "text": "Highest Density Interval\nSimilarly, a Highest Density Interval (HDI) is a shortest interval (i.e. minimizing \\(R - L\\)) containing a prescribed probability mass.",
    "crumbs": [
      "Bayes on a discrete model",
      "Credible sets"
    ]
  },
  {
    "objectID": "w02_discrete_bayes/topic02_models.html",
    "href": "w02_discrete_bayes/topic02_models.html",
    "title": "Bayesian models",
    "section": "",
    "text": "Bayesian interpretation of probability models: from aleatoric to epistemic.\nTerminology: prior, likelihood, joint.\n\n\n\n\nThis week, we will give a new interpretation to the “bag of coin” example (where the uncertainty is “aleatoric”), turning it into a Bayesian model (where the uncertainty is “epistemic”). Mathematically this is the same model but with a different interpretation. This shift of interpretation is the basis of all Bayesian models.",
    "crumbs": [
      "Bayes on a discrete model",
      "Bayesian models"
    ]
  },
  {
    "objectID": "w02_discrete_bayes/topic02_models.html#outline",
    "href": "w02_discrete_bayes/topic02_models.html#outline",
    "title": "Bayesian models",
    "section": "",
    "text": "Bayesian interpretation of probability models: from aleatoric to epistemic.\nTerminology: prior, likelihood, joint.\n\n\n\n\nThis week, we will give a new interpretation to the “bag of coin” example (where the uncertainty is “aleatoric”), turning it into a Bayesian model (where the uncertainty is “epistemic”). Mathematically this is the same model but with a different interpretation. This shift of interpretation is the basis of all Bayesian models.",
    "crumbs": [
      "Bayes on a discrete model",
      "Bayesian models"
    ]
  },
  {
    "objectID": "w02_discrete_bayes/topic02_models.html#what-is-a-bayesian-model",
    "href": "w02_discrete_bayes/topic02_models.html#what-is-a-bayesian-model",
    "title": "Bayesian models",
    "section": "What is a Bayesian model?",
    "text": "What is a Bayesian model?\nA Bayesian model is a probability model equipped with:\n\nrandom variable(s) representing the data (we use \\(Y\\) in this course)\nrandom variable(s) for the unknown quantities (we use \\(X\\) in this course).\n\nNote: concretely, a Bayesian model is a joint PMF over \\(X\\) and \\(Y\\), typically written using the “\\(\\sim\\)” notation introduced last week.",
    "crumbs": [
      "Bayes on a discrete model",
      "Bayesian models"
    ]
  },
  {
    "objectID": "w02_discrete_bayes/topic02_models.html#from-coins-to-rockets",
    "href": "w02_discrete_bayes/topic02_models.html#from-coins-to-rockets",
    "title": "Bayesian models",
    "section": "From coins to rockets",
    "text": "From coins to rockets\nFor this week’s rocket example, we will use the exact same probability model as the bag of coin example from last week.\nUse the following correspondence to link last week’s example with this week’s:\n\n\\((Y_i = 1)\\):\n\n\\(\\leftrightarrow\\) “\\(i\\)-th flip is a heads”\n\\(\\leftrightarrow\\) “\\(i\\)-th launch is a success”\n\n\\((X = k)\\):\n\n\\(\\leftrightarrow\\) “the coin drawn from bag has probability \\(p = k/K\\) of heads”\n\\(\\leftrightarrow\\) “this type of rocket has probability \\(p = k/K\\) of success.”",
    "crumbs": [
      "Bayes on a discrete model",
      "Bayesian models"
    ]
  },
  {
    "objectID": "w02_discrete_bayes/topic02_models.html#terminology",
    "href": "w02_discrete_bayes/topic02_models.html#terminology",
    "title": "Bayesian models",
    "section": "Terminology",
    "text": "Terminology\nThe task of constructing a Bayesian model is often broken down into constructing:\n\nthe prior: the PMF of the unknown \\(X\\),1\nthe likelihood: the conditional PMF of observed \\(Y\\) given \\(X\\).",
    "crumbs": [
      "Bayes on a discrete model",
      "Bayesian models"
    ]
  },
  {
    "objectID": "w02_discrete_bayes/topic02_models.html#epistemic-vs-aleatoric-probability",
    "href": "w02_discrete_bayes/topic02_models.html#epistemic-vs-aleatoric-probability",
    "title": "Bayesian models",
    "section": "Epistemic vs aleatoric probability",
    "text": "Epistemic vs aleatoric probability\n\nIn the bag of coin example, we can replicate the whole “experiment” several times (both prior and likelihood).\nIn the rocket example, we can only replicate launches (likelihood), not the prior sampling part.\n\nTerminology:\n\nEpistemic probability: the part we cannot replicate.\nAleatoric probability: the part that can be replicated.\n\nUses:\n\nBayesian statistics uses both types of probability,\nwhereas other fields of statistics, e.g. MLE, typically uses only aleatoric.",
    "crumbs": [
      "Bayes on a discrete model",
      "Bayesian models"
    ]
  },
  {
    "objectID": "w02_discrete_bayes/topic02_models.html#footnotes",
    "href": "w02_discrete_bayes/topic02_models.html#footnotes",
    "title": "Bayesian models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nif the unknown quantity is continuous, the prior will be expressed using a density. A term that captures both the continuous and discrete case is “distribution” i.e. “prior distribution”.↩︎",
    "crumbs": [
      "Bayes on a discrete model",
      "Bayesian models"
    ]
  },
  {
    "objectID": "w03_ppl/topic02_snis.html",
    "href": "w03_ppl/topic02_snis.html",
    "title": "Importance sampling",
    "section": "",
    "text": "What Self-Normalizing Importance Sampling (SNIS) does.\nIntuition on how SNIS does it.\nSNIS pseudo-code.\n\n\n\n\nSince SNIS is the “inference algorithm” used by simPPLe, you will need a basic understanding of SNIS to complete this week’s exercise.",
    "crumbs": [
      "A first look at PPLs",
      "Importance sampling"
    ]
  },
  {
    "objectID": "w03_ppl/topic02_snis.html#outline",
    "href": "w03_ppl/topic02_snis.html#outline",
    "title": "Importance sampling",
    "section": "",
    "text": "What Self-Normalizing Importance Sampling (SNIS) does.\nIntuition on how SNIS does it.\nSNIS pseudo-code.\n\n\n\n\nSince SNIS is the “inference algorithm” used by simPPLe, you will need a basic understanding of SNIS to complete this week’s exercise.",
    "crumbs": [
      "A first look at PPLs",
      "Importance sampling"
    ]
  },
  {
    "objectID": "w03_ppl/topic02_snis.html#goal",
    "href": "w03_ppl/topic02_snis.html#goal",
    "title": "Importance sampling",
    "section": "Goal",
    "text": "Goal\n\nOur goal is to compute \\(\\mathbb{E}[g(X) | Y = y]\\).\nSNIS will allow us to approximate this conditional expectation.\n\nHow is this different than the simple Monte Carlo method that we used in Ex1.Q.1.3?\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nWe used simple Monte Carlo to compute an expectation without conditioning on \\(Y = y\\), i.e. \\(\\mathbb{E}[g(X)]\\).\nSNIS allows us to take into account observed data.",
    "crumbs": [
      "A first look at PPLs",
      "Importance sampling"
    ]
  },
  {
    "objectID": "w03_ppl/topic02_snis.html#problem-formulation",
    "href": "w03_ppl/topic02_snis.html#problem-formulation",
    "title": "Importance sampling",
    "section": "Problem formulation",
    "text": "Problem formulation\nSNIS is used to solve the following type of problems:\n\nInput: You are given an unnormalized distribution \\(\\gamma(x)\\)\n\nExample: the numerator in Bayes rule.\n\nOutput: You want to approximate an expectation…\n\n…under the renormalized distribution \\(\\pi(x) = \\gamma(x) / Z\\).\nMathematically: SNIS provides an approximation to \\(\\mathbb{E}_\\pi[g(X)] = \\sum \\pi(x) g(x)\\).\n\n\nTerminology: \\(g\\) is called a test function. Think about it as specifying a query.\nExamples: of test functions\n\nLet’s say you want to approximate \\(\\mathbb{P}(X = 1 | Y = y)\\).\n\nWe can do this with SNIS by picking the right \\(g\\)!\nFrom last week’s “trick 2”, if we take \\(g(x) = \\mathbb{1}[x = 1]\\), \\(\\pi(x) = p(x | y)\\), \\[\\mathbb{E}_\\pi[g(X)] = \\sum \\pi(x) g(x) = \\mathbb{E}[\\mathbb{1}[X = 1] | Y = y] = \\mathbb{P}(X = 1 | Y = y).\\]\n\nWhat function \\(g(x)\\) would you take to compute a posterior mean?\n\n\n\n\n\n\n\nClick for choices\n\n\n\n\n\n\n\\(g(x) = \\mathbb{1}[X = x]\\)\n\\(g(x) = \\mathbb{1}[x]\\)\n\\(g(x) = x^2\\)\n\\(g(x) = x\\)\nNone of the above\n\n\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nIf we set \\(g(x) = x\\), then \\(\\sum x \\pi(x) = \\mathbb{E}[X | Y = y]\\).\n\n\n\n\nHow would you proceed to compute \\(\\operatorname{Var}[X | Y = y]\\)?",
    "crumbs": [
      "A first look at PPLs",
      "Importance sampling"
    ]
  },
  {
    "objectID": "w03_ppl/topic02_snis.html#snis-extra-ingredient-the-proposal",
    "href": "w03_ppl/topic02_snis.html#snis-extra-ingredient-the-proposal",
    "title": "Importance sampling",
    "section": "SNIS’ extra ingredient: the proposal",
    "text": "SNIS’ extra ingredient: the proposal\nIn addition to the unnormalized distribution \\(\\gamma\\) and test function \\(f\\), SNIS requires a proposal \\(q(x)\\).\n\nThe proposal will help SNIS explore the space of possible values that \\(X\\) can take.\nThis week, we will use the prior as the proposal, \\(q(x) = \\rho(x)\\).\n\nIt can be generalized to any PMF \\(q(x)\\) such that \\(q(x) = 0 \\Longrightarrow \\pi(x) = 0\\).",
    "crumbs": [
      "A first look at PPLs",
      "Importance sampling"
    ]
  },
  {
    "objectID": "w03_ppl/topic02_snis.html#intuition-decision-tree",
    "href": "w03_ppl/topic02_snis.html#intuition-decision-tree",
    "title": "Importance sampling",
    "section": "Intuition: decision tree",
    "text": "Intuition: decision tree\n\nRecall the forward_sample function from the first exercise, Q.1.3.\nSNIS follows the same general approach (Monte Carlo),\n\ntraverse the decision tree several times to get “samples”\nthe key difference is that these samples will not be equally weighted in SNIS in contrast to Q.1.3.\n\nAt each iteration, just as forward sampling, SNIS goes down the decision tree…\n\n…but for some decision points, the choice is made for us—by the data!\n\nCall these “forced choices.”\n\nWe keep track of the probability of the “forced choices”,\n\nthe product of these probability will be the un-normalized weight of the sample.",
    "crumbs": [
      "A first look at PPLs",
      "Importance sampling"
    ]
  },
  {
    "objectID": "w03_ppl/topic02_snis.html#algorithm",
    "href": "w03_ppl/topic02_snis.html#algorithm",
    "title": "Importance sampling",
    "section": "Algorithm",
    "text": "Algorithm\n\nCall the proposal \\(M\\) times.\n\nDenote the output at iteration \\(m \\in \\{1, 2, \\dots M\\}\\) by: \\[(X^{(m)}) \\sim q(\\cdot)\\]\nCompute \\(g\\) on each, call each of the \\(M\\) outputs \\(G^{(m)}\\) \\[G^{(m)}= g(X^{(m)}).\\]\nCompute also an un-normalized weight for each of the \\(M\\) outputs: \\[W^{(m)}= w(X^{(m)}) = \\frac{\\gamma(X^{(m)})}{q(X^{(m)})}.\\] - Here \\(w(x)\\) is a weighing function.\n\nReturn the ratio \\[\\hat G_M = \\frac{\\sum_{m=1}^M W^{(m)}G^{(m)}}{\\sum_{m=1}^M W^{(m)}} .\\]\n\nExample: compute \\(\\hat G_M\\) in the bag of coin example if \\(g(x) = x\\), \\(X^{(1)} = 1\\), \\(X^{(2)} = 2\\). Use the decision tree above to help you.\n\n\n\n\n\n\nClick for choices\n\n\n\n\n\n\n\\(0\\)\n\\(1.5\\)\n\\(\\approx 1.89\\)\n\\(\\approx 2.17\\)\nNone of the above\n\n\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nWe have:\n\n\n\n\\(m\\)\n\\(X^{(m)}\\)\n\\(W^{(m)}\\)\n\\(W^{(m)}G^{(m)}\\)\n\n\n\n\n1\n1\n\\((1/2)^3\\)\n\\((1/2)^3\\)\n\n\n2\n2\n1\n2\n\n\n\nHence:\n\\[\\hat G_M = \\frac{(1/2)^3 + 2}{(1/2)^3 + 1} \\approx 1.89.\\]",
    "crumbs": [
      "A first look at PPLs",
      "Importance sampling"
    ]
  },
  {
    "objectID": "w03_ppl/topic02_snis.html#weight-simplification",
    "href": "w03_ppl/topic02_snis.html#weight-simplification",
    "title": "Importance sampling",
    "section": "Weight simplification",
    "text": "Weight simplification\n\nRecall from chain rule: \\(\\gamma(x) = p(x, y) = \\rho(x) L(y | x)\\) where \\(\\rho\\) is the prior and \\(L\\) the likelihood.\nHence, since the proposal is the prior \\(q(x) = p(x)\\), the weight calculation simplifies to \\[w(x) = \\frac{\\gamma(x)}{q(x)} = \\frac{\\rho(x) L(y | x)}{\\rho(x)} = L(y | x).\\]",
    "crumbs": [
      "A first look at PPLs",
      "Importance sampling"
    ]
  },
  {
    "objectID": "w03_ppl/topic02_snis.html#theoretical-guarantees-of-snis",
    "href": "w03_ppl/topic02_snis.html#theoretical-guarantees-of-snis",
    "title": "Importance sampling",
    "section": "Theoretical guarantees of SNIS",
    "text": "Theoretical guarantees of SNIS\n\nWe have the same type of result as we encountered in Simple Monte Carlo\nNamely: for any approximation error tolerance, we can find a number of iterations \\(M\\) large enough such that we will be within that error tolerance with high probability after \\(M\\) iterations.\nName for the above property: consistency.\n\nProposition: if \\(\\mathbb{E}_\\pi|g(X)| &lt; \\infty\\), then1 \\[\\hat G_M \\to \\mathbb{E}_\\pi[g(X)],\\] as \\(M\\) goes to \\(\\infty\\).",
    "crumbs": [
      "A first look at PPLs",
      "Importance sampling"
    ]
  },
  {
    "objectID": "w03_ppl/topic02_snis.html#further-readings",
    "href": "w03_ppl/topic02_snis.html#further-readings",
    "title": "Importance sampling",
    "section": "Further readings",
    "text": "Further readings\n\nArt Owen’s notes, available form the author’s website\nChopin and Papaspiliopoulos, Chapter 8 [Available online via UBC Library]",
    "crumbs": [
      "A first look at PPLs",
      "Importance sampling"
    ]
  },
  {
    "objectID": "w03_ppl/topic02_snis.html#footnotes",
    "href": "w03_ppl/topic02_snis.html#footnotes",
    "title": "Importance sampling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHere “\\(\\to\\)” can be taken to be “convergence in probability”, or this can be strengthen to “convergence almost sure.”↩︎",
    "crumbs": [
      "A first look at PPLs",
      "Importance sampling"
    ]
  },
  {
    "objectID": "w03_ppl/topic05_mc_rates.html",
    "href": "w03_ppl/topic05_mc_rates.html",
    "title": "Monte Carlo convergence rate",
    "section": "",
    "text": "Convergence rate of Monte Carlo methods.\nEmpirical scaling laws using physicist’s log-log plot trick.\nMathematical underpinnings.\n\n\n\n\nWhen using Monte Carlo methods, you need to specify the number of iterations (also known as number of samples).\nHow to set the number of iterations?\nWe cover here a heuristic, forming the foundation for more principled methods.",
    "crumbs": [
      "A first look at PPLs",
      "Monte Carlo convergence rate"
    ]
  },
  {
    "objectID": "w03_ppl/topic05_mc_rates.html#outline",
    "href": "w03_ppl/topic05_mc_rates.html#outline",
    "title": "Monte Carlo convergence rate",
    "section": "",
    "text": "Convergence rate of Monte Carlo methods.\nEmpirical scaling laws using physicist’s log-log plot trick.\nMathematical underpinnings.\n\n\n\n\nWhen using Monte Carlo methods, you need to specify the number of iterations (also known as number of samples).\nHow to set the number of iterations?\nWe cover here a heuristic, forming the foundation for more principled methods.",
    "crumbs": [
      "A first look at PPLs",
      "Monte Carlo convergence rate"
    ]
  },
  {
    "objectID": "w03_ppl/topic05_mc_rates.html#setup",
    "href": "w03_ppl/topic05_mc_rates.html#setup",
    "title": "Monte Carlo convergence rate",
    "section": "Setup",
    "text": "Setup\nImportant: we go back to simple Monte Carlo here, to make the argument simpler. However our findings will apply equally to SNIS (and later, to MCMC).",
    "crumbs": [
      "A first look at PPLs",
      "Monte Carlo convergence rate"
    ]
  },
  {
    "objectID": "w03_ppl/topic05_mc_rates.html#motivating-example",
    "href": "w03_ppl/topic05_mc_rates.html#motivating-example",
    "title": "Monte Carlo convergence rate",
    "section": "Motivating example",
    "text": "Motivating example\nLet us revisit Q.1.3 in the first exercise. We will use it to explore tricks to set the number of Monte Carlo iterations.\nSetup: coin bag with a single flip (where we write \\(Y = Y_1\\))\n\n\n\n\\[\n\\begin{align*}\nX &\\sim {\\mathrm{Unif}}\\{0, 1, 2\\} \\\\\nY | X &\\sim {\\mathrm{Bern}}(X/2)\n\\end{align*}\n\\tag{1}\\]\nPlan: We will use our forward simulator and the law of large numbers to approximate \\(\\mathbb{E}[(1 + Y)^X]\\).\nRecall from the exercise 1 solutions (simplified a bit here):\n\ntruth = 1/3 * (1 + 1/2 + 1 + 4)\ntruth\n\n[1] 2.166667\n\nset.seed(1)\nsuppressMessages(require(extraDistr))\n\nforward_sample = function() {\n  x = rdunif(1, min=0, max=2)\n  y = rbern(1, x/2)\n  return(c(x, y))\n}\n\nsimple_monte_carlo = function(n_iterations) {\n  sum = 0.0\n  for (iteration in 1:n_iterations) {\n    sample = forward_sample()\n    sum = sum + (1+sample[2])^sample[1]\n  }\n  return(sum/n_iterations)\n}\n\nLet’s run the simulator with 10 iterations:\n\nsimple_monte_carlo(10)\n\n[1] 2.3\n\n\nIs this reliable? Let’s run it two more times:\n\nsimple_monte_carlo(10)\n\n[1] 2.7\n\nsimple_monte_carlo(10)\n\n[1] 2.1\n\n\n\nOK.. the first digit seems “stabilized” but not the second digit.\nSuppose I want one more digit of accuracy…\n\nBy how much should I increase the number of iteration to get one more digit of accuracy?\n\n\n\n\n\n\nClick for choices\n\n\n\n\n\n\n10 times more iterations\n100 times more iterations\n1000 times more iterations\nthere is no way to answer this questions\nNone of the above",
    "crumbs": [
      "A first look at PPLs",
      "Monte Carlo convergence rate"
    ]
  },
  {
    "objectID": "w03_ppl/topic05_mc_rates.html#empirical-scaling",
    "href": "w03_ppl/topic05_mc_rates.html#empirical-scaling",
    "title": "Monte Carlo convergence rate",
    "section": "Empirical scaling",
    "text": "Empirical scaling\nContinuing on the same example (where we know the truth!), we will now:\n\nvary the number of iterations (\\(10^1, 10^{1.5}, 10^2, 10^{2.5}, 10^3\\)),\n\nfor each number of iteration n_iterations, we run simple_monte_carlo(n_iterations) 500 times,\nand plot the errors in log-log scale.\n\n\nFirst, a function to compute the approximation error of one call to simple_monte_carlo(n_iterations):\n\napproximate_error = function(n_iterations) {\n  mc = simple_monte_carlo(n_iterations)\n  error = abs(mc - truth)\n  return(error)\n}\n\nSecond, running approximate_error on the different numbers of iterations, each 500 times:\n\ndf &lt;- data.frame(\"n_iterations\" = rep(c(10, 32, 100, 316, 1000), each=500))\ndf$errors &lt;- sapply(df$n_iterations, approximate_error)\n\nFinally, plotting the errors in log-log scale (each of the \\(500 \\cdot 5\\) points is the error of one Monte Carlo run):\n\nrequire(ggplot2)\n\nLoading required package: ggplot2\n\nggplot(data=df, aes(x=n_iterations, y=errors)) +\n  stat_summary(fun = mean, geom=\"line\") + # Line averages over 1000 replicates\n  scale_x_log10() +  # Show result in log-log scale\n  scale_y_log10() +\n  theme_minimal() +\n  geom_point()\n\n\n\n\n\n\n\n\n\nGood news: error goes to zero\n\nRecall this property is known as consistency.\nThis confirms the theory covered on the previous page.\nBut here we are interested in the rate (how fast does it go to zero?)\n\nResult suggests a linear fit in the log-log scale \\(\\underbrace{\\log_{10}(\\text{error})}_{y} = a\\; \\underbrace{\\log_{10}(\\text{number of iterations})}_{x} + b\\)\nQuestions:\n\nEyeball the coefficient \\(a = \\Delta y / \\Delta x\\).\nWhat can you this coefficient tell you about the scaling of the error?\n\n\n\n\n\n\n\n\nClick for choices\n\n\n\n\n\n\n\\(2\\)\n\\(1/2\\)\n\\(1\\)\n\\(-1/2\\)\n\\(-2\\)\n\n\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nGet \\(a \\approx -1/2\\)\nHence taking power on each side of\n\n\\(\\underbrace{\\log_{10}(\\text{error})}_{y} = a \\underbrace{\\log_{10}(\\text{number of iterations})}_{x} + b\\)\n\nwe get that the error will scale like \\[\\frac{\\text{constant}}{\\sqrt{\\text{number of iterations}}}\\]\n\n\n\n\nBased on this extra information, let’s try revisit our initial question: By how much should I increase the number of iteration to get one more digit of accuracy?\n\n\n\n\n\n\nClick for choices\n\n\n\n\n\n\n10 times more iterations\n100 times more iterations\n1000 times more iterations\nthere is no way to answer this questions\nNone of the above\n\n\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nsince we now know that the error will scale like \\[\\frac{\\text{constant}}{\\sqrt{\\text{number of iterations}}}\\]\n… if we want to reduce it by factor 10 (i.e. “gain one significant digit”), we need 100x more iterations\nTo gain 2 more significant digits: need 10,000x more iterations, etc\nWhat we did here is an example of an (empirical) asymptotic analysis.",
    "crumbs": [
      "A first look at PPLs",
      "Monte Carlo convergence rate"
    ]
  },
  {
    "objectID": "w03_ppl/topic05_mc_rates.html#mathematical-underpinnings",
    "href": "w03_ppl/topic05_mc_rates.html#mathematical-underpinnings",
    "title": "Monte Carlo convergence rate",
    "section": "Mathematical underpinnings",
    "text": "Mathematical underpinnings\nNotation: recall \\(\\hat G_M\\) is the estimator. Let us denote the truth by \\(g^* = \\mathbb{E}[g(X, Y)]\\).\nCore of the argument: use that for independent random variables \\(V_1, V_2\\), \\(\\operatorname{Var}[V_1 + V_2] = \\operatorname{Var}[V_1] + \\operatorname{Var}[V_2]\\)! Also, \\(\\operatorname{Var}[a V] = a^2 \\operatorname{Var}[V]\\). This gives us:\n\\[\\operatorname{SD}(\\hat G_M) = \\sqrt{\\operatorname{Var}\\frac{1}{M} \\sum_{i=1}^M G^{(m)}} = \\sqrt{\\frac{M \\operatorname{Var}G^{(1)}}{M^2}} = \\frac{\\text{constant}}{\\sqrt{M}}\\]\nIn the following, I will explain why analyzing the standard deviation makes sense…\nSurrogate error measure: Mathematically analyzing the error as we define in our code, \\(\\mathbb{E}|\\hat G_M - g^*|\\), is tricky; it is easier to look instead at the Root Mean Squared Error (RMSE): \\[\\operatorname{RMSE}= \\sqrt{\\operatorname{MSE}} = \\sqrt{ \\mathbb{E}[ (\\hat G_M - g^*)^2 ]}.\\] Sanity check: Note that the units are OK, i.e. the error measured in RMSE and with the more intuitive \\(\\mathbb{E}|\\hat G_M - g^*|\\) has the same units, e.g. meters, or grams or whatever, as the estimator \\(\\hat G_M\\) and truth \\(g^*\\).\nIt’s enough to study the standard deviation:\n\nRecall that the MSE is the sum of variance and bias squared (see wikipedia for proof) \\[\\begin{align*}\n  \\operatorname{MSE}&= \\operatorname{Var}[\\hat G_M] + (\\operatorname{Bias}(\\hat G_M, g^*))^2 \\\\\n  \\operatorname{Bias}(\\hat G_M, g^*) &= \\mathbb{E}[\\hat G_M] - g^*.\n  \\end{align*}\\]\nFor simple Monte Carlo, the bias is zero by linearity of expectation:1 \\[\\mathbb{E}[\\hat G_M] = \\mathbb{E}\\left[\\frac{1}{M} \\sum_{m=1}^M G^{(m)}\\right] = \\frac{1}{M} \\sum_{m=1}^M \\mathbb{E}[G^{(m)}] = \\mathbb{E}[G^{(m)}] = g^*.\\]\n\n\nThe bias of zero gives a simpler expression for RMSE: \\[\\operatorname{RMSE}= \\sqrt{\\operatorname{Var}[\\hat G_M] + 0} = \\operatorname{SD}[\\hat G_M]\\]\nHence for simple Monte Carlo, analyzing the scaling of the standard deviation (SD) is the same as analyzing the RMSE.",
    "crumbs": [
      "A first look at PPLs",
      "Monte Carlo convergence rate"
    ]
  },
  {
    "objectID": "w03_ppl/topic05_mc_rates.html#contextualizing-the-error-rate-of-monte-carlo",
    "href": "w03_ppl/topic05_mc_rates.html#contextualizing-the-error-rate-of-monte-carlo",
    "title": "Monte Carlo convergence rate",
    "section": "Contextualizing the error rate of Monte Carlo",
    "text": "Contextualizing the error rate of Monte Carlo\n\nNumerical methods such as the trapezoidal rule converge much faster in terms of \\(M\\): \\[\\text{error} = \\frac{\\text{constant}}{M^2},\\] i.e. 10 times more iterations gives two digits of extra accuracy (here \\(M\\) is the number of grid points used in a 1d numerical integral)!\nSo why do we use Monte Carlo?\n\nThe constants in the analysis of numerical integration blow up exponentially in the dimensionality of the problem!\nMany Monte Carlo methods can avoid this exponential blow up in the dimensionality.2\nAnd good scalability in the dimensionality of the problem often more important than scalability in number of digits of accuracy\n\n…can’t trust 10th digit anyways because the model almost always has some amount of mis-specification.",
    "crumbs": [
      "A first look at PPLs",
      "Monte Carlo convergence rate"
    ]
  },
  {
    "objectID": "w03_ppl/topic05_mc_rates.html#footnotes",
    "href": "w03_ppl/topic05_mc_rates.html#footnotes",
    "title": "Monte Carlo convergence rate",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor SNIS, the bias is not zero (because we have a ratio), but the squared bias decays faster than the variance term as \\(M \\to \\infty\\) so the argument is essentially the same as simple Monte Carlo.↩︎\nIn particular, Simple Monte Carlo and MCMC can often avoid the curse of dimensionality. But not SNIS! Why are we spending time on SNIS then? Because it can be used to approximate arbitrary posterior distribution, while simple Monte Carlo cannot, and it is much simpler than MCMC, so a good starting point pedagogically. However we will jump to MCMC later in this course.↩︎",
    "crumbs": [
      "A first look at PPLs",
      "Monte Carlo convergence rate"
    ]
  },
  {
    "objectID": "w03_ppl/topic04_consistency.html",
    "href": "w03_ppl/topic04_consistency.html",
    "title": "SNIS consistency",
    "section": "",
    "text": "Recap of SNIS’ consistency guarantee.\nProof of consistency.\n\n\n\n\nWe go over this proof as it demystifies the form of SNIS’ weights.",
    "crumbs": [
      "A first look at PPLs",
      "SNIS consistency"
    ]
  },
  {
    "objectID": "w03_ppl/topic04_consistency.html#outline",
    "href": "w03_ppl/topic04_consistency.html#outline",
    "title": "SNIS consistency",
    "section": "",
    "text": "Recap of SNIS’ consistency guarantee.\nProof of consistency.\n\n\n\n\nWe go over this proof as it demystifies the form of SNIS’ weights.",
    "crumbs": [
      "A first look at PPLs",
      "SNIS consistency"
    ]
  },
  {
    "objectID": "w03_ppl/topic04_consistency.html#notation-and-setup",
    "href": "w03_ppl/topic04_consistency.html#notation-and-setup",
    "title": "SNIS consistency",
    "section": "Notation and setup",
    "text": "Notation and setup\nSee page on SNIS.",
    "crumbs": [
      "A first look at PPLs",
      "SNIS consistency"
    ]
  },
  {
    "objectID": "w03_ppl/topic04_consistency.html#consistency",
    "href": "w03_ppl/topic04_consistency.html#consistency",
    "title": "SNIS consistency",
    "section": "Consistency",
    "text": "Consistency\nProposition: if \\(\\mathbb{E}_\\pi|g(X)| &lt; \\infty\\), then1 \\[\\hat G_M \\to \\mathbb{E}_\\pi[g(X)],\\] as \\(M\\) goes to \\(\\infty\\).\nProof: first, divide both numerator and denominator by \\({\\color{red} M}\\): \\[\\begin{align*}\n\\hat G_M &= \\frac{\\sum_{m=1}^M W^{(m)}G^{(m)}}{\\sum_{m=1}^M W^{(m)}} \\\\\n&= \\frac{{\\color{red} \\frac{1}{M}} \\sum_{m=1}^M W^{(m)}G^{(m)}}{{\\color{red} \\frac{1}{M}} \\sum_{m=1}^M W^{(m)}}.\n\\end{align*}\\]\nWe will analyze the numerator and denominator separately. Let’s start with the numerator.\nQuestion: use the Law of large number to find the limit: \\[\\frac{1}{M} \\sum_{m=1}^M W^{(m)}G^{(m)}\\to\\; ?\\]\n\n\n\n\n\n\nClick for choices\n\n\n\n\n\n\n\\(\\mathbb{E}_q[W^{(1)} G^{(1)}]\\)\n\\(\\mathbb{E}_\\pi[W^{(1)} G^{(1)}]\\)\n\\(\\mathbb{E}_q[G^{(1)}]\\)\n\\(\\mathbb{E}_\\pi[G^{(1)}]\\)\nNone of the above\n\n\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nRecall the LLN: for \\(Z_i\\) with \\(\\mathbb{E}|Z_1| &lt; \\infty\\), \\[ \\frac{1}{M} \\sum_{m=1}^M Z_m \\to \\mathbb{E}[Z_1].\\]\nHere taking \\(Z_m = W^{(m)}G^{(m)}\\) gives: \\[\\frac{1}{M} \\sum_{m=1}^M W^{(m)}G^{(m)}\\to \\mathbb{E}_q[W^{(1)} G^{(1)}],\\] where the subscript \\(q\\) denotes that the random variables \\(X^{(m)}\\sim q\\) in SNIS.\n\n\n\nNow we can simplify the above limit:\n\\[\\begin{align*}\n\\mathbb{E}_q[W^{(1)} G^{(1)}] &= \\int ( w(x) g(x) ) q(x) \\mathrm{d}x \\;\\;\\text{(by LOTUS)} \\\\\n&= \\int \\left( \\frac{\\gamma(x)}{{\\color{red} q(x)}} g(x) \\right) {\\color{red} q(x)} \\mathrm{d}x \\;\\;\\text{(definition of $w$)} \\\\\n&= \\int \\gamma(x) g(x)  \\mathrm{d}x.\n\\end{align*}\\]\nNow the denominator is just a special case where \\(g(x) = 1\\), hence by the same argument we just did: \\[\\frac{1}{M} \\sum_{m=1}^M W^{(m)}\\to \\int \\gamma(x) \\mathrm{d}x = Z.\\]\nNow to combine the convergence of numerator and denominator in one, we use this proposition from probability theory:\nProposition:2 if \\(S_i \\to S\\) and \\(T_i \\to T\\) then \\(S_i / T_i \\to S / T\\).\nNow applying that proposition, we get: \\[\\begin{align*}\n\\hat G_M &= \\frac{\\frac{1}{M} \\sum_{m=1}^M W^{(m)}G^{(m)}}{\\frac{1}{M} \\sum_{m=1}^M W^{(m)}} \\\\\n&\\to \\frac{\\int \\gamma(x) g(x)  \\mathrm{d}x}{Z} \\\\\n&= \\int \\frac{\\gamma(x)}{Z} g(x) \\mathrm{d}x = \\mathbb{E}_\\pi[g(X)].\n\\end{align*}\\]",
    "crumbs": [
      "A first look at PPLs",
      "SNIS consistency"
    ]
  },
  {
    "objectID": "w03_ppl/topic04_consistency.html#footnotes",
    "href": "w03_ppl/topic04_consistency.html#footnotes",
    "title": "SNIS consistency",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs usual, “\\(\\to\\)” will be taken to be “convergence in probability”, or this can be strengthen to “convergence almost sure.”↩︎\nThis is true in probability and almost sure. In the case of almost sure the proof is trivial but outside of the scope of this course.↩︎",
    "crumbs": [
      "A first look at PPLs",
      "SNIS consistency"
    ]
  },
  {
    "objectID": "w05_properties/topic01_asymptotics.html",
    "href": "w05_properties/topic01_asymptotics.html",
    "title": "Asymptotics",
    "section": "",
    "text": "The notion of asymptotic analysis.\nThe two asymptotic regimes considered in this class.\n\n\n\n\nIn statistics, asymptotic analysis is an important tool to understand any estimator, including Bayesian estimators and Monte Carlo methods.",
    "crumbs": [
      "Some theory",
      "Asymptotics"
    ]
  },
  {
    "objectID": "w05_properties/topic01_asymptotics.html#outline",
    "href": "w05_properties/topic01_asymptotics.html#outline",
    "title": "Asymptotics",
    "section": "",
    "text": "The notion of asymptotic analysis.\nThe two asymptotic regimes considered in this class.\n\n\n\n\nIn statistics, asymptotic analysis is an important tool to understand any estimator, including Bayesian estimators and Monte Carlo methods.",
    "crumbs": [
      "Some theory",
      "Asymptotics"
    ]
  },
  {
    "objectID": "w05_properties/topic01_asymptotics.html#a-first-type-of-asymptotics-consistency-of-monte-carlo-methods",
    "href": "w05_properties/topic01_asymptotics.html#a-first-type-of-asymptotics-consistency-of-monte-carlo-methods",
    "title": "Asymptotics",
    "section": "A first type of asymptotics: consistency of Monte Carlo methods",
    "text": "A first type of asymptotics: consistency of Monte Carlo methods\nExample: suppose you are trying to estimate the slope parameter in last week’s regression model.\n\n\n\n\nTypical situation:\n\nFix one Bayesian model and one dataset.\nYou are interested in \\(g^* = \\mathbb{E}[g(X) | Y = y]\\)\n\ne.g., \\(g(\\text{slope}, \\text{sd}) = \\text{slope}\\) in last week’s regression example.\n\nYou try simPPLe with \\(M = 100\\) iterations, get a Monte Carlo approximation \\(\\hat G_{M} \\approx \\mathbb{E}[g(X) | Y = y]\\)\nThen you try \\(M = 1000\\) iterations to gauge the quality of the Monte Carlo approximation \\(\\hat G_{M}\\).\nThen you try \\(M = 10 000\\), etc.\n\nMathematical guarantee: we proved (Monte Carlo) consistency,\n\ni.e. that \\(\\hat G_{M}\\) can get arbitrarily close to \\(g^*\\).\n\n\nMonte Carlo consistency: the limit \\(M\\to\\infty\\) gives us a first example of asymptotic regime: “infinite computation.”",
    "crumbs": [
      "Some theory",
      "Asymptotics"
    ]
  },
  {
    "objectID": "w05_properties/topic01_asymptotics.html#a-second-type-of-asymptotics-big-data",
    "href": "w05_properties/topic01_asymptotics.html#a-second-type-of-asymptotics-big-data",
    "title": "Asymptotics",
    "section": "A second type of asymptotics: “big data”",
    "text": "A second type of asymptotics: “big data”\nQuestion: even after \\(M \\to \\infty\\), can there still be an error in the “infinite compute” limit \\(g^*\\)?\nYou will explore this question in this week’s exercises.",
    "crumbs": [
      "Some theory",
      "Asymptotics"
    ]
  },
  {
    "objectID": "w05_properties/topic04_decision_to_set.html",
    "href": "w05_properties/topic04_decision_to_set.html",
    "title": "Decision theoretic set estimation",
    "section": "",
    "text": "Deriving a set estimate from decision theory.\n\n\n\n\nWe have seen in week 2 some examples of set estimates (quantile based, highest density set).\nThese are actually special cases of decision theory with specific choices of loss functions.\nThis page provides a general framework to answer the question: “how to summarize a posterior distribution with one set?”",
    "crumbs": [
      "Some theory",
      "Decision theoretic set estimation"
    ]
  },
  {
    "objectID": "w05_properties/topic04_decision_to_set.html#outline",
    "href": "w05_properties/topic04_decision_to_set.html#outline",
    "title": "Decision theoretic set estimation",
    "section": "",
    "text": "Deriving a set estimate from decision theory.\n\n\n\n\nWe have seen in week 2 some examples of set estimates (quantile based, highest density set).\nThese are actually special cases of decision theory with specific choices of loss functions.\nThis page provides a general framework to answer the question: “how to summarize a posterior distribution with one set?”",
    "crumbs": [
      "Some theory",
      "Decision theoretic set estimation"
    ]
  },
  {
    "objectID": "w05_properties/topic04_decision_to_set.html#context",
    "href": "w05_properties/topic04_decision_to_set.html#context",
    "title": "Decision theoretic set estimation",
    "section": "Context",
    "text": "Context\n\nThe weakness of point estimates is that they do not capture the uncertainty around the value.\nIdea: instead of returning a single point, return a set of points\n\nusually an interval,\nbut this can be generalized\n\nBayesian terminology: credible interval (\\(\\neq\\) frequentist confidence intervals)\nGoals:\n\nWe would like the credible interval to contain a fixed fraction of the posterior mass (e.g. 95%)\nAt the same time, we would like this credible interval to be as short as possible given that posterior mass constraint",
    "crumbs": [
      "Some theory",
      "Decision theoretic set estimation"
    ]
  },
  {
    "objectID": "w05_properties/topic04_decision_to_set.html#bayes-estimator-formalization",
    "href": "w05_properties/topic04_decision_to_set.html#bayes-estimator-formalization",
    "title": "Decision theoretic set estimation",
    "section": "Bayes estimator formalization",
    "text": "Bayes estimator formalization\nPick:\n\n\\(A = \\{[c, d] : c &lt; d\\}\\),\nconsider the loss function given by \\[\nL([c, d], x) = \\mathbb{1}\\{x \\notin [c, d]\\} + k (d - c)\n\\] for some tuning parameter \\(k\\) to be determined later.\n\nWe get:\n\\[\n\\begin{aligned}\n\\delta_{\\text{B}}(Y) &= \\operatorname{arg\\,min}\\{ \\mathbb{E}[L(a, X) | Y] : a \\in A \\} \\\\\n&=  \\operatorname{arg\\,min}\\{ \\Pr[X \\notin [c, d] | Y] + k(d - c) : [c,d] \\in A \\} \\\\\n&=  \\operatorname{arg\\,min}\\{ \\Pr[X &lt; c|Y] + \\Pr[X &gt; d |Y] + k(d - c) : [c,d] \\in A \\}  \\\\\n&=  \\operatorname{arg\\,min}\\{ \\Pr[X \\le c|Y] - \\Pr[X \\le d |Y] + k(d - c) : [c,d] \\in A \\}\n\\end{aligned}\n\\]\nHere we assume the posterior has a continuous density \\(f\\) to change \\(&lt;\\) into \\(\\le\\).\nAs we did with point estimation, we take the derivative with respect to \\(c\\) and set to zero (then will do the same thing for \\(d\\)).\nNotice that \\(\\Pr[X \\le c|Y]\\) is the posterior cumulative distribution function (CDF)1, so taking the derivative with respect to \\(c\\) yields a density:2\n\\[\nf_{X|Y}(c) - k = 0,\n\\]\n\n\n\nso we see the optimum will be the smallest interval \\([c, d]\\) such that \\(f(c) = f(d) = k\\).\nFinally, set \\(k\\) to capture say 95% of the mass (area in red in the figure shows a 50% example, we would lower \\(k\\) to get to 95%).",
    "crumbs": [
      "Some theory",
      "Decision theoretic set estimation"
    ]
  },
  {
    "objectID": "w05_properties/topic04_decision_to_set.html#footnotes",
    "href": "w05_properties/topic04_decision_to_set.html#footnotes",
    "title": "Decision theoretic set estimation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRecall that a CDF is defined as \\(F(x) = \\mathbb{P}(X \\le x)\\). So a posterior CDF is just \\(\\mathbb{P}(X \\le x | Y)\\).↩︎\nLet us review the argument why “taking the derivative … yields a density”. Let \\(f\\) denote a density and \\(F\\), the CDF of the same random variable. From the definition of a density, \\(F(x) = \\int_{-\\infty}^x f(x') \\mathrm{d}x'\\) (i.e., take \\(A = [-\\infty, x]\\) in the definition of a density). Hence by the Fundamental theorem of calculus (i.e. the fact that an integral is an “anti-derivative”), we have that \\[f(x) = \\frac{\\mathrm{d}F(x)}{\\mathrm{d}x},\\] when the derivative exists.↩︎",
    "crumbs": [
      "Some theory",
      "Decision theoretic set estimation"
    ]
  },
  {
    "objectID": "w05_properties/topic05_calibration.html",
    "href": "w05_properties/topic05_calibration.html",
    "title": "Calibration",
    "section": "",
    "text": "Notion of calibration.\nNominal versus actual coverage.\n\n\n\n\nReturning a credible region is helpful as it conveys how much certain or uncertain we are about our answer.\nHowever, these estimates of uncertainty can be misleading if they are too confident or too hesitant. The notion of calibration formalizes this intuition.",
    "crumbs": [
      "Some theory",
      "Calibration"
    ]
  },
  {
    "objectID": "w05_properties/topic05_calibration.html#outline",
    "href": "w05_properties/topic05_calibration.html#outline",
    "title": "Calibration",
    "section": "",
    "text": "Notion of calibration.\nNominal versus actual coverage.\n\n\n\n\nReturning a credible region is helpful as it conveys how much certain or uncertain we are about our answer.\nHowever, these estimates of uncertainty can be misleading if they are too confident or too hesitant. The notion of calibration formalizes this intuition.",
    "crumbs": [
      "Some theory",
      "Calibration"
    ]
  },
  {
    "objectID": "w05_properties/topic05_calibration.html#example",
    "href": "w05_properties/topic05_calibration.html#example",
    "title": "Calibration",
    "section": "Example",
    "text": "Example\n\nFamiliar in Vancouver: “Tomorrow: 90% chance of rain,”\n\nhere, “90%” is an example of nominal coverage.\n\nOf those days where the forecast says “Tomorrow: 90% chance of rain”…\n\nwhat fraction of those days did it actually rain?\nThat fraction is an example of actual coverage.\n\n\nDefinition: a measure of uncertainty is calibrated if its nominal coverage matches its actual coverage.\n\nQuestion: which weather source(s), if any, provide reasonably calibrated rain uncertainty estimates?",
    "crumbs": [
      "Some theory",
      "Calibration"
    ]
  },
  {
    "objectID": "w05_properties/topic07_calibration_mis_specified.html",
    "href": "w05_properties/topic07_calibration_mis_specified.html",
    "title": "Bayesian calibration: mis-specified case",
    "section": "",
    "text": "Guarantees when the model is mis-specified.\nThe Bernstein-von Mises theorem.\n\n\n\n\nModel mis-specification is the norm. What happen to calibration in that case?",
    "crumbs": [
      "Some theory",
      "Bayesian calibration: mis-specified case"
    ]
  },
  {
    "objectID": "w05_properties/topic07_calibration_mis_specified.html#outline",
    "href": "w05_properties/topic07_calibration_mis_specified.html#outline",
    "title": "Bayesian calibration: mis-specified case",
    "section": "",
    "text": "Guarantees when the model is mis-specified.\nThe Bernstein-von Mises theorem.\n\n\n\n\nModel mis-specification is the norm. What happen to calibration in that case?",
    "crumbs": [
      "Some theory",
      "Bayesian calibration: mis-specified case"
    ]
  },
  {
    "objectID": "w05_properties/topic07_calibration_mis_specified.html#misspecified-models-and-calibration",
    "href": "w05_properties/topic07_calibration_mis_specified.html#misspecified-models-and-calibration",
    "title": "Bayesian calibration: mis-specified case",
    "section": "Misspecified models and calibration",
    "text": "Misspecified models and calibration\nWhat to do if your model is misspecified?\nOne obvious possibility is to make the model better. But in the following we show that under certain conditions, another possibility is to collect more data.\n\nNow suppose we are using the “wrong prior”, i.e. data generation uses uniform prior but we base or posterior computation on a different, non-uniform prior.\nSimilarly to the last page, let’s do it for small (10 launches), medium (100), and large datasets (1000), plotting the nominal coverage (dashed) against the actual coverage (solid line)\n\n\n\nCode\nsuppressPackageStartupMessages(require(\"ggplot2\"))\nsuppressPackageStartupMessages(require(\"dplyr\"))\nsuppressPackageStartupMessages(require(\"tidyr\"))\ntheme_set(theme_bw())\n\n # Using now the same non-uniform prior as before for posterior calculation\nK &lt;- 1000\n\nrdunif &lt;- function(max) { return(ceiling(max*runif(1))) }\n\nposterior_distribution &lt;- function(prior_probs, n_successes, n_trials){         \n  K &lt;- length(prior_probs)-1 # K+1 values that your p can assume\n  n_fails &lt;- n_trials - n_successes\n  p &lt;- seq(0, 1, 1/K)\n  posterior_probs &lt;-                                       # 1. this computes gamma(i)\n    prior_probs *                                          #    - prior\n    p^n_successes * (1-p)^n_fails                          #    - likelihood \n  posterior_probs &lt;- posterior_probs/sum(posterior_probs)  # 2. normalize gamma(i)\n  post_prob &lt;- rbind(p, posterior_probs)\n  return(post_prob)\n}\n\nhigh_density_intervals &lt;- function(alpha, posterior_probs){\n  ordered_probs = posterior_probs[,order(posterior_probs[2,], decreasing = TRUE)]\n  cumulative_probs = cumsum(ordered_probs[2,])\n  index = which.max(cumulative_probs &gt;= (1-alpha))\n  return(ordered_probs[,1:index, drop=FALSE])\n}\n\nhdi_coverage_pr &lt;- function(n_datapoints) {\n  n_inclusions &lt;- 0\n  for (repetition in seq(1:n_repeats)) {\n    i &lt;- rdunif(K + 1) - 1  # Always generate the data using a uniform prior\n    true_p &lt;- i/K\n    x &lt;- rbinom(1, n_datapoints, true_p)\n    post &lt;- posterior_distribution(prior_used_for_computing_posterior, x, n_datapoints)\n    \n    # This if is just a hacky way to check if true parameter is in the HDI credible interval\n    if (sum(abs(true_p - high_density_intervals(alpha, post)[1,]) &lt; 10e-10) == 1) {\n      n_inclusions &lt;- n_inclusions + 1\n    }\n  }\n  return(n_inclusions/n_repeats) # Fraction of simulation where the true parameter was in interval\n}\n\nprior_used_for_computing_posterior &lt;- dnorm(seq(0, 1, 1/K),mean = 0.2, sd=0.2)\nprior_used_for_computing_posterior &lt;- prior_used_for_computing_posterior / sum(prior_used_for_computing_posterior)\n\nset.seed(1)\n\nn_repeats &lt;- 1000\nalpha &lt;- 0.1\n\ndf &lt;- data.frame(\"n_observations\" = c(10, 100, 1000))\ndf$coverage_pr &lt;- sapply(df$n_observations, hdi_coverage_pr)\n\nggplot(data=df, aes(x=n_observations, y=coverage_pr)) +\n  ylim(0, 1) +\n  xlab(\"Number of observations\") + \n  ylab(\"Actual coverage\") +\n  geom_hline(yintercept=1-alpha, linetype=\"dashed\", color = \"black\") +\n  geom_line()\n\n\n\n\n\n\n\n\n\n\nBad news: for small datasets we are no longer calibrated, in the worst possible way\n\nHigher than dash line: would have meant inference is being conservative, i.e. more right than it actually claimed. That’s not too bad.\nLower than dash line: we are being overconfident or anti-conservative, which in some case can be reckless\n\nGood news: this gets quickly corrected as dataset gets larger. Why?",
    "crumbs": [
      "Some theory",
      "Bayesian calibration: mis-specified case"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic08_debug.html",
    "href": "w12_mcmc2/topic08_debug.html",
    "title": "MCMC debugging",
    "section": "",
    "text": "Exact invariance test\n\n\n\n\nIt is easy for bugs to sneak in MCMC code. In the workflow week, we have briefly outlined how simulated data can be used to detect software defects. However, the test discussed there still left “MCMC too slow” as one possible issue confounding software defect.\nHere we describe a new test (the exact invariance test) that excludes “MCMC too slow” from the possible causes of warning. When the exact invariance test rises a warning, the only possible causes are “software defect” and “bad luck.”",
    "crumbs": [
      "MCMC Hacking",
      "MCMC debugging"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic08_debug.html#outline",
    "href": "w12_mcmc2/topic08_debug.html#outline",
    "title": "MCMC debugging",
    "section": "",
    "text": "Exact invariance test\n\n\n\n\nIt is easy for bugs to sneak in MCMC code. In the workflow week, we have briefly outlined how simulated data can be used to detect software defects. However, the test discussed there still left “MCMC too slow” as one possible issue confounding software defect.\nHere we describe a new test (the exact invariance test) that excludes “MCMC too slow” from the possible causes of warning. When the exact invariance test rises a warning, the only possible causes are “software defect” and “bad luck.”",
    "crumbs": [
      "MCMC Hacking",
      "MCMC debugging"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic08_debug.html#example",
    "href": "w12_mcmc2/topic08_debug.html#example",
    "title": "MCMC debugging",
    "section": "Example",
    "text": "Example\nHere is an implementation of the joint distribution of a beta-binomial model with a bug planted!\n\nset.seed(1)\n# prior: Beta(alpha, beta)\nalpha = 1\nbeta = 2 \nn_trials = 3\n\nbuggy_joint = function(x, y) {\n  if (x &lt; 0 || x &gt; 1) return(0.0)\n  dbeta(x, alpha, beta) * dbinom(y, size = n_trials, prob = x, log = TRUE)\n}\n\nCan you spot the bug?\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nIt mixes up log-scale and non-log-scale computations, a common mistake. Concretely, the log = TRUE argument should be removed.\n\n\n\nCan we automatically detect there is a bug?",
    "crumbs": [
      "MCMC Hacking",
      "MCMC debugging"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic08_debug.html#exact-invariance-test",
    "href": "w12_mcmc2/topic08_debug.html#exact-invariance-test",
    "title": "MCMC debugging",
    "section": "Exact invariance test",
    "text": "Exact invariance test\n\nBuilding block: forward simulation-initialized MCMC\nThe first ingredient we need is a forward simulator:\n\nforward = function() {\n  x = rbeta(1, alpha, beta) \n  y = rbinom(1, n_trials, x)\n  return(\n    list(\n      x = x,\n      y = y\n    ))\n}\n\nNext, let’s do the following:\n\nCall our forward simulator. \\[(x, y) \\sim \\text{forward()}. \\]\nRun an MCMC algorithm initialized at the \\((x, y)\\) we just forward simulated.\n\n\nstationary_mcmc = function(joint, n_iterations) {\n1  initialization = forward()\n  y = initialization$y\n  if (n_iterations == 0) { \n    return(initialization$x)\n  } else {\n2    current_x = initialization$x\n3    for (i in 1:n_iterations) {\n      proposed_x = current_x + rnorm(1) \n      ratio = joint(proposed_x, y) / joint(current_x, y) \n      if (runif(1) &lt; ratio) {\n        current_x = proposed_x\n      }\n    }\n    return(current_x)\n  }\n}\n\n\n1\n\nCall our forward simulator.\n\n2\n\nInitialize the MCMC chain.\n\n3\n\nPerform n_iterations rounds of MCMC conditioning on the y we simulated.\n\n\n\n\n\nTypically, we do MCMC on \\(x\\) only…\n\n…but here, view the MCMC as a chain on pairs \\((x, y)\\).\n\nTypically, MCMC targets the conditional, \\(\\pi(x) = p(x | y)\\)…\n\n…but here, view it as targeting the joint, \\(\\pi(x, y) = p(x, y)\\).\n\nWhat is the initial distribution, \\(\\mu_1\\)?\n\nBy construction, we initialize with forward(), so, \\[\\mu_1(x, y) = \\pi(x, y).\\]\n\n\nQuestion: if the code is correct, what is the distribution \\(\\mu_2\\) of the chain after one iteration, i.e., the distribution of \\((X^{(2)}, Y^{(2)})\\)?\n\n\nRepeat 2000 times\n\nWe now repeat what we just did 1000 times.\nWe also do forward simulation alone, 1000 times (by setting n_iterations to zero).\n\n\nexact_invariance = function(joint) {\n  forward_only = replicate(1000, stationary_mcmc(joint, 0))\n  with_mcmc    = replicate(1000, stationary_mcmc(joint, 200))\n\n  ks.test(forward_only, with_mcmc)\n}\n\n\n\nFrequentist test of distributional equality\n\nWe now have 2 sets of samples, each i.i.d.: forward_only and with_mcmc.\nKey property: if they have different distributions, we can conclude that the code is not \\(\\pi\\)-invariant, i.e., buggy.\nHow to check this?\n\nLet’s use frequentist tools!\n\nHere ks.test, the Kolmogorov-Smirnov test, but a 2-samples, unpaired t.test would have worked too.\n\nOne of the rare situations where a “point-null hypothesis” makes perfect sense.\n\ni.e., exact equality of the distribution is precisely what we would expect if the code is correct.\n\n\n\n\ntest_result = exact_invariance(buggy_joint)\ntest_result\n\n\n    Asymptotic two-sample Kolmogorov-Smirnov test\n\ndata:  forward_only and with_mcmc\nD = 0.099, p-value = 0.0001108\nalternative hypothesis: two-sided\n\n\n\nYou do not need to know the details about this test…\n…rather, focus on its interpretation:\n\nwe observed a certain “discrepancy” between the empirical CDFs of forward_only and with_mcmc, here 0.099\nif the code was correct, we would see a discrepancy larger or equal to that only with probability 0.0001107924\nthat last probability is the infamous “p-value” which is often over-used, but a good tool in this present situation.\n\n\nSuccess! The p-value is tiny, we would indeed have caught that bug.\n\n\nChecking that correct code passes the test\nLet’s look at the fixed code:\n\nfixed_joint = function(x, y) {\n  if (x &lt; 0 || x &gt; 1) return(0.0)\n  dbeta(x, alpha, beta) * dbinom(y, size = n_trials, prob = x)\n}\n\nexact_invariance(fixed_joint)\n\n\n    Asymptotic two-sample Kolmogorov-Smirnov test\n\ndata:  forward_only and with_mcmc\nD = 0.027, p-value = 0.8593\nalternative hypothesis: two-sided\n\n\nAgain, success! The p-value is not tiny, as one would hope.",
    "crumbs": [
      "MCMC Hacking",
      "MCMC debugging"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic08_debug.html#references",
    "href": "w12_mcmc2/topic08_debug.html#references",
    "title": "MCMC debugging",
    "section": "References",
    "text": "References\nSee earlier page on checking correctness.",
    "crumbs": [
      "MCMC Hacking",
      "MCMC debugging"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic03_markov_lln.html",
    "href": "w12_mcmc2/topic03_markov_lln.html",
    "title": "Law of large numbers for Markov chain",
    "section": "",
    "text": "Law of Large Numbers (LLN) for Markov chains\n\\(\\pi\\)-invariance\n\n\n\n\nRecall we have seen a special case of the LLN for Markov chain (for MH), when talking about the consistency of MH.\nHere we look at LLNs for Markov chains more generally.",
    "crumbs": [
      "MCMC Hacking",
      "Law of large numbers for Markov chain"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic03_markov_lln.html#outline",
    "href": "w12_mcmc2/topic03_markov_lln.html#outline",
    "title": "Law of large numbers for Markov chain",
    "section": "",
    "text": "Law of Large Numbers (LLN) for Markov chains\n\\(\\pi\\)-invariance\n\n\n\n\nRecall we have seen a special case of the LLN for Markov chain (for MH), when talking about the consistency of MH.\nHere we look at LLNs for Markov chains more generally.",
    "crumbs": [
      "MCMC Hacking",
      "Law of large numbers for Markov chain"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic03_markov_lln.html#law-of-large-numbers-for-markov-chains",
    "href": "w12_mcmc2/topic03_markov_lln.html#law-of-large-numbers-for-markov-chains",
    "title": "Law of large numbers for Markov chain",
    "section": "Law of large numbers for Markov chains",
    "text": "Law of large numbers for Markov chains\nRecall, we mentioned a LLN for MH specifically (simplified here for finite state spaces):\nProposition: (discrete case) if the chain \\(X^{(m)}\\) produced by MH is irreducible, then we have a LLN with respect to \\(\\pi\\), i.e., \\[\\frac{1}{M} \\sum_{m=1}^M g(X^{(m)}) \\to \\mathbb{E}_\\pi[g(X)],\\] with probability one as the number of MCMC iterations \\(M\\) goes to infinity.\nToday we see the above proposition is a corollary of the following two results:\nProposition: (discrete case)\n\nMH satisfies a property called \\(\\pi\\)-invariance\n\\(\\pi\\)-invariance + irreducibility \\(\\Rightarrow\\) LLN with respect to \\(\\pi\\).",
    "crumbs": [
      "MCMC Hacking",
      "Law of large numbers for Markov chain"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic03_markov_lln.html#invariance",
    "href": "w12_mcmc2/topic03_markov_lln.html#invariance",
    "title": "Law of large numbers for Markov chain",
    "section": "Invariance",
    "text": "Invariance\nDefinition: a Markov kernel \\(K\\) is called \\(\\pi\\)-invariant when \\[X \\sim \\pi \\text{ and } X' \\sim K(\\cdot|X) \\Rightarrow X' \\sim \\pi.\\]",
    "crumbs": [
      "MCMC Hacking",
      "Law of large numbers for Markov chain"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic03_markov_lln.html#plan",
    "href": "w12_mcmc2/topic03_markov_lln.html#plan",
    "title": "Law of large numbers for Markov chain",
    "section": "Plan",
    "text": "Plan\n\nWe will prove point 1 above, i.e., that MH is \\(\\pi\\)-invariant.\nFor 2, see further readings.",
    "crumbs": [
      "MCMC Hacking",
      "Law of large numbers for Markov chain"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic10_hmc_intuition.html",
    "href": "w12_mcmc2/topic10_hmc_intuition.html",
    "title": "HMC basics",
    "section": "",
    "text": "Intuition\nConservation of energy\nAugmentation",
    "crumbs": [
      "MCMC Hacking",
      "HMC basics"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic10_hmc_intuition.html#outline",
    "href": "w12_mcmc2/topic10_hmc_intuition.html#outline",
    "title": "HMC basics",
    "section": "",
    "text": "Intuition\nConservation of energy\nAugmentation",
    "crumbs": [
      "MCMC Hacking",
      "HMC basics"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic10_hmc_intuition.html#intuition",
    "href": "w12_mcmc2/topic10_hmc_intuition.html#intuition",
    "title": "HMC basics",
    "section": "Intuition",
    "text": "Intuition\n\nLet \\(\\pi\\) denote our target density.\nTake the log and take the negative: \\(U(x) = - \\log \\pi(x)\\).\nNow imagine a marble rolling on the landscape \\(U(x)\\).\n\nPosition at time \\(t\\): \\(x^{(t)} \\in \\mathbb{R}^d\\)\nMomentum at time \\(t\\): \\(p^{(t)} \\in \\mathbb{R}^d\\)\n\nIf you are rusty with physics, for today since we use marbles of mass one, think of momentum = velocity.\n\n\nHMC at a high level:\n\nproposal: from the current position, let the ball roll for time \\(t\\) (approximately)\n\nthe position at time \\(t\\) is the proposal.\n\nPerform an MH accept/reject step based on that proposal.\nSample a new momentum.",
    "crumbs": [
      "MCMC Hacking",
      "HMC basics"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic10_hmc_intuition.html#conservation-of-energy",
    "href": "w12_mcmc2/topic10_hmc_intuition.html#conservation-of-energy",
    "title": "HMC basics",
    "section": "Conservation of energy",
    "text": "Conservation of energy\nWhy would “rolling a marble on \\(U\\)” be a good proposal?\nKey insight: ignoring friction, recall from elementary physics that we have energy conservation, \\[\\begin{align*}\n\\text{Hamiltonian}^{(t)} &:= \\text{potential energy}^{(t)} + \\text{kinetic energy}^{(t)} = \\text{constant}\n\\end{align*}\\] where: \\[\\begin{align*}\n\\text{potential energy}^{(t)} &:= U(x^{(t)}), \\\\\n\\text{kinetic energy}^{(t)} &:= K(p^{(t)}) :=  \\frac{1}{2} \\sum_{i=1}^d (p_i^{(t)})^2, \\\\\n\\text{Hamiltonian}^{(t)} &:= H(x^{(t)}, p^{(t)}) = U(x^{(t)}) + K(p^{(t)}).\n\\end{align*}\\]\nFormalizing this idea:\n\nLet \\(\\bar \\pi(x, p) \\propto \\exp(-H(x, p))\\).\nNote: the original target \\(\\pi\\) is a marginal of \\(\\bar \\pi\\): \\[\\begin{align*}\n\\int \\bar \\pi(x, p) \\mathrm{d}p &\\propto \\int \\exp(-U(x)) \\exp(-K(p)) \\mathrm{d}p \\\\\n&=  \\exp(-U(x)) \\int \\exp(-K(p)) \\mathrm{d}p \\\\\n&\\propto \\pi(x),\n\\end{align*}\\] using that \\(\\exp(-K(p))\\) is proportional to a standard normal density.\nHence if we can get samples from \\(\\bar \\pi\\) we automatically get samples from \\(\\pi\\)!\n\nJust discard the momenta \\(p\\).\n\nFinally:\n\nif we propose \\((x^{(t)}, p^{(t)})\\) by “rolling a marble on \\(U\\)”, starting at the current point \\((x^{(0)}, p^{(0)})\\)\nwe get \\(\\text{Hamiltonian}^{(t)} = \\text{Hamiltonian}^{(0)}\\),\ni.e. \\(\\bar \\pi(x^{(t)}, p^{(t)}) = \\bar \\pi(x^{(0)}, p^{(0)})\\),\ni.e. the MH ratio is one, so we automatically accept!\n\nIn practice, we need to approximate the motion of the “rolling marble” so we get \\(\\bar \\pi(x^{(t)}, p^{(t)}) \\approx \\bar \\pi(x^{(0)}, p^{(0)})\\) instead.",
    "crumbs": [
      "MCMC Hacking",
      "HMC basics"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic10_hmc_intuition.html#augmentation",
    "href": "w12_mcmc2/topic10_hmc_intuition.html#augmentation",
    "title": "HMC basics",
    "section": "Augmentation",
    "text": "Augmentation\n\nThe general idea of adding an extra variable to help sampling (like \\(p\\) here) is frequently used to design MCMC algorithms.\nTerminology:\n\nthe variable we add (e.g. \\(p\\) here), is called an auxiliary variable,\nthe joint distribution over the auxiliary variable and the one of interest \\(x\\) is called the augmented distribution \\(\\bar \\pi(x, p)\\).\n\nGenerally, we want \\(\\int \\bar \\pi(x, p) \\mathrm{d}p = \\pi(x)\\) so that we can easily recover the sample of interest.",
    "crumbs": [
      "MCMC Hacking",
      "HMC basics"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic10_hmc_intuition.html#sampling-a-new-momentum",
    "href": "w12_mcmc2/topic10_hmc_intuition.html#sampling-a-new-momentum",
    "title": "HMC basics",
    "section": "Sampling a new momentum",
    "text": "Sampling a new momentum\n\nNotice that \\(K\\) is a quadratic function.\nHence, \\(\\exp(\\text{quadratic})\\) is just a normal distribution!\nHow to sample a new momentum?\n\nJust sample \\(p\\) from a standard normal…\n… while keeping \\(x\\) the same.\nThis is an example of Gibbs sampling.\n\nDenote the above kernel by \\(K_\\text{gibbs}\\).\n\nProposition: \\(K_\\text{gibbs}\\) is \\(\\bar \\pi\\)-invariant.\nProof idea: we show a similar result in the discrete case to simplify notation.\n\nSuppose \\(\\bar \\pi(x) = \\pi_1(x_1) \\pi_2(x_2)\\), \\(x = (x_1, x_2)\\) as with HMC\n\n(to map to the above example, think of \\(x_1\\) as \\(x\\) and \\(x_2\\) as \\(p\\)).\n\nDefine \\(K(x' | x) = \\mathbb{1}[x_1 = x'_1] \\pi_2(x'_2)\\).\nNote we have detailed balance: \\[\\begin{align*}\n\\bar \\pi(x) K(x' | x) &= \\pi_1(x_1) \\pi_2(x_2) \\mathbb{1}[x_1 = x'_1] \\pi_2(x'_2) \\\\\n&= \\pi_1({\\color{red} x'_1}) \\pi_2(x_2) \\mathbb{1}[x_1 = x'_1] \\pi_2(x'_2) \\;\\;\\text{(thanks to the indicator)} \\\\\n&= \\pi_1(x'_1) \\pi_2(x_2) \\mathbb{1}[{\\color{red} x'_1 = x_1}] \\pi_2(x'_2) \\\\\n&= \\pi_1(x'_1) {\\color{red} \\pi_2(x'_2)} \\mathbb{1}[x'_1 = x_1] {\\color{red} \\pi_2(x_2)} \\\\\n&= \\bar \\pi(x') K(x | x'),\n\\end{align*}\\]\nhence global balance holds.",
    "crumbs": [
      "MCMC Hacking",
      "HMC basics"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic10_hmc_intuition.html#high-level-picture",
    "href": "w12_mcmc2/topic10_hmc_intuition.html#high-level-picture",
    "title": "HMC basics",
    "section": "High-level picture",
    "text": "High-level picture\nHMC alternates between two kernels:\n\na complicated kernel \\(K_\\text{hmc}\\): “rolling the ball”, combined with MH accept-reject which we will explore in more details\na simple kernel \\(K_\\text{gibbs}\\): Gibbs sampling on the momentum.",
    "crumbs": [
      "MCMC Hacking",
      "HMC basics"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic05_cke.html",
    "href": "w12_mcmc2/topic05_cke.html",
    "title": "Marginals of a Markov chain",
    "section": "",
    "text": "Computing a marginal of a Markov chain.\n\n\n\n\nThe notion of invariance is formally expressed using marginals of a Markov chain. To check invariance we need formulas to write marginals of Markov chains.",
    "crumbs": [
      "MCMC Hacking",
      "Marginals of a Markov chain"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic05_cke.html#outline",
    "href": "w12_mcmc2/topic05_cke.html#outline",
    "title": "Marginals of a Markov chain",
    "section": "",
    "text": "Computing a marginal of a Markov chain.\n\n\n\n\nThe notion of invariance is formally expressed using marginals of a Markov chain. To check invariance we need formulas to write marginals of Markov chains.",
    "crumbs": [
      "MCMC Hacking",
      "Marginals of a Markov chain"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic05_cke.html#example",
    "href": "w12_mcmc2/topic05_cke.html#example",
    "title": "Marginals of a Markov chain",
    "section": "Example",
    "text": "Example\n\nConsider a large group of identical tourists.\nThey travel the world in a finite set of cities (dots).\nEvery weekend they can board a plane and go somewhere else.\n\nThey base their choice of destination only on the current city.\nThey use randomness (e.g., if a tourist is in London, flip a coin to decide to go to Montreal or Stockholm).\n\n\n\n\n\n\nMarkov model for random tourists\n\n\\(X^{(1)}\\): pick one of the tourists at random, \\(X^{(1)}\\) encodes the city this tourist is in at week 1 (before taking a first flight).\nDistribution of \\(X^{(1)}\\): \\(\\mu_1(x)\\).\n\nInterpretation: for city \\(x\\), \\(\\mu_1(x)\\) tells you the fraction of tourists in city \\(x\\) on the first week.\nSize of the dots in the figure is the fraction of tourists in the city.\n\n\nReview: what is the name of the distribution of \\(X^{(1)}\\)?\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nThe initial distribution.\n\n\n\n\n\\(K(x'|x)\\) encodes the probability that a tourist move to city \\(x'\\) next week given they are at \\(x\\) this week.\n\\(X^{(2)} \\sim K(\\cdot|X^{(1)})\\).\nOn the second week, some cities may now have more or less tourists.\nDistribution of \\(X^{(2)}\\): \\(\\mu_2(x)\\).\n\nInterpretation: for city \\(x\\), \\(\\mu_2(x)\\) tells you the fraction of tourists in city \\(x\\) on the second week.",
    "crumbs": [
      "MCMC Hacking",
      "Marginals of a Markov chain"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic05_cke.html#marginal-of-a-markov-chain",
    "href": "w12_mcmc2/topic05_cke.html#marginal-of-a-markov-chain",
    "title": "Marginals of a Markov chain",
    "section": "Marginal of a Markov chain",
    "text": "Marginal of a Markov chain\n\nDefinition\nDefinition: the distribution on \\(X^{(m)}\\) is called the marginal, denoted \\(\\mu_m(x)\\).",
    "crumbs": [
      "MCMC Hacking",
      "Marginals of a Markov chain"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic05_cke.html#computing-a-marginal",
    "href": "w12_mcmc2/topic05_cke.html#computing-a-marginal",
    "title": "Marginals of a Markov chain",
    "section": "Computing a marginal",
    "text": "Computing a marginal\nQuestion: write the marginal distribution at step 2, \\(\\mu_2\\) using \\(\\mu_1\\) and \\(K\\).",
    "crumbs": [
      "MCMC Hacking",
      "Marginals of a Markov chain"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic06_balance.html",
    "href": "w12_mcmc2/topic06_balance.html",
    "title": "Balance equations",
    "section": "",
    "text": "Global balance (synonym: invariance)\nDetailed balance (synonym: time-reversibility)\n\n\n\n\nWe need formulas to help us prove that MH is \\(\\pi\\)-invariant (and hence combined with irreducibility, admits a LLN).",
    "crumbs": [
      "MCMC Hacking",
      "Balance equations"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic06_balance.html#outline",
    "href": "w12_mcmc2/topic06_balance.html#outline",
    "title": "Balance equations",
    "section": "",
    "text": "Global balance (synonym: invariance)\nDetailed balance (synonym: time-reversibility)\n\n\n\n\nWe need formulas to help us prove that MH is \\(\\pi\\)-invariant (and hence combined with irreducibility, admits a LLN).",
    "crumbs": [
      "MCMC Hacking",
      "Balance equations"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic06_balance.html#example",
    "href": "w12_mcmc2/topic06_balance.html#example",
    "title": "Balance equations",
    "section": "Example",
    "text": "Example\nWe continue our eccentric tourists example…\nQuestion: which travelling scheme(s) are \\(\\pi\\)-invariant?\nIntuition: \\(\\pi\\)-invariance in this context just means the fraction of tourists in each city will stay constant over time.\n\nPairs of cities, where each pair of cities is assigned a fixed number of planes going back and forth between the two cities (see figure, planes always full).\nA tour where a fixed number of planes, equal to the number of cities, go around the world on a common route.\nA central “hub”, where all flights land at the hub (see figure, again, planes always full).\n\n\n\na:  b:  c:",
    "crumbs": [
      "MCMC Hacking",
      "Balance equations"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic06_balance.html#global-balance",
    "href": "w12_mcmc2/topic06_balance.html#global-balance",
    "title": "Balance equations",
    "section": "Global balance",
    "text": "Global balance\nQuestion: combine\n\nthe definition of \\(\\pi\\)-invariance, with\n\nthe formula we derived for the marginals of a Markov chain, i.e., \\[\\mu_2(x') = \\sum_x \\mu_1(x) K(x' | x),\\]\n\nto get an equation characterizing \\(\\pi\\)-invariance in terms of \\(K\\).",
    "crumbs": [
      "MCMC Hacking",
      "Balance equations"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic06_balance.html#detailed-balance",
    "href": "w12_mcmc2/topic06_balance.html#detailed-balance",
    "title": "Balance equations",
    "section": "Detailed balance",
    "text": "Detailed balance\nQuestion: write mathematically “pairs of cities, where each pair of cities is assigned a fixed number of planes going back and forth between the two cities (planes always full).”",
    "crumbs": [
      "MCMC Hacking",
      "Balance equations"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic06_balance.html#relationship-between-detailed-and-global-balance",
    "href": "w12_mcmc2/topic06_balance.html#relationship-between-detailed-and-global-balance",
    "title": "Balance equations",
    "section": "Relationship between detailed and global balance",
    "text": "Relationship between detailed and global balance\nQuestion: what is the relationship between local and global balance? Does one imply the other?",
    "crumbs": [
      "MCMC Hacking",
      "Balance equations"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic02_mh_as_markov.html",
    "href": "w12_mcmc2/topic02_mh_as_markov.html",
    "title": "MH as a Markov chain",
    "section": "",
    "text": "Homogeneous Markov chains\nTransition kernel\nInitial distribution\nMH as a Markov chain\n\n\n\n\nWhy is the MH ratio defined as \\(\\gamma(x')/\\gamma(x)\\)?\nIf we modified the MH ratio, would MCMC still “work”?\nRecall: by “MCMC still work” we mean “MCMC still be consistent” i.e. enjoy a law of large numbers.",
    "crumbs": [
      "MCMC Hacking",
      "MH as a Markov chain"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic02_mh_as_markov.html#outline",
    "href": "w12_mcmc2/topic02_mh_as_markov.html#outline",
    "title": "MH as a Markov chain",
    "section": "",
    "text": "Homogeneous Markov chains\nTransition kernel\nInitial distribution\nMH as a Markov chain\n\n\n\n\nWhy is the MH ratio defined as \\(\\gamma(x')/\\gamma(x)\\)?\nIf we modified the MH ratio, would MCMC still “work”?\nRecall: by “MCMC still work” we mean “MCMC still be consistent” i.e. enjoy a law of large numbers.",
    "crumbs": [
      "MCMC Hacking",
      "MH as a Markov chain"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic02_mh_as_markov.html#markov-chain-definitions",
    "href": "w12_mcmc2/topic02_mh_as_markov.html#markov-chain-definitions",
    "title": "MH as a Markov chain",
    "section": "Markov chain definitions",
    "text": "Markov chain definitions\nTo answer the question above:\n\nwe need to study in more details law of large numbers for Markov chains.\nWe will need some concepts related to Markov chains\n\nRecall: the random variables \\(X^{(1)}, X^{(2)}, \\dots\\) are called a Markov chain if they admit the following “chain” graphical model.\n\n\n\nNote: today we will assume the state space is discrete, we’ll relax this soon.\nDefinition: let \\(K_m(x' | x) = \\mathbb{P}(X^{(m+1)} = x' | X^{(m)}= x)\\). We call \\(K_m\\) the transition kernel (for the \\(m\\)-th step).\nDefinition: if all the transition kernels are equal, \\(K_m = K\\) for some \\(K\\), we say the Markov chain is homogeneous.\nDefinition: the marginal distribution of the first state, \\(X^{(1)}\\) is called the initial distribution.",
    "crumbs": [
      "MCMC Hacking",
      "MH as a Markov chain"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic02_mh_as_markov.html#the-mh-algorithm-as-a-markov-chain",
    "href": "w12_mcmc2/topic02_mh_as_markov.html#the-mh-algorithm-as-a-markov-chain",
    "title": "MH as a Markov chain",
    "section": "The MH algorithm as a Markov chain",
    "text": "The MH algorithm as a Markov chain\nRecall MH involves:\n\na proposal \\(q(x'|x)\\)\na MH ratio \\(r(x, x') = \\gamma(x') / \\gamma(x)\\), and\nan acceptance probability \\(\\alpha(x, x') = \\min(1, r(x, x'))\\).\n\nQuestion: write the transition kernel \\(K(x' | x)\\), assuming \\(x \\neq x'\\).",
    "crumbs": [
      "MCMC Hacking",
      "MH as a Markov chain"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic07_alternation.html",
    "href": "w12_mcmc2/topic07_alternation.html",
    "title": "Kernel alternations",
    "section": "",
    "text": "Alternations of MCMC kernels\nCode implementation\nAlternations preserve invariance\n\n\n\n\nAlternation of kernels is another method to combine kernels. It is mathematically slightly more complicated than mixtures, but can yield faster mixing (in some situations, much faster).\nHere we just give an introduction to kernel alternation. We will talk about the reason why they can yield much faster performance when we talk about non-reversibility.",
    "crumbs": [
      "MCMC Hacking",
      "Kernel alternations"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic07_alternation.html#outline",
    "href": "w12_mcmc2/topic07_alternation.html#outline",
    "title": "Kernel alternations",
    "section": "",
    "text": "Alternations of MCMC kernels\nCode implementation\nAlternations preserve invariance\n\n\n\n\nAlternation of kernels is another method to combine kernels. It is mathematically slightly more complicated than mixtures, but can yield faster mixing (in some situations, much faster).\nHere we just give an introduction to kernel alternation. We will talk about the reason why they can yield much faster performance when we talk about non-reversibility.",
    "crumbs": [
      "MCMC Hacking",
      "Kernel alternations"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic07_alternation.html#alternation-of-mcmc-kernels",
    "href": "w12_mcmc2/topic07_alternation.html#alternation-of-mcmc-kernels",
    "title": "Kernel alternations",
    "section": "Alternation of MCMC kernels",
    "text": "Alternation of MCMC kernels\n\nExample\nWe use the same example as the one we used for MCMC mixtures.\n\n\nCode\n# prior: Beta(alpha, beta)\nalpha = 1\nbeta = 2 \n\n# observations: binomial draws\nn_successes = 3 \nn_trials = 3\n\ngamma_beta_binomial = function(p) {\n  if (p &lt; 0 || p &gt; 1) return(0.0)\n  dbeta(p, alpha, beta) * dbinom(x = n_successes, size = n_trials, prob = p)\n}\n\n\n\n\nIntuition\nSetup: Denote the 2 kernels we want to use by \\(K_1\\) and \\(K_2\\).\nIdea: at each MCMC iteration, let \\(x\\) denote the current state,\n\nSample from \\(K_1\\) from the current state.\nSample from \\(K_2\\) from the state output by \\(K_1\\).\n\n\n\nCoding up kernel alternation\nHere is an example how to implement the alternation. It is based on the same normal proposal MH kernels, \\(K_1\\) and \\(K_2\\), with standard deviations 1 and 2 respectively.\n\n\nCode\nkernel = function(gamma, current_point, proposal_sd) {\n  dim = length(current_point)\n  proposal = rnorm(dim, mean = current_point, sd = proposal_sd) \n  ratio = gamma(proposal) / gamma(current_point) \n  if (runif(1) &lt; ratio) {\n    return(proposal)\n  } else {\n    return(current_point)\n  }\n}\n\n\n\n# simple Metropolis-Hastings algorithm (normal proposal)\nmcmc_alternation = function(gamma, initial_point, n_iters) {\n  samples = numeric(n_iters) \n  dim = length(initial_point)\n  current_point = initial_point\n  for (i in 1:n_iters) {\n\n1    intermediate_point = kernel(gamma, current_point, 1)\n2    current_point = kernel(gamma, intermediate_point, 2)\n    \n    samples[i] = current_point\n  }\n  return(samples)\n}\n\nsamples = mcmc_alternation(gamma_beta_binomial, 0.5,  1000)\n\n\n1\n\nSample from \\(K_1\\).\n\n2\n\nSample from \\(K_2\\) from the state output by \\(K_1\\).\n\n\n\n\n\n\nMathematical notation for kernel mixtures\nQuestion: write a mathematical expression for the alternation \\(K_\\text{alt}\\) in terms of \\(K_1\\) and \\(K_2\\).",
    "crumbs": [
      "MCMC Hacking",
      "Kernel alternations"
    ]
  },
  {
    "objectID": "w12_mcmc2/topic07_alternation.html#invariance-result",
    "href": "w12_mcmc2/topic07_alternation.html#invariance-result",
    "title": "Kernel alternations",
    "section": "Invariance result",
    "text": "Invariance result\n\nAlternation preserves invariance\nProposition: if \\(K_i\\) are \\(\\pi\\)-invariant, then their alternation is \\(\\pi\\)-invariant.\nProof:\n\\[\\begin{align*}\n\\sum_{x} \\pi(x) K_\\text{alt}(x' | x) &= \\sum_{x} \\pi(x) \\sum_{\\check x} K_1(\\check x | x) K_2(x' | \\check x) \\\\\n&=  \\sum_{\\check x} K_2(x' | \\check x) \\sum_{x} \\pi(x)  K_1(\\check x | x) \\\\\n&= \\sum_{\\check x} K_2(x' | \\check x) \\pi(\\check x) \\;\\;\\text{(invariance of $K_1$)} \\\\\n&= \\pi(x') \\;\\;\\text{(invariance of $K_2$)}.\n\\end{align*}\\]",
    "crumbs": [
      "MCMC Hacking",
      "Kernel alternations"
    ]
  },
  {
    "objectID": "w09_workflow/topic06_grouped_data.html",
    "href": "w09_workflow/topic06_grouped_data.html",
    "title": "Grouped data",
    "section": "",
    "text": "Using the package tidybayes to feed complex data into Stan.\n\n\n\n\nData organized in various groups is a frequent feature of Bayesian models, in particular in hierarchical models.\nGetting grouped data into stan is tedious and error prone. The package tidybayes automates much of this.",
    "crumbs": [
      "Bayesian workflow",
      "Grouped data"
    ]
  },
  {
    "objectID": "w09_workflow/topic06_grouped_data.html#outline",
    "href": "w09_workflow/topic06_grouped_data.html#outline",
    "title": "Grouped data",
    "section": "",
    "text": "Using the package tidybayes to feed complex data into Stan.\n\n\n\n\nData organized in various groups is a frequent feature of Bayesian models, in particular in hierarchical models.\nGetting grouped data into stan is tedious and error prone. The package tidybayes automates much of this.",
    "crumbs": [
      "Bayesian workflow",
      "Grouped data"
    ]
  },
  {
    "objectID": "w09_workflow/topic06_grouped_data.html#pre-reading",
    "href": "w09_workflow/topic06_grouped_data.html#pre-reading",
    "title": "Grouped data",
    "section": "Pre-reading",
    "text": "Pre-reading\nIf you have never heard about “tidy data”, while it is not strictly essential for this course, it is a good investment to skim this tutorial on tidy data.",
    "crumbs": [
      "Bayesian workflow",
      "Grouped data"
    ]
  },
  {
    "objectID": "w09_workflow/topic06_grouped_data.html#example",
    "href": "w09_workflow/topic06_grouped_data.html#example",
    "title": "Grouped data",
    "section": "Example",
    "text": "Example\nFirst, install the packages magrittr and tidybayes (and ggplot2 if you have not done so already), then import them:\n\nsuppressPackageStartupMessages(require(rstan))\nsuppressPackageStartupMessages(require(magrittr))\nsuppressPackageStartupMessages(require(tidybayes))\nsuppressPackageStartupMessages(require(ggplot2))\n\n\nData prep\nWe load the data used in exercise 6:\n\ndata = read.csv(url(\"https://raw.githubusercontent.com/UBC-Stat-ML/web447/main/exercises/ex06_assets/vaccines_full.csv\"))\ndata$is_vaccinated = ifelse(data$arms == \"vaccinated\", 1, 0)\nrmarkdown::paged_table(data)\n\n\n  \n\n\n\nThe magic conversion of “tidy data” (data in a format like the above) into a format that can be consumed by Stan is done using compose_data:\n\nstan_converted = compose_data(data)\n\nstan_converted\n\n$trials\n[1] 1 1 3 3 2 2\n\n$n_trials\n[1] 3\n\n$arms\n[1] 2 1 2 1 2 1\n\n$n_arms\n[1] 2\n\n$groupSizes\n[1]  5807  5829 18198 18325 14134 14073\n\n$numbersOfCases\n[1]  30 101   8 162  11 185\n\n$is_vaccinated\n[1] 1 0 1 0 1 0\n\n$n\n[1] 6\n\n\n\n\nStan model\nHere we consider a simple, non-hierarchical model but fitting all the data at once. In the exercise, you will modify this model to follow the hierarchical structure of exercise 6.\n\ndata {\n  int n;\n  int n_trials;\n  array[n] int&lt;lower=1,upper=n_trials&gt; trials;\n  array[n] int arms;\n  int n_arms;\n  array[n] int groupSizes;\n  array[n] int numbersOfCases;\n  array[n] int is_vaccinated;\n}\n\nparameters {\n  vector&lt;lower=0,upper=1&gt;[n_trials] efficiencies;\n  vector&lt;lower=0,upper=1&gt;[n_trials] prevalences;\n}\n\nmodel {\n\n  for (trial in 1:n_trials) {\n    efficiencies[trial] ~ beta(1, 1);\n    prevalences[trial] ~ beta(1, 1);\n  }\n\n  for (i in 1:n) {\n    numbersOfCases[i] ~ binomial(groupSizes[i], prevalences[trials[i]] * (is_vaccinated[i] == 1 ? 1.0 - efficiencies[trials[i]] : 1.0));\n  }\n}\n\n\n\nFitting\nThe fit object returned by sampling does not know about the string labels attached to each trial integer index. We use fit %&lt;&gt;% recover_types(data) to add back that information:\n\nfit = sampling(\n  vaccines,\n  seed = 1,\n  data = stan_converted, \n  refresh = 0,\n  iter = 10000                  \n)\nfit %&lt;&gt;% recover_types(data)\n\n\n\nGetting draws\nThe output of Stan is not tidy either.\n\nfit\n\nInference for Stan model: anon_model.\n4 chains, each with iter=10000; warmup=5000; thin=1; \npost-warmup draws per chain=5000, total post-warmup draws=20000.\n\n                    mean se_mean   sd     2.5%      25%      50%      75%\nefficiencies[1]     0.69    0.00 0.07     0.55     0.65     0.69     0.73\nefficiencies[2]     0.94    0.00 0.02     0.89     0.92     0.94     0.95\nefficiencies[3]     0.94    0.00 0.02     0.90     0.93     0.95     0.96\nprevalences[1]      0.02    0.00 0.00     0.01     0.02     0.02     0.02\nprevalences[2]      0.01    0.00 0.00     0.01     0.01     0.01     0.01\nprevalences[3]      0.01    0.00 0.00     0.01     0.01     0.01     0.01\nlp__            -2793.24    0.02 1.77 -2797.52 -2794.21 -2792.89 -2791.93\n                   97.5% n_eff Rhat\nefficiencies[1]     0.80 22048    1\nefficiencies[2]     0.97 23668    1\nefficiencies[3]     0.97 24974    1\nprevalences[1]      0.02 22889    1\nprevalences[2]      0.02 25768    1\nprevalences[3]      0.01 25400    1\nlp__            -2790.81  8951    1\n\nSamples were drawn using NUTS(diag_e) at Thu Mar 14 17:33:47 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\nUse spread_draws to put the draws into a tidy format:\n\nfit %&gt;% spread_draws(efficiencies[trials], prevalences[trials]) %&gt;% head(10)\n\n# A tibble: 10 × 6\n# Groups:   trials [1]\n   trials               efficiencies .chain .iteration .draw prevalences\n   &lt;chr&gt;                       &lt;dbl&gt;  &lt;int&gt;      &lt;int&gt; &lt;int&gt;       &lt;dbl&gt;\n 1 AZ-Oxford (combined)        0.479      1          1     1      0.0120\n 2 AZ-Oxford (combined)        0.862      1          2     2      0.0218\n 3 AZ-Oxford (combined)        0.712      1          3     3      0.0194\n 4 AZ-Oxford (combined)        0.695      1          4     4      0.0163\n 5 AZ-Oxford (combined)        0.660      1          5     5      0.0164\n 6 AZ-Oxford (combined)        0.642      1          6     6      0.0188\n 7 AZ-Oxford (combined)        0.613      1          7     7      0.0176\n 8 AZ-Oxford (combined)        0.720      1          8     8      0.0150\n 9 AZ-Oxford (combined)        0.645      1          9     9      0.0150\n10 AZ-Oxford (combined)        0.687      1         10    10      0.0164\n\n\n\n\nPlotting\nNow that we have draws in tidy format, instead of using specialized MCMC plotting libraries we can just use ggplot:\n\nfit %&gt;%\n  spread_draws(efficiencies[trials]) %&gt;%\n  ggplot(aes(x = efficiencies, y = trials)) +\n  stat_halfeye() + \n  theme_minimal()\n\n\n\n\n\n\n\n\nThis makes it easier to customize plots.\n\n\nSummaries\ntidybayes also offers convenient ways to compute summaries such as High Density Intervals (HDI)\n\nfit %&gt;%\n  spread_draws(efficiencies[trials]) %&gt;%\n  median_hdi(efficiencies)\n\n# A tibble: 3 × 7\n  trials               efficiencies .lower .upper .width .point .interval\n  &lt;chr&gt;                       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 AZ-Oxford (combined)        0.694  0.558  0.806   0.95 median hdi      \n2 Moderna-NIH                 0.937  0.896  0.970   0.95 median hdi      \n3 Pfizer-BioNTech             0.946  0.905  0.977   0.95 median hdi",
    "crumbs": [
      "Bayesian workflow",
      "Grouped data"
    ]
  },
  {
    "objectID": "w09_workflow/topic06_grouped_data.html#more-information",
    "href": "w09_workflow/topic06_grouped_data.html#more-information",
    "title": "Grouped data",
    "section": "More information",
    "text": "More information\nSee the tidybayes documentation.",
    "crumbs": [
      "Bayesian workflow",
      "Grouped data"
    ]
  },
  {
    "objectID": "w09_workflow/topic02_goodness_of_fit.html",
    "href": "w09_workflow/topic02_goodness_of_fit.html",
    "title": "Goodness of fit",
    "section": "",
    "text": "General notion of goodness-of-fit checks.\nSpecific example for Bayesian models: posterior predictive checks.\nLimitations.\n\n\n\n\nIs your statistical model missing some critical aspect of the data? We have approached this question in a qualitative way earlier in the course. Today, we provide a more quantitative approach. In practice, both qualitative and quantitative model criticism are essential ingredients of an effective Bayesian data analysis.",
    "crumbs": [
      "Bayesian workflow",
      "Goodness of fit"
    ]
  },
  {
    "objectID": "w09_workflow/topic02_goodness_of_fit.html#outline",
    "href": "w09_workflow/topic02_goodness_of_fit.html#outline",
    "title": "Goodness of fit",
    "section": "",
    "text": "General notion of goodness-of-fit checks.\nSpecific example for Bayesian models: posterior predictive checks.\nLimitations.\n\n\n\n\nIs your statistical model missing some critical aspect of the data? We have approached this question in a qualitative way earlier in the course. Today, we provide a more quantitative approach. In practice, both qualitative and quantitative model criticism are essential ingredients of an effective Bayesian data analysis.",
    "crumbs": [
      "Bayesian workflow",
      "Goodness of fit"
    ]
  },
  {
    "objectID": "w09_workflow/topic02_goodness_of_fit.html#what-is-goodness-of-fit",
    "href": "w09_workflow/topic02_goodness_of_fit.html#what-is-goodness-of-fit",
    "title": "Goodness of fit",
    "section": "What is goodness-of-fit?",
    "text": "What is goodness-of-fit?\n\nGoodness-of-fit: a procedure to assess if a model is good (approximately well-specified) or bad (grossly mis-specified).\nApplies to both Bayesian and non-Bayesian models, obviously we focus on the former today.",
    "crumbs": [
      "Bayesian workflow",
      "Goodness of fit"
    ]
  },
  {
    "objectID": "w09_workflow/topic02_goodness_of_fit.html#review-calibration",
    "href": "w09_workflow/topic02_goodness_of_fit.html#review-calibration",
    "title": "Goodness of fit",
    "section": "Review: calibration",
    "text": "Review: calibration\nTo understand today’s material we need to review the notion of calibration of credible intervals.\n\n\n\nQuestion: for well-specified models, credible intervals are…\n\ncalibrated for small data, calibrated for large data\nnot calibrated for small data, calibrated for large data\nonly approximately calibrated for both small and large data\nnone of the above",
    "crumbs": [
      "Bayesian workflow",
      "Goodness of fit"
    ]
  },
  {
    "objectID": "w09_workflow/topic02_goodness_of_fit.html#from-calibration-to-goodness-of-fit",
    "href": "w09_workflow/topic02_goodness_of_fit.html#from-calibration-to-goodness-of-fit",
    "title": "Goodness of fit",
    "section": "From calibration to goodness-of-fit",
    "text": "From calibration to goodness-of-fit\nQuestion: Can we do goodness-of-fit check calibration on latent variables \\(X\\)?\n\nYes\nNo\n\n\nReview: prediction\n\nRecall that Bayesian models can be used to predict the next observation, \\(y_{n+1}\\).\nWe did this…\n\nmathematically,\nin simPPLe,\nin the first quiz,\nand in this week’s exercise you will do it in Stan using generated quantities.\n\n\nQuestion: Can we do goodness-of-fit check calibration on a prediction?\n\nYes\nNo\n\n\n\nPosterior predictive check\n\nLet \\(C(y)\\) denote a 99% credible interval computed from data \\(y\\).\nLet \\(y_{\\backslash n}\\) denote the data excluding point \\(n\\).\nOutput a warning if \\(y_n \\notin C(y_{\\backslash n})\\).\n\nProposition: if the model is well-specified, \\[\\mathbb{P}(Y_n \\in C(Y_{\\backslash n})) = 99\\%.\\]\nProof: special case of our generic result on calibration of credible intervals..\nQuestion: what are potential cause(s) of a posterior predictive “warning”, (i.e., \\(y_n \\notin C(y_{\\backslash n})\\)):\n\nModel mis-specification.\nPosterior is not approximately normal.\nMCMC too slow and/or not enough samples.\nBad luck.\nSoftware defect.",
    "crumbs": [
      "Bayesian workflow",
      "Goodness of fit"
    ]
  },
  {
    "objectID": "w09_workflow/topic04_mcmc_diagnostics.html",
    "href": "w09_workflow/topic04_mcmc_diagnostics.html",
    "title": "MCMC diagnostics",
    "section": "",
    "text": "Notion of mixing.\nHeuristics to detect pathological behaviour:\n\n\n\n\nWe have seen that MCMC is consistent, however the speed of convergence can vary considerably due to the dependence between the successive draws.\nWhen convergence is too slow it may be necessary to change the inference algorithm, either into another MCMC algorithm or to a variational method.",
    "crumbs": [
      "Bayesian workflow",
      "MCMC diagnostics"
    ]
  },
  {
    "objectID": "w09_workflow/topic04_mcmc_diagnostics.html#outline",
    "href": "w09_workflow/topic04_mcmc_diagnostics.html#outline",
    "title": "MCMC diagnostics",
    "section": "",
    "text": "Notion of mixing.\nHeuristics to detect pathological behaviour:\n\n\n\n\nWe have seen that MCMC is consistent, however the speed of convergence can vary considerably due to the dependence between the successive draws.\nWhen convergence is too slow it may be necessary to change the inference algorithm, either into another MCMC algorithm or to a variational method.",
    "crumbs": [
      "Bayesian workflow",
      "MCMC diagnostics"
    ]
  },
  {
    "objectID": "w09_workflow/topic04_mcmc_diagnostics.html#overview",
    "href": "w09_workflow/topic04_mcmc_diagnostics.html#overview",
    "title": "MCMC diagnostics",
    "section": "Overview",
    "text": "Overview\nInformally, there are two possible situations to distinguish:\nFast mixing:\n\nThe chain is almost like i.i.d. sampling, just a constant time slower (e.g., 2x slower).\n\nThis constant is related to the relative effective sample size (e.g., if 2x slower, relative ESS would be \\(1/2\\)).\nNotice Stan estimates ESS when printing a fit object, look for column n_eff.\nMore on this soon.\n\nFast mixing happens when the dependence between time step \\(i\\) and \\(i+m\\) decays exponentially in \\(m\\).1\n\nSlow/torpid mixing:\n\nTerminology: slow/torpid mixing.\nMCMC still consistent, but you may have to wait for years to get usable answer!\nIn this case, changes have to be made to the sampler.\nWe will cover two alternatives to consider for these difficult targets:\n\nTempering methods (week 12).\nVariational methods (week 13).",
    "crumbs": [
      "Bayesian workflow",
      "MCMC diagnostics"
    ]
  },
  {
    "objectID": "w09_workflow/topic04_mcmc_diagnostics.html#heuristics-to-detect-slow-mixing-chains",
    "href": "w09_workflow/topic04_mcmc_diagnostics.html#heuristics-to-detect-slow-mixing-chains",
    "title": "MCMC diagnostics",
    "section": "Heuristics to detect slow mixing chains",
    "text": "Heuristics to detect slow mixing chains\n\nKey idea: run several independent chains from “over-dispersed” initializations.\n\nOver-dispersed: use at least as much noise as the prior (roughly).\n\nCheck for differences between the independent chains:\n\nTrace plots.\nRank plots.\n\nThese are not bullet-proof methods…\n… but it is still a good idea to use them unless theory provides guarantees (e.g. log-concave distributions).\n\n\nExample of fast and slow mixing chains\n\nsuppressPackageStartupMessages(require(rstan))\nsuppressPackageStartupMessages(require(ggplot2))\nsuppressPackageStartupMessages(require(bayesplot))\n\nEasy problem: a familiar beta-binomial problem.\n\ndata {\n  int&lt;lower=0&gt; n_trials;\n  int&lt;lower=0&gt; n_successes;\n}\nparameters {\n  real&lt;lower=0, upper=1&gt; p;\n}\n\nmodel {\n  p ~ uniform(0, 1);\n  n_successes ~ binomial(n_trials, p);\n}\n\nChallenging problem: binomial likelihood, but with “too many parameters.”\n\nWrite \\(p = p_1 p_2\\), where \\(p_i \\sim {\\mathrm{Unif}}(0, 1)\\).\nThis creates an unidentifiability: for each value \\(p\\) there are several possible \\(p_1, p_2\\) such that \\(p = p_1 p_2\\).\nThe posterior looks like a “thin ridge” (see visualization here, obtained using tempering, covered in week 12).\n\n\n\n\n\ndata {\n  int&lt;lower=0&gt; n_trials;\n  int&lt;lower=0&gt; n_successes;\n}\nparameters {\n  real&lt;lower=0, upper=1&gt; p1;\n  real&lt;lower=0, upper=1&gt; p2;\n}\n\nmodel {\n  p1 ~ uniform(0, 1);\n  p2 ~ uniform(0, 1);\n  n_successes ~ binomial(n_trials, p1 * p2);\n}\n\n\nWe run both easy and hard problems with 2 parallel chains each.\nIn practice use more than 2.\n\n\nfit_easy = sampling(\n  betabinom,\n  seed = 1,\n  chains = 2,\n  refresh = 0,\n  data = list(n_trials=10000000, n_successes=10000000/2),       \n  iter = 1000                   \n)\n\nfit_hard = sampling(\n  unid,\n  seed = 1,\n  chains = 2,\n  refresh = 0,\n  data = list(n_trials=10000000, n_successes=10000000/2),       \n  iter = 1000                   \n)\n\nTrace plots: spot the difference between the easy and hard problems (each line in a given plot is an independent chain).\n\nmcmc_trace(fit_easy, pars = c(\"p\")) + theme_minimal()\n\n\n\n\n\n\n\nmcmc_trace(fit_hard, pars = c(\"p1\")) + theme_minimal()\n\n\n\n\n\n\n\n\nRank histogram:\n\nFirst look at the chains combined and compute ranks.\nDisplay for each chain the rank distribution for the samples in that chain.\nIn the fast mixing case, all histograms should be approximately uniform.\n\n\nmcmc_rank_hist(fit_easy, pars = c(\"p\")) + theme_minimal()\n\n\n\n\n\n\n\nmcmc_rank_hist(fit_hard, pars = c(\"p1\")) + theme_minimal()",
    "crumbs": [
      "Bayesian workflow",
      "MCMC diagnostics"
    ]
  },
  {
    "objectID": "w09_workflow/topic04_mcmc_diagnostics.html#footnotes",
    "href": "w09_workflow/topic04_mcmc_diagnostics.html#footnotes",
    "title": "MCMC diagnostics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“Fast mixing” is often formalized using one of several equivalent definitions of geometric ergodicity.↩︎",
    "crumbs": [
      "Bayesian workflow",
      "MCMC diagnostics"
    ]
  },
  {
    "objectID": "w11_quiz2/topic02_q2_practice_questions.html",
    "href": "w11_quiz2/topic02_q2_practice_questions.html",
    "title": "Practice questions (Quiz 2)",
    "section": "",
    "text": "Caution\n\n\n\nPage under construction: information on this page may change.",
    "crumbs": [
      "Quiz 2",
      "Practice questions (Quiz 2)"
    ]
  },
  {
    "objectID": "w11_quiz2/topic02_q2_practice_questions.html#outline",
    "href": "w11_quiz2/topic02_q2_practice_questions.html#outline",
    "title": "Practice questions (Quiz 2)",
    "section": "Outline",
    "text": "Outline\n\nTopics\n\nRepresentative example of questions to prep for Quiz 2.\n\n\n\nImportant note\nMake sure to practice the similar page for Quiz 1 since quiz 2 will cover that material as well.\n\n\nLibraries needed to run the example below\n\nsuppressPackageStartupMessages(require(rstan))",
    "crumbs": [
      "Quiz 2",
      "Practice questions (Quiz 2)"
    ]
  },
  {
    "objectID": "w11_quiz2/topic02_q2_practice_questions.html#model-construction",
    "href": "w11_quiz2/topic02_q2_practice_questions.html#model-construction",
    "title": "Practice questions (Quiz 2)",
    "section": "Model construction",
    "text": "Model construction\nConsider the following setup\n\nYou have a cohort of 15 unemployed persons who are all starting a job search process at the same time.\nFor each participant, you have collected 2 covariates: their age and their number of years of education.\nYou contact the participants each day for 10 days and record the day they secured a new job.\nAt the end of your study, 3 of the participants are still looking for a job.\n\n\nUsing the ~ notation\nDefine a Bayesian model to handle this dataset. Introduce all random variables, and specify for each its data type, if it observed or not.\n\n\n\nRao-blackwellization\nWrite the joint density of the model in the last part, before and after Rao-Blackwellization. You can introduce symbols for densities and CDFs, for example leave the density of the exponential as \\(p_\\text{Exp}(x; \\lambda),\\) and similarly use \\(F_\\text{Name}(\\cdot; \\cdot)\\) to denote CDFs.",
    "crumbs": [
      "Quiz 2",
      "Practice questions (Quiz 2)"
    ]
  },
  {
    "objectID": "w11_quiz2/topic02_q2_practice_questions.html#model-debugging",
    "href": "w11_quiz2/topic02_q2_practice_questions.html#model-debugging",
    "title": "Practice questions (Quiz 2)",
    "section": "Model debugging",
    "text": "Model debugging\nLet:\n\n\\(C(y)\\) denote a 99% credible interval computed from data \\(y\\).\nLet \\(y\\) and \\(\\check y\\) denote a real and synthetic (simulated) data respectively.\n\nSuppose you observed the following:\n\n\\(y_n \\notin C(y_{\\backslash n})\\)\n\\(\\check y_n \\notin C(\\check y_{\\backslash n})\\)\nOn both synthetic and real data, trace plots and ESS both look good.\nYou replicated these experiments several times and always get the same results.\n\nWhat would you do next? Justify your answer.",
    "crumbs": [
      "Quiz 2",
      "Practice questions (Quiz 2)"
    ]
  },
  {
    "objectID": "w11_quiz2/topic02_q2_practice_questions.html#normalization-constant",
    "href": "w11_quiz2/topic02_q2_practice_questions.html#normalization-constant",
    "title": "Practice questions (Quiz 2)",
    "section": "Normalization constant",
    "text": "Normalization constant\nConsider the Bayesian model:\n\\[\\begin{align*}\nX &\\sim {\\mathrm{Exp}}(1/100) \\\\\nY &\\sim {\\mathrm{Poisson}}(X).\n\\end{align*}\\]\nWhen using MCMC, will the output change if, for \\(x &gt; 0\\)…\n\nYou used \\(\\check f(x) = \\exp(-(1/100) x)\\) instead of \\(f(x) = (1/100) \\exp(-(1/100) x)\\) for the prior?\nYou used \\(\\hat f(y|x) = x^y / y!\\) instead of \\(f(y|x) = \\exp(-x) x^y / y!\\) for the likelihood?",
    "crumbs": [
      "Quiz 2",
      "Practice questions (Quiz 2)"
    ]
  },
  {
    "objectID": "w11_quiz2/topic02_q2_practice_questions.html#intervals",
    "href": "w11_quiz2/topic02_q2_practice_questions.html#intervals",
    "title": "Practice questions (Quiz 2)",
    "section": "Intervals",
    "text": "Intervals\nSuppose you see the following output from a Stan MCMC fit object:\nInference for Stan model: anon_model.\n1 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=1000.\n\n            mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nslope       0.42    0.00 0.04  0.34  0.40  0.42  0.45  0.51   719    1\nsigma       0.24    0.00 0.04  0.18  0.21  0.24  0.27  0.33   639    1\nprediction  0.64    0.01 0.27  0.14  0.47  0.64  0.81  1.20  1023    1\nlp__       21.32    0.06 1.04 18.46 20.94 21.62 22.09 22.36   331    1\n\nSamples were drawn using NUTS(diag_e) at Thu Mar 14 14:41:50 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\nReport a 80% confidence interval to capture the Monte Carlo error for the posterior mean of the prediction parameter.\n\nSome of the following will be helpful in answering that question:\n\nqnorm(0.8)\n\n[1] 0.8416212\n\nqnorm(0.9)\n\n[1] 1.281552\n\nqnorm(0.95)\n\n[1] 1.644854\n\n\n\nReport a 50% credible interval for the prediction parameter. You can ignore Monte Carlo error in this sub-question.",
    "crumbs": [
      "Quiz 2",
      "Practice questions (Quiz 2)"
    ]
  },
  {
    "objectID": "w11_quiz2/topic02_q2_practice_questions.html#bias-and-consistency",
    "href": "w11_quiz2/topic02_q2_practice_questions.html#bias-and-consistency",
    "title": "Practice questions (Quiz 2)",
    "section": "Bias and consistency",
    "text": "Bias and consistency\nLet \\(\\hat G_M\\) denote a Monte Carlo estimator based on \\(M\\) iterations, providing an approximation for an intractable expectation, \\(g^* = \\mathbb{E}_\\pi[g(X)]\\).\n\nDefine the notion of bias.\nDefine the notion of consistency.\nIn the context of Monte Carlo methods, what is more important, a bias of zero (unbiasness) or consistency? Why?",
    "crumbs": [
      "Quiz 2",
      "Practice questions (Quiz 2)"
    ]
  },
  {
    "objectID": "w11_quiz2/topic02_q2_practice_questions.html#stan-based-prediction",
    "href": "w11_quiz2/topic02_q2_practice_questions.html#stan-based-prediction",
    "title": "Practice questions (Quiz 2)",
    "section": "Stan-based prediction",
    "text": "Stan-based prediction\nConsider the following code to perform Bayesian linear regression on galaxy distances and velocities:\n\ndata {\n  int&lt;lower=0&gt; N; // number of observations\n  vector[N] xs;   // independent variable\n  vector[N] ys;   // dependent variable\n}\n\nparameters {\n  real slope;\n  real&lt;lower=0&gt; sigma;\n}\n\nmodel {\n  // prior\n  slope     ~ student_t(3, 0, 100);\n  sigma     ~ exponential(0.001);\n\n  // likelihood\n  ys ~ normal(slope*xs, sigma);\n}\n\nHow would you modify this code to predict the velocity of a galaxy at distance 1.5? Hint: use the function normal_rng(mean, sd) to generate a normal random variable with the provided mean and standard deviation parameters.",
    "crumbs": [
      "Quiz 2",
      "Practice questions (Quiz 2)"
    ]
  },
  {
    "objectID": "w11_quiz2/topic02_q2_practice_questions.html#metropolis-hastings",
    "href": "w11_quiz2/topic02_q2_practice_questions.html#metropolis-hastings",
    "title": "Practice questions (Quiz 2)",
    "section": "Metropolis-Hastings",
    "text": "Metropolis-Hastings\nFill the two gaps in the pseudo code below:\n\nInitialize \\(X^{(0)}\\) arbitrarily\nFor \\(m = 1, 2, \\dots, M\\) do:\n\nDenote the proposal at iteration \\(m \\in \\{1, 2, \\dots, M\\}\\) by: \\[\\tilde X^{(m)}\\sim q(\\cdot | X^{(m-1)}).\\]\nCompute the MH ratio: \\[R^{(m)}= \\frac{\\gamma(\\tilde X^{(m)})}{\\gamma(X^{(m-1)})}.\\]\nSample an acceptance Bernoulli: \\[A^{(m)}\\sim \\text{???}.\\]\n\nIf \\(A^{(m)}= 1\\), we accept the proposed sample: \\[X^{(m)}= \\text{???}\\]\nElse, \\(A^{(m)}= 0\\), and we reject the proposed sample and stay at previous position: \\[X^{(m)}= X^{(m-1)}.\\]",
    "crumbs": [
      "Quiz 2",
      "Practice questions (Quiz 2)"
    ]
  },
  {
    "objectID": "w11_quiz2/topic02_q2_practice_questions.html#stan",
    "href": "w11_quiz2/topic02_q2_practice_questions.html#stan",
    "title": "Practice questions (Quiz 2)",
    "section": "Stan",
    "text": "Stan\n\nExplain the difference between the parameters block and the transformed parameters block in Stan.\nWhy do you think Stan uses 4 independent chains by default?",
    "crumbs": [
      "Quiz 2",
      "Practice questions (Quiz 2)"
    ]
  },
  {
    "objectID": "w11_quiz2/topic02_q2_practice_questions.html#reasoning-about-mh",
    "href": "w11_quiz2/topic02_q2_practice_questions.html#reasoning-about-mh",
    "title": "Practice questions (Quiz 2)",
    "section": "Reasoning about MH",
    "text": "Reasoning about MH\nConsider the MH algorithm where we use as proposal a normal centered at the current point with standard deviation \\(\\sigma_p\\).\nYou observe the following trace plot:\n\n\n\n\n\n\n\n\n\n\nIs this chain mixing well?\nWhy or why not?\nIf it is not, what course of action do you recommend?",
    "crumbs": [
      "Quiz 2",
      "Practice questions (Quiz 2)"
    ]
  },
  {
    "objectID": "w11_quiz2/topic02_q2_practice_questions.html#irreducibility",
    "href": "w11_quiz2/topic02_q2_practice_questions.html#irreducibility",
    "title": "Practice questions (Quiz 2)",
    "section": "Irreducibility",
    "text": "Irreducibility\nConsider the following MH setup:\n\n\\(\\gamma(x) = \\mathbb{1}[x \\in \\{1, 2, \\dots, 10\\}]\\)\n\\(q(x' | x) = \\mathbb{1}[x' \\in \\{x-1, x+1\\}]/2\\).\n\n\nDefine irreducibility.\nProve that the MH algorithm is irreducible in this setup.",
    "crumbs": [
      "Quiz 2",
      "Practice questions (Quiz 2)"
    ]
  },
  {
    "objectID": "w11_quiz2/topic02_q2_practice_questions.html#mcmc-diagnostics",
    "href": "w11_quiz2/topic02_q2_practice_questions.html#mcmc-diagnostics",
    "title": "Practice questions (Quiz 2)",
    "section": "MCMC diagnostics",
    "text": "MCMC diagnostics\nExplain how to detect slow mixing from a rank plot.",
    "crumbs": [
      "Quiz 2",
      "Practice questions (Quiz 2)"
    ]
  },
  {
    "objectID": "w11_quiz2/topic02_q2_practice_questions.html#mcse",
    "href": "w11_quiz2/topic02_q2_practice_questions.html#mcse",
    "title": "Practice questions (Quiz 2)",
    "section": "MCSE",
    "text": "MCSE\nHow would you proceed if you want to decrease Monte Carlo Standard Error (MCSE)?",
    "crumbs": [
      "Quiz 2",
      "Practice questions (Quiz 2)"
    ]
  },
  {
    "objectID": "w11_quiz2/topic02_q2_practice_questions.html#debugging",
    "href": "w11_quiz2/topic02_q2_practice_questions.html#debugging",
    "title": "Practice questions (Quiz 2)",
    "section": "Debugging",
    "text": "Debugging\nYou wish to write Stan model for logistic regression with normal priors on the parameters with prior variance 100.\nConsider the following draft of a Stan model:\ndata { \n  int N\n  array[N] int y\n}\n\nmodel {\n  slope ~ normal(0, 100);\n  intercept ~ normal(0, 100);\n  for (i in 1:N) {\n    y[i] ~ bernoulli(inv_logit(intercept + slope * i));\n  }\n}\nIdentify as many bugs as you can, and correct each.",
    "crumbs": [
      "Quiz 2",
      "Practice questions (Quiz 2)"
    ]
  },
  {
    "objectID": "w11_quiz2/topic02_q2_practice_questions.html#effective-sample-size-from-asymptotic-variance",
    "href": "w11_quiz2/topic02_q2_practice_questions.html#effective-sample-size-from-asymptotic-variance",
    "title": "Practice questions (Quiz 2)",
    "section": "Effective sample size from asymptotic variance",
    "text": "Effective sample size from asymptotic variance\nRecall that the CLT for i.i.d. and Markov chains give us the following approximations:\n\n\\(\\sqrt{M} (\\bar X_\\text{Markov} - \\mu) \\approx \\sigma_a G,\\)\n\\(\\sqrt{n_e} (\\bar X_\\text{iid} - \\mu) \\approx \\sigma G,\\)\n\nwhere \\(M\\) is the number of iterations, \\(n_e\\) is the effective sample size, \\(\\bar X_\\text{Markov}\\) and \\(\\bar X_\\text{iid}\\) are the MC estimators based on MCMC and i.i.d. sampling respectively, \\(G\\) is standard normal, \\(\\mu\\), \\(\\sigma\\) are the posterior mean and standard deviation, and \\(\\sigma_a\\) is the asymptotic variance.\nUse these two approximations to write a formula for the effective sample size based on \\(\\sigma\\), \\(\\sigma_a\\) and \\(M\\).",
    "crumbs": [
      "Quiz 2",
      "Practice questions (Quiz 2)"
    ]
  },
  {
    "objectID": "w11_quiz2/topic02_q2_practice_questions.html#independence-prior-vs-posterior",
    "href": "w11_quiz2/topic02_q2_practice_questions.html#independence-prior-vs-posterior",
    "title": "Practice questions (Quiz 2)",
    "section": "Independence: prior vs posterior",
    "text": "Independence: prior vs posterior\nSuppose we place a prior where \\(X_1\\) and \\(X_2\\) are independent, say \\(X_i \\sim {\\mathrm{Bern}}(0.5)\\) independently. If we condition on some data \\(Y\\), will \\(X_1\\) and \\(X_2\\) always be independent under the posterior?",
    "crumbs": [
      "Quiz 2",
      "Practice questions (Quiz 2)"
    ]
  },
  {
    "objectID": "w11_quiz2/topic03_q2_logistics.html",
    "href": "w11_quiz2/topic03_q2_logistics.html",
    "title": "Logistics",
    "section": "",
    "text": "You only need pencil, eraser, student or government id.\nThe quiz is closed book (in particular, no electronics, including simple calculator permitted).",
    "crumbs": [
      "Quiz 2",
      "Logistics"
    ]
  },
  {
    "objectID": "w11_quiz2/topic03_q2_logistics.html#what-to-bring",
    "href": "w11_quiz2/topic03_q2_logistics.html#what-to-bring",
    "title": "Logistics",
    "section": "",
    "text": "You only need pencil, eraser, student or government id.\nThe quiz is closed book (in particular, no electronics, including simple calculator permitted).",
    "crumbs": [
      "Quiz 2",
      "Logistics"
    ]
  },
  {
    "objectID": "w11_quiz2/topic03_q2_logistics.html#time",
    "href": "w11_quiz2/topic03_q2_logistics.html#time",
    "title": "Logistics",
    "section": "Time",
    "text": "Time\nWe will start promptly at 9:30 (please arrive on time) and end at 10:45 (75 minutes).",
    "crumbs": [
      "Quiz 2",
      "Logistics"
    ]
  },
  {
    "objectID": "w11_quiz2/topic03_q2_logistics.html#distribution-reference",
    "href": "w11_quiz2/topic03_q2_logistics.html#distribution-reference",
    "title": "Logistics",
    "section": "Distribution reference",
    "text": "Distribution reference\nThe cover page of the exam will have the following table. Note that compared to last quiz, additional distributions have been added.\n\n\n\n\n\n\n\n\nName\nAbbreviation\nParameters\n\n\n\n\nBernoulli\n\\({\\mathrm{Bern}}(p)\\)\nSuccess probability \\(p \\in [0, 1]\\)\n\n\nBinomial\n\\({\\mathrm{Binom}}(n, p)\\)\nNumber of trials \\(n \\in \\mathbb{N}\\), success probability \\(p \\in [0, 1]\\)\n\n\nUniform\n\\({\\mathrm{Unif}}(a, b)\\)\nLeft and right bounds, \\(a &lt; b\\)\n\n\nNormal\n\\(\\mathcal{N}(\\mu, \\sigma)\\)\nMean \\(\\mu \\in \\mathbb{R}\\) and standard deviation \\(\\sigma &gt; 0\\)\n\n\n\n\\(\\mathcal{N}(\\mu, \\sigma^2)\\)\nMean \\(\\mu \\in \\mathbb{R}\\) and variance \\(\\sigma^2 &gt; 0\\)\n\n\n\n\\(\\mathcal{N}(\\mu, \\tau)\\)\nMean \\(\\mu \\in \\mathbb{R}\\) and precision \\(\\tau = 1/\\sigma^2 &gt; 0\\)\n\n\nExponential\n\\({\\mathrm{Exp}}(\\lambda)\\)\nRate \\(\\lambda\\) (\\(=1/\\)mean)\n\n\nBeta\n\\({\\mathrm{Beta}}(\\alpha, \\beta)\\)\nShape parameters \\(\\alpha &gt; 0\\) and \\(\\beta &gt; 0\\)\n\n\n\n\\({\\mathrm{Beta}}(\\mu, s)\\)\nMean parameter \\(\\mu \\in (0, 1)\\) and concentration \\(s&gt;0\\)\n\n\nPoisson\n\\({\\mathrm{Poisson}}(\\lambda)\\)\nMean \\(\\lambda &gt; 0\\)\n\n\nNegative Binomial\n\\({\\mathrm{NegBinom}}(\\mu, \\phi)\\)\nMean parameter \\(\\mu &gt; 0\\) and concentration \\(\\phi &gt;0\\)\n\n\nGamma\n\\({\\mathrm{Gam}}(\\alpha, \\beta)\\)\nShape parameters \\(\\alpha &gt; 0\\) and rate \\(\\beta &gt; 0\\)\n\n\nCategorical\n\\({\\mathrm{Categorical}}(p_1, \\dots, p_K)\\)\nProbabilities \\(p_k &gt; 0\\), \\(\\sum_k p_k = 1\\)\n\n\nDirichlet\n\\({\\mathrm{Dir}}(\\alpha_1, \\dots, \\alpha_K)\\)\nConcentrations \\(\\alpha_i &gt; 0\\)\n\n\nMultivariate Normal\n\\(\\mathcal{N}(\\mu, \\Sigma)\\)\nMean vector \\(\\mu \\in \\mathbb{R}^K\\), covariance matrix \\(\\Sigma \\succ 0\\)",
    "crumbs": [
      "Quiz 2",
      "Logistics"
    ]
  },
  {
    "objectID": "w13_advanced_infer/topic06_stoch_grad.html",
    "href": "w13_advanced_infer/topic06_stoch_grad.html",
    "title": "Stochastic gradient estimation",
    "section": "",
    "text": "Stochastic gradient estimation problem\nPush-out estimator (also known as “reparameterization trick”)\nSubsampling\n\n\n\n\nTo optimize our variational objective \\(L\\) using SGD, we need to construct an unbiased estimator of the gradient of \\(L\\).\nWe review one particularly effective method to do so, coming from the operations research literature, the push-out estimator Rubinstein, 1992. It is called the “reparameterization trick” in the machine learning literature.",
    "crumbs": [
      "Advanced inference",
      "Stochastic gradient estimation"
    ]
  },
  {
    "objectID": "w13_advanced_infer/topic06_stoch_grad.html#outline",
    "href": "w13_advanced_infer/topic06_stoch_grad.html#outline",
    "title": "Stochastic gradient estimation",
    "section": "",
    "text": "Stochastic gradient estimation problem\nPush-out estimator (also known as “reparameterization trick”)\nSubsampling\n\n\n\n\nTo optimize our variational objective \\(L\\) using SGD, we need to construct an unbiased estimator of the gradient of \\(L\\).\nWe review one particularly effective method to do so, coming from the operations research literature, the push-out estimator Rubinstein, 1992. It is called the “reparameterization trick” in the machine learning literature.",
    "crumbs": [
      "Advanced inference",
      "Stochastic gradient estimation"
    ]
  },
  {
    "objectID": "w13_advanced_infer/topic06_stoch_grad.html#variational-inference-objective",
    "href": "w13_advanced_infer/topic06_stoch_grad.html#variational-inference-objective",
    "title": "Stochastic gradient estimation",
    "section": "Variational inference objective",
    "text": "Variational inference objective\nRecall, our VI objective function is \\[\\begin{align*}\nL(\\phi) &= \\int q_\\phi(x) \\left[ \\log q_\\phi(x)  - \\log \\gamma(x)   \\right] \\mathrm{d}x \\\\\n&= \\mathbb{E}_\\phi\\left[ \\log q_\\phi(X)  - \\log \\gamma(X) \\right],\n\\end{align*}\\] where we use write a subscript \\(\\phi\\) on \\(\\mathbb{E}_\\phi[\\cdot]\\) to emphasize that the expectation is with respect to a distribution \\(q_\\phi\\) that depends on \\(\\phi\\).",
    "crumbs": [
      "Advanced inference",
      "Stochastic gradient estimation"
    ]
  },
  {
    "objectID": "w13_advanced_infer/topic06_stoch_grad.html#generic-gradient-estimation-setup",
    "href": "w13_advanced_infer/topic06_stoch_grad.html#generic-gradient-estimation-setup",
    "title": "Stochastic gradient estimation",
    "section": "Generic gradient estimation setup",
    "text": "Generic gradient estimation setup\nTo make notation cleaner, we abstract out the problem to:\nDefinition: the stochastic gradient estimation problem consists in finding an unbiased estimator for the gradient of \\(\\mathbb{E}_\\phi[h(\\phi, X)],\\) where \\(X \\sim q_\\phi\\).\nExample: for VI, \\(h(\\phi, x) = \\log q_\\phi(x)  - \\log \\gamma(x)\\).",
    "crumbs": [
      "Advanced inference",
      "Stochastic gradient estimation"
    ]
  },
  {
    "objectID": "w13_advanced_infer/topic06_stoch_grad.html#difficulty",
    "href": "w13_advanced_infer/topic06_stoch_grad.html#difficulty",
    "title": "Stochastic gradient estimation",
    "section": "Difficulty",
    "text": "Difficulty\nFind the most serious error in the buggy argument below:\n\\[\\begin{align*}\n\\nabla \\mathbb{E}_\\phi[h(\\phi, X)] &= \\mathbb{E}_\\phi[\\nabla h(\\phi, X)] \\;\\;\\text{(interchange of $\\nabla$ and $\\mathbb{E}$)} \\\\\n&\\approx \\underbrace{ \\frac{1}{M} \\sum_{m=1}^M \\nabla h(\\phi, X^{(m)})}_{\\text{broken stochastic gradient estimator}} \\;\\;\\text{(simple Monte Carlo)}.\n\\end{align*}\\]",
    "crumbs": [
      "Advanced inference",
      "Stochastic gradient estimation"
    ]
  },
  {
    "objectID": "w13_advanced_infer/topic06_stoch_grad.html#solution-reparameterization",
    "href": "w13_advanced_infer/topic06_stoch_grad.html#solution-reparameterization",
    "title": "Stochastic gradient estimation",
    "section": "Solution: reparameterization",
    "text": "Solution: reparameterization\nIdea: move the parts of \\(q\\) that depend on \\(\\phi\\) into \\(h\\) via reparametrization.\nExample:\n\nsuppose \\(\\{q_\\phi\\}\\) is a normal family, so \\(\\phi = (\\mu, \\sigma^2)\\).\nNote:\n\nif \\(S \\sim \\mathcal{N}(0, 1)\\) is standard normal,\nthen \\(\\sigma S + \\mu \\sim \\mathcal{N}(\\mu, \\sigma^2)\\)\nhence: \\[\\mathbb{E}_\\phi[h(\\phi, X)] = \\mathbb{E}[h(\\phi, \\sigma S + \\mu)].\\]\n\nNotice that on the right-hand side, the distribution with respect to which we take the expectation no longer depends on \\(\\phi\\)!\n\nGeneral method: (“push-out estimator” or “reparameterization trick”)\n\nif for all \\(\\phi\\), \\(X_\\phi \\sim q_\\phi\\)…\n…you can write \\(X_\\phi = f(S, \\phi)\\)\n\nfor some random variable \\(S\\)\n\nand function \\(f(s, \\phi)\\) (e.g., \\(f(s, \\phi) = \\sigma s + \\mu\\) in the normal case),1\n\nthen:\n\n\\[\\begin{align*}\n\\nabla \\mathbb{E}_\\phi[h(\\phi, X)] &= \\nabla \\mathbb{E}[h(\\phi, f(S, \\phi))] \\\\\n&= \\mathbb{E}[\\nabla\\{h(\\phi, f(S, \\phi))\\}] \\;\\;\\text{(assuming Leibniz integral rule applies)} \\\\\n&\\approx \\frac{1}{M} \\sum_{m=1}^M \\nabla\\{h(\\phi, f(S^{(m)}, \\phi))\\}.\n\\end{align*}\\]\nFinally, one typically use reverse mode autodiff to compute \\(\\nabla\\{h(\\phi, f(S^{(m)}, \\phi))\\}\\).",
    "crumbs": [
      "Advanced inference",
      "Stochastic gradient estimation"
    ]
  },
  {
    "objectID": "w13_advanced_infer/topic06_stoch_grad.html#subsampling",
    "href": "w13_advanced_infer/topic06_stoch_grad.html#subsampling",
    "title": "Stochastic gradient estimation",
    "section": "Subsampling",
    "text": "Subsampling\n\nOften (but not always), the function \\(h\\) be written as: \\[h(\\phi, x) = \\sum_{i=1}^N h_i(\\phi, x),\\]\nexample: when the data is i.i.d. (recall we are in log space, \\(h(\\phi, x) = \\log q_\\phi(x)  - \\log \\gamma(x)\\))\n\n\\(N\\) is then the number of data points\n\nWhen \\(N\\) is large “sub-sampling” will tradeoff:\n\ncomputationally cheaper gradient…\n…at the cost of more noise (variance) and hence more SGD iterations \\(t\\) needed.\n\nIdea: sub-sampling consists in\n\nsampling one term (data point) \\(I \\sim {\\mathrm{Unif}}\\{1, 2, \\dots, N\\}\\)\ncomputing unbiased estimate \\(\\hat G_I\\) for the random term\ndebiasing by returning \\(N \\hat G_I\\)\n\n\nProperty: the subsampling estimator \\(N \\hat G_I\\) is unbiased provided for each \\(i\\), \\(\\hat G_i\\) is unbiased.\nProof:\n\\[\\begin{align*}\n\\mathbb{E}[N \\hat G_I] &= N \\mathbb{E}[ \\mathbb{E}[\\hat G_I|I]] \\;\\;\\text{(law of total expectation)} \\\\\n&= N \\sum_{i=1}^N \\frac{1}{N} \\mathbb{E}[\\hat G_i] \\\\\n&= \\sum_{i=1}^N \\mathbb{E}[\\hat G_i].\n\\end{align*}\\]\nMini-batching: this idea can be extended to picking a small subset of points (typically, the maximum that can fit in the GPU memory).",
    "crumbs": [
      "Advanced inference",
      "Stochastic gradient estimation"
    ]
  },
  {
    "objectID": "w13_advanced_infer/topic06_stoch_grad.html#references",
    "href": "w13_advanced_infer/topic06_stoch_grad.html#references",
    "title": "Stochastic gradient estimation",
    "section": "References",
    "text": "References\nSee Mohamed et al, 2020, Monte Carlo Gradient Estimation in Machine Learning, JMLR.",
    "crumbs": [
      "Advanced inference",
      "Stochastic gradient estimation"
    ]
  },
  {
    "objectID": "w13_advanced_infer/topic06_stoch_grad.html#footnotes",
    "href": "w13_advanced_infer/topic06_stoch_grad.html#footnotes",
    "title": "Stochastic gradient estimation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTypically, in order for the interchange of gradient and expectation to hold, \\(f\\) has to be differentiable everywhere (not just almost everywhere, see e.g., Stat 547C notes, section 6.8 for an example).↩︎",
    "crumbs": [
      "Advanced inference",
      "Stochastic gradient estimation"
    ]
  },
  {
    "objectID": "w13_advanced_infer/topic01_intro.html",
    "href": "w13_advanced_infer/topic01_intro.html",
    "title": "Overview",
    "section": "",
    "text": "Resources when…\n\n…the model is complex but data not too large: tempering methods (Nikola’s tutorial),\n…the model is not too complex but data is large: variational methods (VI) (today),\n…the model is complex and data is large: largely open, we need more young researchers like you!!!",
    "crumbs": [
      "Advanced inference",
      "Overview"
    ]
  },
  {
    "objectID": "w13_advanced_infer/topic01_intro.html#this-weeks-outline",
    "href": "w13_advanced_infer/topic01_intro.html#this-weeks-outline",
    "title": "Overview",
    "section": "",
    "text": "Resources when…\n\n…the model is complex but data not too large: tempering methods (Nikola’s tutorial),\n…the model is not too complex but data is large: variational methods (VI) (today),\n…the model is complex and data is large: largely open, we need more young researchers like you!!!",
    "crumbs": [
      "Advanced inference",
      "Overview"
    ]
  },
  {
    "objectID": "w13_advanced_infer/topic04_opt.html",
    "href": "w13_advanced_infer/topic04_opt.html",
    "title": "Optimizing the KL",
    "section": "",
    "text": "Black-box / automatic differentiation variational inference (ADVI)\nCoordinate ascent variational inference (CAVI)\n\n\n\n\nWe have now identified our objective function, the ELBO. We still need to pick a numerical method to optimize it.",
    "crumbs": [
      "Advanced inference",
      "Optimizing the KL"
    ]
  },
  {
    "objectID": "w13_advanced_infer/topic04_opt.html#outline",
    "href": "w13_advanced_infer/topic04_opt.html#outline",
    "title": "Optimizing the KL",
    "section": "",
    "text": "Black-box / automatic differentiation variational inference (ADVI)\nCoordinate ascent variational inference (CAVI)\n\n\n\n\nWe have now identified our objective function, the ELBO. We still need to pick a numerical method to optimize it.",
    "crumbs": [
      "Advanced inference",
      "Optimizing the KL"
    ]
  },
  {
    "objectID": "w13_advanced_infer/topic04_opt.html#overview",
    "href": "w13_advanced_infer/topic04_opt.html#overview",
    "title": "Optimizing the KL",
    "section": "Overview",
    "text": "Overview\n\nBefore ~2015, the user had to do mathematical derivation each time they wanted to apply VI to a new model.\nThis changed with the advent of “black box methods” such ADVI.\n\n\nIn this course we focus on 2 since they are easier to use.\nHowever, 1 is still useful as it can be much faster in practice.",
    "crumbs": [
      "Advanced inference",
      "Optimizing the KL"
    ]
  },
  {
    "objectID": "w13_advanced_infer/topic04_opt.html#black-box-methods",
    "href": "w13_advanced_infer/topic04_opt.html#black-box-methods",
    "title": "Optimizing the KL",
    "section": "Black box methods",
    "text": "Black box methods\n\nIdea: use a gradient descent method to minimize \\(L(\\phi)\\).\n\n\n\n\n\nDifficulty: the objective function \\(L\\) has an integral over \\(q\\). How to compute its gradient?\nSolution:\n\napproximate the gradient using a Monte Carlo method.\nFeed that gradient into a Stochastic Gradient Descent (SGD) algorithm.\nConvergence guarantees typically ask that this approximation be unbiased.",
    "crumbs": [
      "Advanced inference",
      "Optimizing the KL"
    ]
  },
  {
    "objectID": "w13_advanced_infer/topic04_opt.html#references",
    "href": "w13_advanced_infer/topic04_opt.html#references",
    "title": "Optimizing the KL",
    "section": "References",
    "text": "References\n\nSee Blei et al., 2018.",
    "crumbs": [
      "Advanced inference",
      "Optimizing the KL"
    ]
  },
  {
    "objectID": "exercises/ex03.html",
    "href": "exercises/ex03.html",
    "title": "Exercise 3: Write your own PPL!",
    "section": "",
    "text": "Introduce Monte Carlo integration in continuous spaces.\nImplement importance sampling.\nBuild a universal probabilistic programming language in &lt;30 lines of code.",
    "crumbs": [
      "A first look at PPLs",
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/ex03.html#goals",
    "href": "exercises/ex03.html#goals",
    "title": "Exercise 3: Write your own PPL!",
    "section": "",
    "text": "Introduce Monte Carlo integration in continuous spaces.\nImplement importance sampling.\nBuild a universal probabilistic programming language in &lt;30 lines of code.",
    "crumbs": [
      "A first look at PPLs",
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/ex03.html#q.1-functions-on-the-unit-interval",
    "href": "exercises/ex03.html#q.1-functions-on-the-unit-interval",
    "title": "Exercise 3: Write your own PPL!",
    "section": "Q.1: functions on the unit interval",
    "text": "Q.1: functions on the unit interval\nFor this question, use Simple Monte Carlo. The main twist compared to week one is that you will use a continuous random variable.\n\nWrite a function mc_estimate that takes a function \\(f:[0,1]\\to\\mathbb{R}\\) and outputs a Monte Carlo estimate of \\(\\int_0^1 f(x)\\mathrm{d}x\\) using \\(n=10000\\) independent samples from \\({\\mathrm{Unif}}(0,1)\\).\nConsider the function \\(f:[0,1] \\to [0,\\infty)\\) given by \\[\nf(x) = e^{-x^2}.\n\\] It is possible to show that \\[\n\\int_0^1 f(x) \\, \\mathrm{d}x = \\frac{\\sqrt{\\pi}}{2} \\text{erf}(1) \\approx 0.7468241,\n\\tag{1}\\] where \\(\\text{erf}(\\cdot)\\) is the error function. Test your implementation of mc_estimate by checking that it produces an answer close to the value in Equation 1.\nApproximate the following integral using mc_estimate: \\[\n\\int_0^1 \\sin(\\cos(\\sin(x))) \\mathrm{d}x.\n\\]",
    "crumbs": [
      "A first look at PPLs",
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/ex03.html#sec-simPPLe",
    "href": "exercises/ex03.html#sec-simPPLe",
    "title": "Exercise 3: Write your own PPL!",
    "section": "Q.2: implementing SNIS for simPPLe",
    "text": "Q.2: implementing SNIS for simPPLe\nIn this question, you will write the function posterior that we used in the PPL introduction.\n\nFirst, install the package distr, which allows us to work with distributions as objects—a necessary ingredient of every PPL. Load or install it using\n\n\nif (!require(distr)){\n  install.packages(\"distr\")\n  require(distr)\n}\n\n\nRead this short tutorial on distr. Nothing to submit for this item.\nRead the “scaffold code”, and use distr and two of the functions below to create a fair coin, flip it, and to compute the probability of that flip:\n\n\n\nex03_scaffold.R\n\nsuppressPackageStartupMessages(library(distr))\n\n## Utilities to make the distr library a bit nicer to use\n\np &lt;- function(distribution, realization) {\n  d(distribution)(realization) # return the PMF or density \n}\n\nBern = function(probability_to_get_one) {\n  DiscreteDistribution(supp = 0:1, prob = c(1-probability_to_get_one, probability_to_get_one))\n}\n\n## Key functions called by simPPLe programs\n\n# Use simulate(distribution) for unobserved random variables\nsimulate &lt;- function(distribution) {\n  r(distribution)(1) # sample once from the given distribution\n}\n\n# Use observe(realization, distribution) for observed random variables\nobserve = function(realization, distribution) {\n  # `&lt;&lt;-` lets us modify variables that live in the global scope from inside a function\n  weight &lt;&lt;- weight * p(distribution, realization) \n}\n\n\nComplete the implementation of the function posterior:\n\nposterior = function(ppl_function, number_of_iterations) {\n  numerator = 0.0\n  denominator = 0.0\n  for (i in 1:number_of_iterations) {\n    weight &lt;&lt;- 1.0\n    # update numerator and denominator\n  }\n  return(numerator/denominator)\n}\n\nTest your program by checking that you can approximate the posterior probability of the fair coin obtained in exercise 1, Q.2.s",
    "crumbs": [
      "A first look at PPLs",
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/ex04.html",
    "href": "exercises/ex04.html",
    "title": "Exercise 4: the joy of probabilistic inference",
    "section": "",
    "text": "Make sure to read the simPPLe setup page before you begin.",
    "crumbs": [
      "The joy of probabilistic modelling",
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/ex04.html#q.1-logistic-rocket-improvement",
    "href": "exercises/ex04.html#q.1-logistic-rocket-improvement",
    "title": "Exercise 4: the joy of probabilistic inference",
    "section": "Q.1: logistic rocket improvement",
    "text": "Q.1: logistic rocket improvement\nConsider the Ariane 1 data we used this week,\nsuccess_indicators = c(1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1)\nand the model described in the same page.\nRecall that we discussed a model where the reliability of the rocket changes in time. This will allow us to incorporate, for example, the fact that engineering teams implement fixes based on past launches and therefore the probability of success should increase.\n\nWrite a function called logistic_regression containing a simPPLe probabilistic programming description of the model described in class. Your function should return a vector containing 3 elements in the following order:\n\nthe intercept (\\(\\in \\mathbb{R}\\)),\nthe slope (\\(\\in \\mathbb{R}\\)),\na prediction if one more launch would have been successful (1) or a failure (0) (\\(\\in \\{0, 1\\}\\)).\n\nFollow the instructions in the appendix below to get some helper functions. Use these functions to reproduce the lecture’s bivariate posterior plot over the intercept and slope parameters.\nEstimate the probability that the next launch is a success given the data under the logistic model.\nCreate a variant of the same model but where the slope is set to zero. Estimate the probability that the next launch is a success given the data under this simplified model.",
    "crumbs": [
      "The joy of probabilistic modelling",
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/ex04.html#q.2-choosing-a-model",
    "href": "exercises/ex04.html#q.2-choosing-a-model",
    "title": "Exercise 4: the joy of probabilistic inference",
    "section": "Q.2: choosing a model",
    "text": "Q.2: choosing a model\nYou debate with your friend whether the logistic model or the simplified model (with slope equals to zero) should be preferred. To stop that debate, write a unified model which gives probability 1/2 to the simplified model, and 1/2 to the logistic model. Estimate the posterior probability that the logistic model is preferred under the unified model given the same data as in Q.1.",
    "crumbs": [
      "The joy of probabilistic modelling",
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/ex04.html#sec-utils",
    "href": "exercises/ex04.html#sec-utils",
    "title": "Exercise 4: the joy of probabilistic inference",
    "section": "Appendix",
    "text": "Appendix\nFor Q.1.2, you will need to copy the following code and paste it in a file called simple_utils.R. We have demonstrated the use of these functions in the lectures.\n\n\nsimple_utils.R\n\n\nposterior_particles = function(ppl_function, number_of_iterations) {\n  weight &lt;&lt;- 1.0\n  sample = ppl_function()\n  dimension = length(sample) \n  samples = matrix(0, nrow = number_of_iterations, ncol = dimension, \n                   dimnames = list(1:number_of_iterations, names(sample)))\n  weights = rep(0, number_of_iterations)\n  for (i in 1:number_of_iterations) {\n    weight &lt;&lt;- 1.0       # reset the weight accumulator\n    sample = ppl_function()\n    samples[i,] = sample\n    weights[i]  = weight\n  }\n  return(list(samples=samples, weights=weights))\n}\n\ness = function(particles){\n  w = particles$weights\n  return(effective_sample_size(w))\n}\n\neffective_sample_size = function(w){\n  (sum(w)^2)/sum(w^2)\n}\n\nrepresentative_sample = function(snis_output, percentile=0.9999){\n  ess = effective_sample_size(snis_output$weights)\n  idx_ordered_weights = order(snis_output$weights, decreasing = TRUE)\n  acc_norm_weights = cumsum(snis_output$weights[idx_ordered_weights])/sum(snis_output$weights)\n  reduced_sample_size = max(round(ess), max(which(acc_norm_weights &lt; percentile)))\n  idx_subset = idx_ordered_weights[1:reduced_sample_size]\n  list(samples = snis_output$samples[idx_subset,], weights = snis_output$weights[idx_subset])\n}\nweighted_scatter_plot = function(\n    snis_output,\n    plot_idxs = 1:2,\n    base_color_hex = hcl.colors(1, palette = \"viridis\"),\n    plot_options = list(xlab=\"Param 1\", ylab=\"Param 2\")\n){\n  base_color = col2rgb(base_color_hex)/255\n  \n  # find the subset with almost all the mass\n  snis_subset = representative_sample(snis_output)\n  \n  # linear transform of weights to [0,1]\n  extreme_weights = range(snis_subset$weights, na.rm = T)\n  alphas = (snis_subset$weights-extreme_weights[1])/diff(extreme_weights)\n  \n  # create colors with transparencies and plot\n  points_color_alphas = rgb(base_color[1],base_color[2],base_color[3], alphas)\n  call_args=c(\n    list(x=snis_subset$samples[, plot_idxs], col=points_color_alphas), \n    plot_options\n  )\n  do.call(plot, call_args)\n}",
    "crumbs": [
      "The joy of probabilistic modelling",
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/ex05.html",
    "href": "exercises/ex05.html",
    "title": "Exercise 5: Bayesian theory",
    "section": "",
    "text": "Caution\n\n\n\nPage under construction: information on this page may change.",
    "crumbs": [
      "Some theory",
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/ex06.html",
    "href": "exercises/ex06.html",
    "title": "Exercise 6: Hierarchical models",
    "section": "",
    "text": "Caution\n\n\n\nPage under construction: information on this page may change.",
    "crumbs": [
      "Hierarchical models",
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/ex03_distr_tutorial.html",
    "href": "exercises/ex03_distr_tutorial.html",
    "title": "STAT 405",
    "section": "",
    "text": "Here are the functionalities in distr you will need for the exercise\nLoading the package without annoying prompt:\nsuppressPackageStartupMessages(require(distr))"
  },
  {
    "objectID": "exercises/ex03_distr_tutorial.html#creating-a-distribution-object",
    "href": "exercises/ex03_distr_tutorial.html#creating-a-distribution-object",
    "title": "STAT 405",
    "section": "Creating a distribution object",
    "text": "Creating a distribution object\nLet’s create a Poisson with parameter \\(\\lambda = 3.2\\), which we will use in the following to demonstrate the functionalities in distr you will need for the exercise\n\ndistPoisson &lt;- Pois(lambda = 3.2)"
  },
  {
    "objectID": "exercises/ex03_distr_tutorial.html#sampling-from-it",
    "href": "exercises/ex03_distr_tutorial.html#sampling-from-it",
    "title": "STAT 405",
    "section": "Sampling from it",
    "text": "Sampling from it\nHere, we sample from our Poisson distribution 1 time:\n\nr(distPoisson)(1)\n\n[1] 4"
  },
  {
    "objectID": "exercises/ex03_distr_tutorial.html#evaluating-the-pmf-or-density",
    "href": "exercises/ex03_distr_tutorial.html#evaluating-the-pmf-or-density",
    "title": "STAT 405",
    "section": "Evaluating the PMF or density",
    "text": "Evaluating the PMF or density\nHere we compute the PMF of a poisson at the realization \\(x = 4\\):\n\nd(distPoisson)(4)\n\n[1] 0.1780928"
  },
  {
    "objectID": "w07_quiz1/topic02_practice_questions.html",
    "href": "w07_quiz1/topic02_practice_questions.html",
    "title": "Practice questions",
    "section": "",
    "text": "Caution\n\n\n\nPage under construction: information on this page may change.",
    "crumbs": [
      "Quiz 1",
      "Practice questions"
    ]
  },
  {
    "objectID": "w07_quiz1/topic02_practice_questions.html#outline",
    "href": "w07_quiz1/topic02_practice_questions.html#outline",
    "title": "Practice questions",
    "section": "Outline",
    "text": "Outline\n\nTopics\n\nRepresentative example of questions to prep for Quiz 1.\n\n\n\nImportant note\nThe quiz will not focus on multiple choice questions—we use mostly multiple choice in this page just to make it easier to pace the questions during the lecture.\nBut try to answer before looking at the possible choices to get a more realistic practice!",
    "crumbs": [
      "Quiz 1",
      "Practice questions"
    ]
  },
  {
    "objectID": "w07_quiz1/topic02_practice_questions.html#snis",
    "href": "w07_quiz1/topic02_practice_questions.html#snis",
    "title": "Practice questions",
    "section": "SNIS",
    "text": "SNIS\nConsider the following partial code for simPPLe’s SNIS engine:\nposterior = function(ppl_function, number_of_iterations) {\n  numerator = 0.0\n  denominator = 0.0\n  for (i in 1:number_of_iterations) {\n    weight &lt;&lt;- 1.0\n    g_i = ppl_function()\n    numerator = numerator + weight * g_i\n    ???????????????\n  }\n  return(numerator/denominator)\n}\nWhat is the missing line, ????????????????\n\n\n\n\n\n\nClick for choices\n\n\n\n\n\n\ndenominator = denominator + g_i\ndenominator = denominator + g_i^2\ndenominator = denominator + weight\ndenominator = denominator + weight^2\nNone of the above.\n\n\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\ndenominator = denominator + weight\nSee exercise 3 and the page on simPPLe.",
    "crumbs": [
      "Quiz 1",
      "Practice questions"
    ]
  },
  {
    "objectID": "w07_quiz1/topic02_practice_questions.html#comparing-two-parameters",
    "href": "w07_quiz1/topic02_practice_questions.html#comparing-two-parameters",
    "title": "Practice questions",
    "section": "Comparing two parameters",
    "text": "Comparing two parameters\nConsider the Bayesian model: for \\(i \\in \\{1, 2\\}\\),\n\\[\\begin{align*}\n\\theta_i &\\sim {\\mathrm{Unif}}(0, 1) \\\\\ny_i | \\theta_i &\\sim {\\mathrm{Beta}}(\\theta_i).\n\\end{align*}\\]\nYou would like to answer the question: is \\(\\theta_1\\) “clearly different” than \\(\\theta_2\\)? To be concrete, let’s say that “clearly different” means their value differs by at least \\(0.1\\).\nHow would you provide a Bayesian answer to the above question?\n\n\n\n\n\n\nClick for choices\n\n\n\n\n\n\nCompute \\(\\mathbb{P}(|\\theta_1 - \\theta_2| &gt; 0.1 | y_1, y_2)\\).\nCompute \\(\\mathbb{E}[\\theta_1 | y_1]\\) and \\(\\mathbb{E}[\\theta_2 | y_2]\\), and check if they differ by more than 0.1.\nCompute the MAP of \\(\\theta_1 | y_1\\) and of \\(\\theta_2 | y_2\\), and check if they differ by more than 0.1.\nCompute the posterior median of \\(\\theta_1 | y_1\\) and of \\(\\theta_2 | y_2\\), and check if they differ by more than 0.1.\nNone of the above\n\n\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nCompute \\(\\mathbb{P}(|\\theta_1 - \\theta_2| &gt; 0.1 | y_1, y_2)\\).\nSee the page on bivariate posteriors.",
    "crumbs": [
      "Quiz 1",
      "Practice questions"
    ]
  },
  {
    "objectID": "w07_quiz1/topic02_practice_questions.html#normal-distributions",
    "href": "w07_quiz1/topic02_practice_questions.html#normal-distributions",
    "title": "Practice questions",
    "section": "Normal distributions",
    "text": "Normal distributions\nThe variability of the normal can be parameterized with the standard derivation, variance, or precision.\nWhich of the three has the same units of measurement as the mean parameter?\n\n\n\n\n\n\nClick for choices\n\n\n\n\n\n\nStandard deviation.\nVariance.\nPrecision.\nNone of them.\n\n\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nThe standard deviation.\nSee the page on normal distributions.",
    "crumbs": [
      "Quiz 1",
      "Practice questions"
    ]
  },
  {
    "objectID": "w07_quiz1/topic02_practice_questions.html#discrete-inference",
    "href": "w07_quiz1/topic02_practice_questions.html#discrete-inference",
    "title": "Practice questions",
    "section": "Discrete inference",
    "text": "Discrete inference\nConsider the small HMM on the right defined on binary random variables.\n\n\n\nEach edge from a parent random variable to a child in the HMM’s graphical model work the same way: the value of the child is the same as the parent with probability \\(2/3\\), and it is flipped with probability \\(1/3\\). The distribution of \\(X_1\\) is \\({\\mathrm{Bern}}(1/2)\\). Mathematically: \\[\\begin{align*}\nX_1 &\\sim {\\mathrm{Bern}}(1/2) \\\\\nX_2 | X_1 &\\sim {\\mathrm{Bern}}((2/3)X_1 + (1/3)(1-X_1)) \\\\\nY_i | X_i &\\sim {\\mathrm{Bern}}((2/3)X_i + (1/3)(1-X_i)).\n\\end{align*}\\]\nCompute \\(\\mathbb{P}(X_2 = 0 | Y_1 = 0, Y_2 = 0)\\).\n\n\n\n\n\n\nClick for choices\n\n\n\n\n\n\n\\(1/7\\)\n\\(2/7\\)\n\\(3/7\\)\n\\(5/7\\)\n\\(8/9\\)\n\n\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nListing the state pairs in the order \\((x_1, x_2) \\in ((0, 0), (0, 1), (1, 0), (1, 1))\\) We have \\(\\gamma \\propto (2^3, 2, 2, 2)\\), hence \\(\\pi = (8/14, 2/14, 2/14, 2/14)\\) so \\(\\mathbb{P}(X_2 = 0 | Y_1 = 0, Y_2 = 0) = 4/7 + 1/7 = 5/7\\).\nSee exercise 1, Bayes rule and detailed derivation of last clicker question in the predictive inference page.",
    "crumbs": [
      "Quiz 1",
      "Practice questions"
    ]
  },
  {
    "objectID": "w07_quiz1/topic02_practice_questions.html#model-construction",
    "href": "w07_quiz1/topic02_practice_questions.html#model-construction",
    "title": "Practice questions",
    "section": "Model construction",
    "text": "Model construction\nConsider the following dataset, showing the monthly number of sun spots from 2010 to 2024:\n\n\nCode\n# source: https://www.sidc.be/SILSO/infosnmtot\nsuppressPackageStartupMessages(require(\"dplyr\"))\nsuppressPackageStartupMessages(require(\"ggplot2\"))\n\ndf = read.csv(\"../data/sunspots-SN_m_tot_V2.0.csv\", sep = \";\", header=FALSE) %&gt;%\n        mutate(count = ceiling(V4)) %&gt;%\n        rename(year = V3) %&gt;%\n        filter(year &gt; 2005)\n\n#by_year = df %&gt;% group_by(year) %&gt;% summarize(count = sum(count))\n\ndf %&gt;% ggplot(aes(x = year, y = count)) + geom_point() + ylab(\"Number of sun spots\") + theme_minimal()\n\n\n\n\n\n\n\n\n\nDenote the observations by \\(y_i\\) where \\(i\\) is the number of months since January 2005, and \\(y_i\\) is the number of sun spots observed that month.\nDesign a Bayesian model to predict the number of sun spots in the next decade. Describe it using the “~” notation. To help you, the following table will be provided during the quiz:\n\n\n\n\n\n\n\n\nName\nAbbreviation\nParameters\n\n\n\n\nBernoulli\n\\({\\mathrm{Bern}}(p)\\)\nSuccess probability \\(p \\in [0, 1]\\)\n\n\nBinomial\n\\({\\mathrm{Binom}}(n, p)\\)\nNumber of trials \\(n \\in \\mathbb{N}\\), success probability \\(p \\in [0, 1]\\)\n\n\nUniform\n\\({\\mathrm{Unif}}(a, b)\\)\nLeft and right bounds, \\(a &lt; b\\)\n\n\nNormal\n\\(\\mathcal{N}(\\mu, \\sigma)\\)\nMean \\(\\mu \\in \\mathbb{R}\\) and standard deviation \\(\\sigma &gt; 0\\)\n\n\nExponential\n\\({\\mathrm{Exp}}(\\lambda)\\)\nRate \\(\\lambda\\) (\\(=1/\\)mean)\n\n\nBeta\n\\({\\mathrm{Beta}}(\\alpha, \\beta)\\)\nShape parameters \\(\\alpha &gt; 0\\) and \\(\\beta &gt; 0\\)\n\n\n\nAlso describe at least one potential source of model mis-specification.\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nSeveral possible answers are possible, here is an example:\n\\[\\begin{align*}\n\\theta_1 &\\sim {\\mathrm{Exp}}(1/1000) \\\\\n\\theta_2 &\\sim {\\mathrm{Exp}}(1/1000) \\\\\n\\theta_3 &\\sim {\\mathrm{Unif}}(0, 2\\pi) \\\\\ny_i | \\theta &\\sim {\\mathrm{Poisson}}(\\theta_1 (\\sin(\\theta_2 i + \\theta_3) + 1))\n\\end{align*}\\]\nSome things we look at:\n\nThe distribution’s support for each variable matches the variable’s datatype.\nThe datatype of each distribution parameter is adequate (for example, here \\(\\theta_1 (\\sin(\\theta_2 i + \\theta_3) + 1) \\ge 0\\)).\nThe model is sufficiently flexible to capture the pattern observed in EDA.\n\nSee the page on the step-by-step construction of a GLM.\nMany answers possible for model mis-specification. For example, the Poisson distribution will force the mean and variance to be equal, which often does not hold in practice (in this specific example, this might be especially problematic for values \\(i\\) where \\(\\sin(\\theta_2 i + \\theta_3) + 1\\) is zero or close to zero).",
    "crumbs": [
      "Quiz 1",
      "Practice questions"
    ]
  },
  {
    "objectID": "w07_quiz1/topic02_practice_questions.html#ppl-based-prediction",
    "href": "w07_quiz1/topic02_practice_questions.html#ppl-based-prediction",
    "title": "Practice questions",
    "section": "PPL-based prediction",
    "text": "PPL-based prediction\nRecall the model covered in class to perform Bayesian linear regression on galaxy distances and velocities:\nregression = function() {\n  slope = simulate(Norm(0, 1))\n  sd = simulate(Exp(10))\n  for (i in 1:nrow(df)) { \n    distance = df[i, \"distance\"]\n    velocity = df[i, \"velocity\"]\n    observe(velocity, Norm(distance * slope, sd))\n  }\n  c(slope, sd)\n}\n\nposterior(regression, 1000)\nHow would you modify this code to predict the velocity of a galaxy at distance 1.5?\n\n\n\n\n\n\nClick for choices\n\n\n\n\n\n\nChange the line c(slope, sd) to observe(Norm(1.5 * slope, sd))\nChange the line c(slope, sd) to observe(Norm(1.5, sd))\nChange the line c(slope, sd) to simulate(Norm(1.5 * slope, sd))\nChange the line c(slope, sd) to simulate(Norm(1.5, sd))\nNone of the above.\n\n\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nChange the line c(slope, sd) to simulate(Norm(1.5 * slope, sd))\nSee exercise 4.",
    "crumbs": [
      "Quiz 1",
      "Practice questions"
    ]
  },
  {
    "objectID": "w07_quiz1/topic02_practice_questions.html#asymptotics",
    "href": "w07_quiz1/topic02_practice_questions.html#asymptotics",
    "title": "Practice questions",
    "section": "Asymptotics",
    "text": "Asymptotics\nLet \\(x\\) denote a parameter of interest. The true value is \\(x^*\\). We have a Bayesian model with \\(X\\) and \\(Y\\), and we approximate the posterior mean using SNIS. Let \\(\\hat G_M\\) denote the output of SNIS with test function \\(g(x) = x\\) and \\(M\\) iterations.\nWhere does the following limit converge to? \\[\\lim_{M\\to\\infty} \\hat G_M = \\;?\\]\n\n\n\n\n\n\nClick for choices\n\n\n\n\n\n\n\\(x^*\\)\n\\(\\mathbb{E}[X]\\)\n\\(\\mathbb{E}[X | Y = y]\\)\n\\(0\\)\nNone of the above.\n\n\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nAs discussed in the convergence of SNIS, the correct answer is \\(\\mathbb{E}[X | Y = y]\\).",
    "crumbs": [
      "Quiz 1",
      "Practice questions"
    ]
  },
  {
    "objectID": "w07_quiz1/topic02_practice_questions.html#beta-binomial-conjugacy",
    "href": "w07_quiz1/topic02_practice_questions.html#beta-binomial-conjugacy",
    "title": "Practice questions",
    "section": "Beta-binomial conjugacy",
    "text": "Beta-binomial conjugacy\nRecall that a binomial likelihood has the following PMF: \\[p(y | \\theta) = \\binom{n}{y} \\theta^y (1-\\theta)^{n-y},\\] where \\(n\\) is the observed number of trials (e.g. launches) and \\(y\\) is the number of observed successes.\nWe place a Beta prior on \\(\\theta\\), with hyper-parameters \\(\\alpha = 1\\) and \\(\\beta = 2\\). Recall that beta densities have the following form: \\[b_{\\alpha, \\beta}(\\theta) = \\frac{1}{Z(\\alpha, \\beta)} \\theta^{\\alpha - 1} (1-\\theta)^{\\beta - 1},\\] where \\(Z(\\alpha, \\beta)\\) is a normalization constant (i.e. \\(Z(\\alpha, \\beta)\\) depends on \\(\\alpha, \\beta\\) but not \\(\\theta\\)).\nShow that the posterior on \\(\\theta\\) given that we observed 3 successful launches out of 3 (\\(y = 3, n = 3\\)) has a beta density, i.e., that \\[f_{\\theta|Y = 3}(\\theta) = b_{\\alpha', \\beta'}(\\theta),\\] for some \\(\\alpha', \\beta'\\).\nWhat is \\(\\alpha'\\) and \\(\\beta'\\)?\n\n\n\n\n\n\nClick for choices\n\n\n\n\n\n\n\\(\\alpha' = 2, \\beta' = 4\\)\n\\(\\alpha' = 3, \\beta' = 0\\)\n\\(\\alpha' = 3, \\beta' = 2\\)\n\\(\\alpha' = 4, \\beta' = 2\\)\nNone of the above.\n\n\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nWe use the same technique as in Exercise 5, Q1, which is that it is enough to show that \\(f_{\\theta|Y}(\\theta) \\propto b_{\\alpha', \\beta'}(\\theta)\\) as this will imply they are equal.\nBy Bayes rule: \\[\\begin{align*}\nf_{\\theta|Y}(\\theta) &\\propto b_{\\alpha, \\beta}(\\theta) p(y | \\theta) \\\\\n&= \\left( \\frac{1}{Z(\\alpha, \\beta)} \\theta^{\\alpha - 1} (1-\\theta)^{\\beta - 1} \\right)  \\left(\\binom{n}{y} \\theta^y (1-\\theta)^{n-y}\\right) \\\\\n&\\propto \\theta^{(\\alpha + y) - 1} (1-\\theta)^{(\\beta + n - y) - 1} \\\\\n&= b_{\\alpha + y, \\beta + n-y},\n\\end{align*}\\] hence \\(\\alpha' = \\alpha + y\\) and \\(\\beta' = \\beta + (n-y)\\).\nIn our example here, \\(\\alpha = 1\\), \\(\\beta = 2\\), \\(y = 3, n = 3\\), so \\(\\alpha' = 1 + 3 = 4\\) and \\(\\beta' = 2 + (3-3) = 2\\).",
    "crumbs": [
      "Quiz 1",
      "Practice questions"
    ]
  },
  {
    "objectID": "w07_quiz1/topic02_practice_questions.html#credible-intervals",
    "href": "w07_quiz1/topic02_practice_questions.html#credible-intervals",
    "title": "Practice questions",
    "section": "Credible intervals",
    "text": "Credible intervals\nYou computed two credible intervals based on the same continuous, unimodal posterior distribution: one is a 90% quantile-based interval, the other, a highest density interval (HDI). Which of the intervals is shortest?\n\n\n\n\n\n\nClick for choices\n\n\n\n\n\n\nThe quantile-based interval.\nThe HDI interval.\nIt cannot be established with more information.\nThey will always be of equal length.\n\n\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nThe HDI interval, by definition.",
    "crumbs": [
      "Quiz 1",
      "Practice questions"
    ]
  },
  {
    "objectID": "w07_quiz1/topic02_practice_questions.html#decision-theory",
    "href": "w07_quiz1/topic02_practice_questions.html#decision-theory",
    "title": "Practice questions",
    "section": "Decision theory",
    "text": "Decision theory\n\nA patient is suspected to have a certain disease.\nThere is a drastic treatment, which always cures the disease, but costs 1M (in terms of medical facility use and/or physical toll).\nIf left uncured, the disease will lead to death which a policy-maker might model as a loss of 10M.\n\nLet \\(D\\) denote the indicator function on the disease, and \\(Y\\), data from diagnostic. You posterior gives \\(\\mathbb{E}[D | Y = y] = 0.2\\).\nWhat would a Bayesian recommend?\n\n\n\n\n\n\nClick for choices\n\n\n\n\n\n\nPick \\(a=1\\) with probability 0.2.\nPick \\(a=0\\) with probability 0.2.\nPick \\(a=1\\).\nPick \\(a=0\\).\nPick either \\(a=1\\) or \\(a=0\\) as they incur the same loss.\n\n\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nLet \\(a\\) denote the indicator that the treatment is used. We get the following losses:\n\n\n\n\n\\(a = 0\\)\n\\(a = 1\\)\n\n\n\n\n\\(d = 0\\)\n\\(0\\)\n\\(1M\\)\n\n\n\\(d = 1\\)\n\\(10M\\)\n\\(1M\\)\n\n\n\nWe compute the expected losses for the two possible actions:\n\\[\\mathbb{E}[L(0, D) | Y = y] = (0.2) (10M) = 2M\\]\nand\n\\[\\mathbb{E}[L(1, D) | Y = y] = 1M\\]\nHence the action that minimizes the loss is \\(a = 1\\), i.e. to use the treatment.",
    "crumbs": [
      "Quiz 1",
      "Practice questions"
    ]
  },
  {
    "objectID": "w07_quiz1/topic02_practice_questions.html#posterior-expectation",
    "href": "w07_quiz1/topic02_practice_questions.html#posterior-expectation",
    "title": "Practice questions",
    "section": "Posterior expectation",
    "text": "Posterior expectation\nA discrete random variable \\(X\\) can take values \\(-1, 2, 5\\). Suppose its posterior PMF given \\(Y\\) is proportional to: \\[\\gamma \\propto (0.2, 0.1, 0.2).\\] Compute \\(\\mathbb{E}[X^2 | Y]\\).\n\n\n\n\n\n\nClick for choices\n\n\n\n\n\n\n56/5 = 11.2\n23/25 = 0.92\n19/25 = 0.76\n17/25 = 0.68\nNone of the above.\n\n\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nWe first compute \\(\\pi\\) by normalization: \\[\\pi = \\frac{\\gamma}{0.2 + 0.1 + 0.2} = (2/5, 1/5, 2/5).\\]\nWe can then compute the conditional expectation using LOTUS: \\[\\mathbb{E}[X^2 | Y] = (-1)^2 (2/5) + 2^2 (1/5) + 5^2 (2/5) = 56/5.\\]\nSee Exercise 1 and expectations review.",
    "crumbs": [
      "Quiz 1",
      "Practice questions"
    ]
  },
  {
    "objectID": "w07_quiz1/topic02_practice_questions.html#non-standard-loss-minimization",
    "href": "w07_quiz1/topic02_practice_questions.html#non-standard-loss-minimization",
    "title": "Practice questions",
    "section": "Non-standard loss minimization",
    "text": "Non-standard loss minimization\nDerive the Bayes estimator for the loss \\(L(a, x) = x^2 - 10ax + a^2\\). The posterior mean is \\(\\mathbb{E}[X | Y = y] = 1\\) and \\(\\operatorname{Var}[X | Y = y] = 1\\).\n\n\n\n\n\n\nClick for choices\n\n\n\n\n\n\n1\n5\n10\n15\nNone of the above\n\n\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nWe proceed as in the square loss example:\n\\[\n\\begin{aligned}\n\\delta_{\\text{B}}(Y) &= \\operatorname{arg\\,min}\\{ \\mathbb{E}[L(a, X) | Y] : a \\in A \\} \\\\\n&=  \\operatorname{arg\\,min}\\{ \\mathbb{E}[ X^2 - 10aX + a^2 | Y] : a \\in A \\} \\\\\n&=  \\operatorname{arg\\,min}\\{ - 10 a\\mathbb{E}[X | Y] + a^2: a \\in A \\}.\n\\end{aligned}\n\\]\nComputing the derivative with respect to \\(a\\) and setting to zero: \\[-10 \\mathbb{E}[X | Y] + 2a = 0,\\] hence the Bayes estimate or Bayes action is \\(a = 10\\mathbb{E}[X | Y] / 2 = 5\\).",
    "crumbs": [
      "Quiz 1",
      "Practice questions"
    ]
  },
  {
    "objectID": "w07_quiz1/topic02_practice_questions.html#notation",
    "href": "w07_quiz1/topic02_practice_questions.html#notation",
    "title": "Practice questions",
    "section": "Notation",
    "text": "Notation\nWhich of the following statement(s), if any, follow(s) the notation used in class (i.e., make sense)?\n\n\\(\\mathbb{P}(X) \\in [0, 1]\\), where \\(\\mathbb{P}\\) is a probability function and \\(X\\) is a random variable.\n\\(p(x) \\in [0, 1]\\), where \\(p\\) is a PMF and \\(x\\) is a realization.\n\\(\\mathbb{P}(x) \\in [0, 1]\\), where \\(\\mathbb{P}\\) is a PMF, and \\(x\\) is a realization.\n\\((X = 1) \\subset S\\), where \\(X\\) is a random variable and \\(S\\) is the sample space.\n\\(\\mathbb{E}[X + x] = \\mathbb{E}[X] + x\\), where \\(x\\) is a realization\n\\(f(x) \\in [0, 1]\\), where \\(f\\) is a density.\n\n\n\n\n\n\n\nClick for choices\n\n\n\n\n\n\nd, e, f\na, b, d, e, f\na, b, d, e\nb, d, e\nNone of the above.\n\n\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nCorrect choice is: b, d, e\n\na, c are incorrect since \\(\\mathbb{P}\\) takes events as input, and \\(\\mathbb{P}\\) is not a PMF but a probability function.\nf is incorrect since a density can be greater than one.",
    "crumbs": [
      "Quiz 1",
      "Practice questions"
    ]
  },
  {
    "objectID": "w07_quiz1/topic02_practice_questions.html#optimality-of-bayes-estimator",
    "href": "w07_quiz1/topic02_practice_questions.html#optimality-of-bayes-estimator",
    "title": "Practice questions",
    "section": "Optimality of Bayes estimator",
    "text": "Optimality of Bayes estimator\nState the theoretical guarantee that a Bayes estimator \\(\\delta_{\\text{B}}\\) has compared to another estimator \\(\\delta_{\\text{o}}\\) in terms of average loss.\n\n\n\n\n\n\nClick for choices\n\n\n\n\n\n\n\\(\\delta_{\\text{B}}\\le \\delta_{\\text{o}}\\)\n\\(\\mathbb{E}[\\delta_{\\text{B}}] \\le \\mathbb{E}[\\delta_{\\text{B}}]\\)\n\\(\\mathbb{E}[L(\\delta_{\\text{B}}(Y), X) | Y] \\le \\mathbb{E}[L(\\delta_{\\text{o}}(Y), X) | Y]\\)\n\\(\\mathbb{E}[L(\\delta_{\\text{B}}(Y), X) | Y] \\le 0\\)\nNone of the above.\n\n\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nNone of the above. The correct inequality is: \\[\\mathbb{E}[L(\\delta_{\\text{B}}(Y), X)] \\le \\mathbb{E}[L(\\delta_{\\text{o}}(Y), X)].\\]",
    "crumbs": [
      "Quiz 1",
      "Practice questions"
    ]
  },
  {
    "objectID": "w07_quiz1/topic02_practice_questions.html#bias-of-mc-methods",
    "href": "w07_quiz1/topic02_practice_questions.html#bias-of-mc-methods",
    "title": "Practice questions",
    "section": "Bias of MC methods",
    "text": "Bias of MC methods\nDefine formally the notion of bias of a Monte Carlo method.\nWe have covered two Monte Carlo methods: simple Monte Carlo and SNIS. Which one has the smallest bias?\n\n\n\n\n\n\nClick for choices\n\n\n\n\n\n\nSNIS has smaller bias.\nSimple Monte Carlo has smaller bias.\nThey cannot be compared in general.\nAlways equal.\n\n\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nThe bias is defined as \\(\\hat G_M - g^*\\).\nFrom our discussion on Monte Carlo convergence rate, simple Monte Carlo has a bias of zero, while SNIS is biased.",
    "crumbs": [
      "Quiz 1",
      "Practice questions"
    ]
  },
  {
    "objectID": "w07_quiz1/topic03_logistics.html",
    "href": "w07_quiz1/topic03_logistics.html",
    "title": "Logistics",
    "section": "",
    "text": "You only need pencil, eraser, student or government id.\nThe quiz is closed book (in particular, no electronics, including simple calculator permitted).",
    "crumbs": [
      "Quiz 1",
      "Logistics"
    ]
  },
  {
    "objectID": "w07_quiz1/topic03_logistics.html#what-to-bring",
    "href": "w07_quiz1/topic03_logistics.html#what-to-bring",
    "title": "Logistics",
    "section": "",
    "text": "You only need pencil, eraser, student or government id.\nThe quiz is closed book (in particular, no electronics, including simple calculator permitted).",
    "crumbs": [
      "Quiz 1",
      "Logistics"
    ]
  },
  {
    "objectID": "w07_quiz1/topic03_logistics.html#time",
    "href": "w07_quiz1/topic03_logistics.html#time",
    "title": "Logistics",
    "section": "Time",
    "text": "Time\nWe will start promptly at 9:30 (please arrive on time) and end at 10:45 (75 minutes).",
    "crumbs": [
      "Quiz 1",
      "Logistics"
    ]
  },
  {
    "objectID": "w07_quiz1/topic03_logistics.html#distribution-reference",
    "href": "w07_quiz1/topic03_logistics.html#distribution-reference",
    "title": "Logistics",
    "section": "Distribution reference",
    "text": "Distribution reference\nThe cover page of the exam will have the following table.\n\n\n\n\n\n\n\n\nName\nAbbreviation\nParameters\n\n\n\n\nBernoulli\n\\({\\mathrm{Bern}}(p)\\)\nSuccess probability \\(p \\in [0, 1]\\)\n\n\nBinomial\n\\({\\mathrm{Binom}}(n, p)\\)\nNumber of trials \\(n \\in \\mathbb{N}\\), success probability \\(p \\in [0, 1]\\)\n\n\nUniform\n\\({\\mathrm{Unif}}(a, b)\\)\nLeft and right bounds, \\(a &lt; b\\)\n\n\nNormal\n\\(\\mathcal{N}(\\mu, \\sigma)\\)\nMean \\(\\mu \\in \\mathbb{R}\\) and standard deviation \\(\\sigma &gt; 0\\)\n\n\n\n\\(\\mathcal{N}(\\mu, \\sigma^2)\\)\nMean \\(\\mu \\in \\mathbb{R}\\) and variance \\(\\sigma^2 &gt; 0\\)\n\n\n\n\\(\\mathcal{N}(\\mu, \\tau)\\)\nMean \\(\\mu \\in \\mathbb{R}\\) and precision \\(\\tau = 1/\\sigma^2 &gt; 0\\)\n\n\nExponential\n\\({\\mathrm{Exp}}(\\lambda)\\)\nRate \\(\\lambda\\) (\\(=1/\\)mean)\n\n\nBeta\n\\({\\mathrm{Beta}}(\\alpha, \\beta)\\)\nShape parameters \\(\\alpha &gt; 0\\) and \\(\\beta &gt; 0\\)\n\n\n\n\\({\\mathrm{Beta}}(\\mu, s)\\)\nMean parameter \\(\\mu \\in (0, 1)\\) and concentration \\(s&gt;0\\)",
    "crumbs": [
      "Quiz 1",
      "Logistics"
    ]
  },
  {
    "objectID": "w00_intro/topic04_examples.html",
    "href": "w00_intro/topic04_examples.html",
    "title": "Examples",
    "section": "",
    "text": "Origins of life/cancer/language and characterization of their respective evolutionary processes\nModelling high-throughput genomics data (single-cell sequencing, CRISPR-CAS9, ultra-deep, expression)\nBlack-hole imaging\n“Classical” tasks: classification, regression, clustering, etc\nA/B testing and Bayesian optimization\nDetermining fate of the universe from cosmic microwave background",
    "crumbs": [
      "Introduction",
      "Examples"
    ]
  },
  {
    "objectID": "w00_intro/topic04_examples.html#some-examples-of-bayesian-inference",
    "href": "w00_intro/topic04_examples.html#some-examples-of-bayesian-inference",
    "title": "Examples",
    "section": "",
    "text": "Origins of life/cancer/language and characterization of their respective evolutionary processes\nModelling high-throughput genomics data (single-cell sequencing, CRISPR-CAS9, ultra-deep, expression)\nBlack-hole imaging\n“Classical” tasks: classification, regression, clustering, etc\nA/B testing and Bayesian optimization\nDetermining fate of the universe from cosmic microwave background",
    "crumbs": [
      "Introduction",
      "Examples"
    ]
  },
  {
    "objectID": "w00_intro/topic04_examples.html#to-read-more-on-example-applications",
    "href": "w00_intro/topic04_examples.html#to-read-more-on-example-applications",
    "title": "Examples",
    "section": "To read more on example applications",
    "text": "To read more on example applications\nFor an entertaining popular science book on various applications of Bayesian statistics, have a look at The Theory That Would Not Die. Sharon Bertsch McGrayne. PDF available via UBC library.",
    "crumbs": [
      "Introduction",
      "Examples"
    ]
  },
  {
    "objectID": "w00_intro/topic01_why.html",
    "href": "w00_intro/topic01_why.html",
    "title": "Why?",
    "section": "",
    "text": "Address most data analysis issues (missing data, non-standard data types, non-iid, weird loss functions, adding expert knowledge, …)\n\nBayesian analysis: address those in a (semi) automated fashion / principled framework (“reductionist”)\n\nReductionism can be bad or good (main con of reductionism is computational)\n\nFrequentist statistics: every problem is a new problem\n\nImplementation complexity\n\nEfficient in analyst’s time (thanks to PPLs)\nHarder to scale computationally\n\\(\\Longrightarrow\\) shines on small data problems (there a much more of those than the “big data” hype would like you to think)\n\nStatistical properties\n\nOptimal if the model is well-specified\nSub-optimal in certain cases when the model is mis-specified\n\nThankfully the modelling flexibility makes it easier to build better models\nImportant to make model checks",
    "crumbs": [
      "Introduction",
      "Why?"
    ]
  },
  {
    "objectID": "w00_intro/topic01_why.html#bayesian-analysis-pros-and-cons",
    "href": "w00_intro/topic01_why.html#bayesian-analysis-pros-and-cons",
    "title": "Why?",
    "section": "",
    "text": "Address most data analysis issues (missing data, non-standard data types, non-iid, weird loss functions, adding expert knowledge, …)\n\nBayesian analysis: address those in a (semi) automated fashion / principled framework (“reductionist”)\n\nReductionism can be bad or good (main con of reductionism is computational)\n\nFrequentist statistics: every problem is a new problem\n\nImplementation complexity\n\nEfficient in analyst’s time (thanks to PPLs)\nHarder to scale computationally\n\\(\\Longrightarrow\\) shines on small data problems (there a much more of those than the “big data” hype would like you to think)\n\nStatistical properties\n\nOptimal if the model is well-specified\nSub-optimal in certain cases when the model is mis-specified\n\nThankfully the modelling flexibility makes it easier to build better models\nImportant to make model checks",
    "crumbs": [
      "Introduction",
      "Why?"
    ]
  },
  {
    "objectID": "w00_intro/topic01_why.html#week-2-example",
    "href": "w00_intro/topic01_why.html#week-2-example",
    "title": "Why?",
    "section": "Week 2 example",
    "text": "Week 2 example\n\nWould you rather get strapped to…\n\n“shiny rocket”: 1 success, 0 failures\n“rugged rocket”: 98 successes, 2 failures",
    "crumbs": [
      "Introduction",
      "Why?"
    ]
  },
  {
    "objectID": "w00_intro/topic01_why.html#paradox",
    "href": "w00_intro/topic01_why.html#paradox",
    "title": "Why?",
    "section": "Paradox?",
    "text": "Paradox?\n\nMaximum likelihood point estimates:\n\n“shiny rocket”: 100% success rate (1 success, 0 failures)\n“rugged rocket”: 98% success rate (98 successes, 2 failures)\n\nWhat is missing?",
    "crumbs": [
      "Introduction",
      "Why?"
    ]
  },
  {
    "objectID": "w00_intro/topic01_why.html#uncertainty-estimates",
    "href": "w00_intro/topic01_why.html#uncertainty-estimates",
    "title": "Why?",
    "section": "Uncertainty estimates",
    "text": "Uncertainty estimates\n\nTake-home message:\n\nPoint estimates are often insufficient, and can be very dangerous\nWe want some measure of uncertainty\n\nBayesian inference provides one way to build uncertainty measures\n\nBayesian measures of uncertainty we will describe: credible intervals\n\nAlternatives exist:\n\nConfidence intervals, from frequentist statistics\n“End product” looks similar, but very different in interpretation and construction",
    "crumbs": [
      "Introduction",
      "Why?"
    ]
  },
  {
    "objectID": "w00_intro/topic01_why.html#uncertainty-will-not-go-away",
    "href": "w00_intro/topic01_why.html#uncertainty-will-not-go-away",
    "title": "Why?",
    "section": "Uncertainty will not go away",
    "text": "Uncertainty will not go away\n\n\n\n\nJust collect more data??\n\nJust launch more rockets and wait? Collecting more data might be too costly/dangerous/unethical.\nIn some cases the data is just “gone”, i.e. we will never be able to collect more after a point (e.g.: phylogenetic tree inference)",
    "crumbs": [
      "Introduction",
      "Why?"
    ]
  },
  {
    "objectID": "w00_intro/topic05_history.html",
    "href": "w00_intro/topic05_history.html",
    "title": "A bit of history",
    "section": "",
    "text": "Bayesian statistics: short historical overview\n\nPrecursors:\n\nThomas Bayes (1702–1761); first special case of Bayes rule, published posthumously in 1763\nPierre-Simon Laplace (1749–1827): generalizations, more applications\nIdea temporarily buried by frequentists in the 1920’s\n\nSecond wave: theoretical foundations\n\nBruno de Finetti, 1930: partial justification for exchangeable data\nStein paradox and crisis in frequentist statistics (1955)\nObjective-Subjective Bayes ‘divide’\n\nPopularization of Subjective Bayes by Leonard Savage in the ’50s\nInception of the Objective Bayes school: Harold Jeffreys (1939), further development by José-Miguel Bernardo (1979)\n\n\nThird wave: coming of age as a versatile data analysis tool\n\nInception: Nicholas Metropolis, Arianna Rosenbluth, Marshall Rosenbluth, Augusta Teller and Edward Teller (1953), Metropolis-Hastings algorithm in the physics literature\nIntroduction to Bayesian statistics: Stuart Geman and Donald Geman (1984)\nFirst major PPL: WinBUGS, David Lunn, Andrew Thomas, Nicky Best, David Spiegelhalter (2000)",
    "crumbs": [
      "Introduction",
      "A bit of history"
    ]
  },
  {
    "objectID": "w10_modelling/topic07_mixtures.html",
    "href": "w10_modelling/topic07_mixtures.html",
    "title": "Mixtures",
    "section": "",
    "text": "Mathematical definition of mixtures\nImplementation in Stan\n\n\n\n\nMixtures are motivated by cases where you believe the data is composed of two or more distinct sub-populations.\nOne use of mixtures is to figure out, for each data point, from which sub-population it comes from.\nBeyond the sub-population example, mixtures are also used to get more flexible distribution families.",
    "crumbs": [
      "Modelling techniques",
      "Mixtures"
    ]
  },
  {
    "objectID": "w10_modelling/topic07_mixtures.html#outline",
    "href": "w10_modelling/topic07_mixtures.html#outline",
    "title": "Mixtures",
    "section": "",
    "text": "Mathematical definition of mixtures\nImplementation in Stan\n\n\n\n\nMixtures are motivated by cases where you believe the data is composed of two or more distinct sub-populations.\nOne use of mixtures is to figure out, for each data point, from which sub-population it comes from.\nBeyond the sub-population example, mixtures are also used to get more flexible distribution families.",
    "crumbs": [
      "Modelling techniques",
      "Mixtures"
    ]
  },
  {
    "objectID": "w10_modelling/topic07_mixtures.html#example-detection-of-random-guessing",
    "href": "w10_modelling/topic07_mixtures.html#example-detection-of-random-guessing",
    "title": "Mixtures",
    "section": "Example: detection of random guessing",
    "text": "Example: detection of random guessing\n\nWe look at a dataset from Albert and Hu, 2020.\nWe have scores for 30 students on a test.\nThe test has 20 questions.\nAll questions are TRUE/FALSE with same weight.\nEvery student attempted every question.\n\n\nsuppressPackageStartupMessages(require(rstan))\nsuppressPackageStartupMessages(require(ggplot2))\nsuppressPackageStartupMessages(require(dplyr))\nsuppressPackageStartupMessages(require(bayesplot))\n\ndata = read.csv(url(\"https://github.com/UBC-Stat-ML/web447/raw/main/data/ScoreData.csv\"))\nrmarkdown::paged_table(data)\n\n\n  \n\n\n\n\nplot(data$Score,\n      ylim = c(0, 20),\n      xlab = \"Student index\", \n      ylab = \"Score (out of 20 questions)\")\n\n\n\n\n\n\n\n\n\nNotice a cluster of students with scores around 50%,\nand a second cluster with much better scores.\nHypothesis: some students answered completely at random.",
    "crumbs": [
      "Modelling techniques",
      "Mixtures"
    ]
  },
  {
    "objectID": "w10_modelling/topic07_mixtures.html#simplified-problem",
    "href": "w10_modelling/topic07_mixtures.html#simplified-problem",
    "title": "Mixtures",
    "section": "Simplified problem",
    "text": "Simplified problem\n\nSuppose first that we have the score for only one student.\nE.g.: first one, who got 9/20.\nYou want a model to put a probability on: “is this student is guessing at random?”\n\nReal application: analysis of clicker scores where grade is participation-based.\n\n\n\nMathematical model\n\nStart with the data: \\(Y\\), an integer between 0 and 20.\n\nQuestion: what is a good choice of likelihood?\n\nThe parameters of the likelihood depend on whether the student is guessing or not\n\nLet \\(G\\) denote an indicator on guessing, i.e.:\n\n\\(G = 1\\): guessing,\n\\(G = 0\\): not guessing, i.e., trying at the best of their ability.\n\n\n\nQuestion: what is a good choice of prior on \\(G\\)?\n\nThe likelihood given \\(G = 1\\) (guessing) is easy: \\[Y \\sim {\\mathrm{Binom}}(20, 0.5).\\]\nThe likelihood given \\(G = 0\\) (trying at the best of their ability) is slightly trickier:\n\nwhat is “the best of their ability”?\nBayesian recipe: treat it as an unknown ability parameters \\(A\\)\n\n\nFull model:\n\\[\\begin{align*}\nA &\\sim {\\mathrm{Unif}}(0, 1) \\\\\nG &\\sim {\\mathrm{Bern}}(1/3) \\\\\nY &\\sim {\\mathrm{Binom}}(20, 0.5 G + A(1-G)).\n\\end{align*}\\]\n\n\nMarginalization\n\nStan does not support discrete latent random variables like \\(G\\)\nTherefore we use Rao-Blackwellization:\n\nJoint distribution before Rao-Blackwellization: \\[\\gamma(a, g, y) = p(a) p(g) p(y|a, g).\\]\nJoint distribution after Rao-Blackwellization: \\[\\begin{align*}\n\\gamma(a, y) &= \\sum_{g=0}^1 \\gamma(a, g, y) \\\\\n&= \\sum_{g=0}^1 p(a) p(g) p(y|a, g) \\\\\n&= p(a) \\sum_{g=0}^1 p(g) p(y|a, g) \\\\\n&= \\underbrace{p(a)}_\\text{(a)} \\underbrace{\\left( \\underbrace{p(g=1) p(y|a, g=1)}_\\text{(b)} + \\underbrace{p(g=0) p(y|a, g=0)}_\\text{(c)} \\right)}_\\text{(d)}.\n\\end{align*}\\]\n\n\nQuestion: match-up (a)-(d) with the labelled statements in the Stan code below.\n\ndata {\n  int&lt;lower=0, upper=20&gt; score;\n}\n\nparameters {\n  real&lt;lower=0, upper=1&gt; ability;\n}\n\ntransformed parameters {\n1  real complete_likelihood_guessing\n    = 1.0/3 * exp(binomial_lpmf(score | 20, 0.5)); \n2  real complete_likelihood_non_guessing\n    = 2.0/3 * exp(binomial_lpmf(score | 20, ability)); \n}\n\nmodel {\n3  ability ~ uniform(0, 1);\n4  target +=\n    log(complete_likelihood_guessing + complete_likelihood_non_guessing); \n}\n\ngenerated quantities {\n5  real guessing_probability =\n    complete_likelihood_guessing / (complete_likelihood_guessing + complete_likelihood_non_guessing);\n}\n\n\n1\n\nStan statement 1 (used in clicker question)\n\n2\n\nStan statement 2\n\n3\n\nStan statement 3\n\n4\n\nStan statement 4\n\n5\n\nStan statement 5\n\n\n\n\n\n\nRe-instatiation\n\nWe care about the posterior mean of \\(G\\), \\(\\mathbb{E}[G | Y = y]\\)…\n…but we have marginalized \\(G\\) 🙁\nTo compute the posterior mean, we first use the law of total expectation: \\[\\mathbb{E}[G | Y] = \\mathbb{E}[{\\color{red} \\mathbb{E}[G | A, Y]} | Y],\\]\nLet us look at the part in red, and see how we can compute it at every MCMC iteration based on the current value of \\(A^{(m)}\\)…\n\nUsing the fact the mean of a Bernoulli is the probability that it takes value 1: \\[\\mathbb{E}[G | A, Y] = \\mathbb{P}(G = 1 | A, Y).\\]\nBy Bayes rule (noting that a conditional probability like \\(\\mathbb{P}(\\cdot | A)\\) is a probability): \\[\\mathbb{P}(G = 1 | A, Y = y) = \\frac{\\mathbb{P}(G = 1, Y = y | A)}{\\mathbb{P}(G = 0, Y = y | A) + \\mathbb{P}(G = 1, Y = y | A)}.\\]\nThis is what gets computed in statement ⑤ in the above Stan code.\n\nNow, recall from the LLN for Markov chains gives us condition so that for a test function \\(h\\), we have that the Monte Carlo samples, \\(A^{(1)}, A^{(2)}, \\dots, A^{(M)}\\) satisfy, \\[\\frac{1}{M} \\sum_{m=1}^M h(A^{(m)}) \\to \\mathbb{E}[h(A) | Y = y],\\] with probability one.\nTherefore from taking \\(h(A^{(m)}) = \\mathbb{P}(G = 1 | A^{(m)}, Y = y)\\), we obtain, \\[\\frac{1}{M} \\sum_{m=1}^M \\mathbb{P}(G = 1 | A^{(m)}, Y = y) \\to \\mathbb{E}[G | Y = y],\\] with probability one.\n\n\n\nTesting the model\nProbably guessing: feeding a score of 9 out of 20, we get:\n\nfit = sampling(\n  students_guessing_simplified,\n  seed = 1,\n  refresh = 0,\n  data = list(score = 9)       \n)\n\nmean(extract(fit)$guessing_probability)\n\n[1] 0.613919\n\n\nProbably not guessing: feeding a score of 19 out of 20, we get:\n\nfit = sampling(\n  students_guessing_simplified,\n  seed = 1,\n  refresh = 0,\n  data = list(score = 19)      \n)\n\nmean(extract(fit)$guessing_probability)\n\n[1] 9.302208e-05\n\n\nWhat is the key limitation of this approach?\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nThe student could have a low score because the questions were too difficult!\nWith some assumptions (unimodality of the students abilities) we can avoid this limitation.",
    "crumbs": [
      "Modelling techniques",
      "Mixtures"
    ]
  },
  {
    "objectID": "w10_modelling/topic07_mixtures.html#full-model-getting-rid-of-key-limitation",
    "href": "w10_modelling/topic07_mixtures.html#full-model-getting-rid-of-key-limitation",
    "title": "Mixtures",
    "section": "Full model: getting rid of key limitation",
    "text": "Full model: getting rid of key limitation\n\nIdea: use a hierarchical model!\nRandom variables that are student-specific:\n\n\\(Y_i \\sim {\\mathrm{Binom}}(20, 0.5 G_i + A_i (1 - G_i)).\\)\n\\(A_i \\sim {\\mathrm{Beta}}(\\mu, S)\\)\n\\(G_i \\sim {\\mathrm{Bern}}(F)\\)\n\nRandom variables that are global:\n\nPopulation fraction that are guessing: \\(F \\sim {\\mathrm{Unif}}(0, 1)\\),\nPopulation parameters for the abilities:\n\n\\(\\mu \\sim {\\mathrm{Unif}}(0, 1)\\)\n\\(S \\sim {\\mathrm{Exp}}(1/100)\\).\n\n\nStan implementation:\n\n\ndata {\n  int n_students; \n  array[n_students] int&lt;lower=0, upper=20&gt; scores; \n}\n\nparameters {\n  real&lt;lower=0, upper=1&gt; fraction_guessing;\n  real&lt;lower=0, upper=1&gt; non_guessing_population_mean;\n  real&lt;lower=0&gt; non_guessing_population_spread;\n  vector&lt;lower=0, upper=1&gt;[n_students] abilities;\n}\n\ntransformed parameters {\n  vector[n_students] complete_loglikelihood_guessing; \n  vector[n_students] complete_loglikelihood_non_guessing; \n  for (i in 1:n_students) {\n    complete_loglikelihood_guessing[i]     \n      = log(fraction_guessing) + binomial_lpmf(scores[i] | 20, 0.5); \n    complete_loglikelihood_non_guessing[i] \n      = log1p(-fraction_guessing) + binomial_lpmf(scores[i] | 20, abilities[i]);\n  }\n}\n\nmodel {\n  fraction_guessing ~ uniform(0, 1);\n  non_guessing_population_mean ~ uniform(0, 1);\n  non_guessing_population_spread ~ exponential(1.0/100);\n  for (i in 1:n_students) {\n    abilities[i] ~ beta_proportion(non_guessing_population_mean, non_guessing_population_spread);\n    target += \n      log_sum_exp(complete_loglikelihood_guessing[i], complete_loglikelihood_non_guessing[i]);\n  }\n}\n\ngenerated quantities {\n  vector[n_students] guessing_probabilities = inv_logit(complete_loglikelihood_guessing - complete_loglikelihood_non_guessing);\n  real predictive_score_non_guessing = 20 * beta_proportion_rng(non_guessing_population_mean, non_guessing_population_spread);\n}\n\n\nfit = sampling(\n  students_guessing,\n  seed = 1,\n  refresh = 0,\n  data = list(\n            n_students = length(data$Score),\n            scores = data$Score\n          )                  \n)\n\nCredible intervals and posterior medians: 90% (thick lines) and 50% (thin lines)\n\nmcmc_intervals(fit, regex_pars = c(\"guessing_probabilities.*\")) + \n  theme_minimal() + \n  scale_x_continuous(limits = c(0, 1)) \n\n\n\n\n\n\n\n\nPredictive score distribution: for the non guessing sub-population…\n\nmcmc_areas_ridges(fit, regex_pars = c(\"predictive_score_non_guessing*\")) + \n  theme_minimal() + \n  scale_x_continuous(limits = c(0, 20))",
    "crumbs": [
      "Modelling techniques",
      "Mixtures"
    ]
  },
  {
    "objectID": "w10_modelling/topic06_rao.html",
    "href": "w10_modelling/topic06_rao.html",
    "title": "Rao-Blackwellization",
    "section": "",
    "text": "Rao-Blackwellization: what and why\nImplementing Rao-Blackwellization in Stan\nMathematical underpinnings\n\n\n\n\nRao-Blackwellization is an important technique for two reasons:\n\nIt can speed-up MCMC considerably.\nIn languages that do not support discrete latent variables (for example, Stan), this is they only way to implement certain model (e.g. mixture models, coming next)",
    "crumbs": [
      "Modelling techniques",
      "Rao-Blackwellization"
    ]
  },
  {
    "objectID": "w10_modelling/topic06_rao.html#outline",
    "href": "w10_modelling/topic06_rao.html#outline",
    "title": "Rao-Blackwellization",
    "section": "",
    "text": "Rao-Blackwellization: what and why\nImplementing Rao-Blackwellization in Stan\nMathematical underpinnings\n\n\n\n\nRao-Blackwellization is an important technique for two reasons:\n\nIt can speed-up MCMC considerably.\nIn languages that do not support discrete latent variables (for example, Stan), this is they only way to implement certain model (e.g. mixture models, coming next)",
    "crumbs": [
      "Modelling techniques",
      "Rao-Blackwellization"
    ]
  },
  {
    "objectID": "w10_modelling/topic06_rao.html#stan-demo",
    "href": "w10_modelling/topic06_rao.html#stan-demo",
    "title": "Rao-Blackwellization",
    "section": "Stan demo",
    "text": "Stan demo\n\nWe revisit the Chernobyl example.\nThis time we implement it in Stan differently to demonstrate Rao-Blackwellization\n\n\n\nCode\nset.seed(1)\n\n# detection limit: value higher than that stay at the limit\nlimit = 1.1 \n\nn_measurements = 10\n\ntrue_mean = 5.6\n\n# data, if we were able to observe perfectly\ny = rexp(n_measurements, 1.0/true_mean)\n\n# number of measurements higher than the detection limit\nn_above_limit = sum(y &gt;= limit)\nn_below_limit = sum(y &lt; limit)\n\n# subset of the measurements that are below the limit:\ndata_below_limit = y[y &lt; limit]\n\n# measurements: those higher than the limit stay at the limit\nmeasurements = ifelse(y &lt; limit, y, limit)\n\n\n\nsuppressPackageStartupMessages(require(rstan))\n\n\ndata {\n  int&lt;lower=0&gt; n_above_limit;\n  int&lt;lower=0&gt; n_below_limit;\n  real&lt;lower=0&gt; limit;\n  vector&lt;upper=limit&gt;[n_below_limit] data_below_limit;\n}\n\nparameters {\n1  real&lt;lower=0&gt; rate;\n}\n\nmodel {\n  // prior\n  rate ~ exponential(1.0/100);\n  \n  // likelihood\n2  target += n_above_limit * exponential_lccdf(limit | rate);\n  data_below_limit ~ exponential(rate); \n}\n\ngenerated quantities {\n  real mean = 1.0/rate;\n}\n\n\n1\n\nNotice that this time we are not including all these \\(H_i\\) above the detection limit! How is this possible?\n\n2\n\nWhat is that exponential_lccdf?!\n\n\n\n\nLet us make sure it works empiricially first:\n\nfit = sampling(\n  chernobyl_rao_blackwellized,\n  seed = 1,\n  chains = 1,\n  data = list(\n            limit = limit,\n            n_above_limit = n_above_limit, \n            n_below_limit = n_below_limit,\n            data_below_limit = data_below_limit\n          ),       \n  iter = 100000                   \n)\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.7e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.17 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:     1 / 100000 [  0%]  (Warmup)\nChain 1: Iteration: 10000 / 100000 [ 10%]  (Warmup)\nChain 1: Iteration: 20000 / 100000 [ 20%]  (Warmup)\nChain 1: Iteration: 30000 / 100000 [ 30%]  (Warmup)\nChain 1: Iteration: 40000 / 100000 [ 40%]  (Warmup)\nChain 1: Iteration: 50000 / 100000 [ 50%]  (Warmup)\nChain 1: Iteration: 50001 / 100000 [ 50%]  (Sampling)\nChain 1: Iteration: 60000 / 100000 [ 60%]  (Sampling)\nChain 1: Iteration: 70000 / 100000 [ 70%]  (Sampling)\nChain 1: Iteration: 80000 / 100000 [ 80%]  (Sampling)\nChain 1: Iteration: 90000 / 100000 [ 90%]  (Sampling)\nChain 1: Iteration: 100000 / 100000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.154 seconds (Warm-up)\nChain 1:                0.152 seconds (Sampling)\nChain 1:                0.306 seconds (Total)\nChain 1: \n\n\n\nfit\n\nInference for Stan model: anon_model.\n1 chains, each with iter=1e+05; warmup=50000; thin=1; \npost-warmup draws per chain=50000, total post-warmup draws=50000.\n\n      mean se_mean   sd   2.5%   25%   50%   75% 97.5% n_eff Rhat\nrate  0.39    0.00 0.20   0.11  0.25  0.36  0.50  0.86 17497    1\nmean  3.39    0.02 2.35   1.16  1.99  2.78  4.02  9.25 13206    1\nlp__ -8.24    0.01 0.73 -10.30 -8.40 -7.96 -7.77 -7.72 18171    1\n\nSamples were drawn using NUTS(diag_e) at Tue Mar 19 07:17:10 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\nCompare: this to the result from the page on censoring, where we got:\n  ...\n1.258 seconds (Total)\n  ...\nmean  3.35    0.02 2.43   1.16   1.97   2.74   3.96   9.20\n  ...\nConclusion: Essentially the same result (i.e., within MCSE), but the new version is faster! How is this possible?",
    "crumbs": [
      "Modelling techniques",
      "Rao-Blackwellization"
    ]
  },
  {
    "objectID": "w10_modelling/topic06_rao.html#mathematical-underpinnings",
    "href": "w10_modelling/topic06_rao.html#mathematical-underpinnings",
    "title": "Rao-Blackwellization",
    "section": "Mathematical underpinnings",
    "text": "Mathematical underpinnings\n\nConsider a simplified example where there is only one observation:\n\n\\[\\begin{align*}\nX &\\sim {\\mathrm{Exp}}(1/100) \\\\\nH &\\sim {\\mathrm{Exp}}(X) \\\\\nC &= \\mathbb{1}[H \\ge L], \\\\\nY &= C L + (1 - C) H.\n\\end{align*} \\tag{1}\\]\n\nSuppose our one observation is censored (\\(C = 1\\)).\nThe first Stan model we implemented targets a distribution over both \\(X\\) and \\(H\\), \\(\\gamma(x, h) = p(x, h, y)\\).\nKey idea behind Rao-Blackwellization:\n\nReduce the problem to a target over \\(x\\) only, \\(\\gamma(x)\\).\nThis is because we do not care much about \\(h\\): it is a nuisance variable.\nBut we would like to do so in such as way that the result of inference on \\(x\\) is the same with \\(\\gamma(x, h)\\) and \\(\\gamma(x)\\) (just faster with Rao-Blackwellization).\n\n\nQuestion: how to define \\(\\gamma(x)\\) from \\(\\gamma(x, h)\\) so that the result on \\(x\\) stay the same?\nQuestion: compute \\(\\gamma(x)\\) in our simplified example, Equation 1.",
    "crumbs": [
      "Modelling techniques",
      "Rao-Blackwellization"
    ]
  },
  {
    "objectID": "w10_modelling/topic04_censoring.html",
    "href": "w10_modelling/topic04_censoring.html",
    "title": "Censoring",
    "section": "",
    "text": "Recognizing censoring\nModelling censoring\n\n\n\n\nCensoring is an example of a data collection process. We will see a case study where ignoring the data collection process literally cost many lives…",
    "crumbs": [
      "Modelling techniques",
      "Censoring"
    ]
  },
  {
    "objectID": "w10_modelling/topic04_censoring.html#outline",
    "href": "w10_modelling/topic04_censoring.html#outline",
    "title": "Censoring",
    "section": "",
    "text": "Recognizing censoring\nModelling censoring\n\n\n\n\nCensoring is an example of a data collection process. We will see a case study where ignoring the data collection process literally cost many lives…",
    "crumbs": [
      "Modelling techniques",
      "Censoring"
    ]
  },
  {
    "objectID": "w10_modelling/topic04_censoring.html#example-chernobyl-1986",
    "href": "w10_modelling/topic04_censoring.html#example-chernobyl-1986",
    "title": "Censoring",
    "section": "Example: Chernobyl, 1986",
    "text": "Example: Chernobyl, 1986\n\nIn 1986, the Chernobyl Nuclear Power Plant exploded.\nInitially, the reactor crew chief, A. Akimov, assumed the core reactor was intact.\n\n\n\n\n\nData generation\n\nThe numbers used in this page are synthetic but inspired by a true story.\nLet us not look at the data generating code just yet, we will come back to it.\n\n\n\nCode\nset.seed(1)\n\n# detection limit: value higher than that stay at the limit\nlimit = 1.1 \n\nn_measurements = 10\n\ntrue_mean = 5.6\n\n# data, if we were able to observe perfectly\ny = rexp(n_measurements, 1.0/true_mean)\n\n# number of measurements higher than the detection limit\nn_above_limit = sum(y &gt;= limit)\nn_below_limit = sum(y &lt; limit)\n\n# subset of the measurements that are below the limit:\ndata_below_limit = y[y &lt; limit]\n\n# measurements: those higher than the limit stay at the limit\nmeasurements = ifelse(y &lt; limit, y, limit)\n\n\n\n\nMeasuring radiation levels\n\nA dosimeter is a device measuring ionizing radiation.\n\nUnit: roentgens per second (R/s).\n\nSuppose that:\n\nIf ionizing radiation is greater than 1.5 R/s \\(\\Rightarrow\\) we think the reactor is breached (radiation coming out of reactor core).\nIf ionizing radiation is smaller than 1.5 R/s \\(\\Rightarrow\\) we think the reactor is not breached.\n\nWe send 10 workers each with a dosimeter.\nThe average over the 10 readings is:\n\n\nmean(measurements)\n\n[1] 1.012227\n\n\n\nAll good?\nLet us look at the raw data and histogram.\n\n\nsuppressPackageStartupMessages(require(\"ggplot2\"))\ndf = data.frame(measurements = measurements)\n\nggplot(df, aes(x = measurements)) +\n  geom_histogram() + \n  geom_rug(alpha = 0.1) + \n  theme_minimal()\n\n\n\n\n\n\n\nmeasurements\n\n [1] 1.1000000 1.1000000 0.8159577 0.7828535 1.1000000 1.1000000 1.1000000\n [8] 1.1000000 1.1000000 0.8234575\n\n\nQuestion: is this concerning?",
    "crumbs": [
      "Modelling techniques",
      "Censoring"
    ]
  },
  {
    "objectID": "w10_modelling/topic04_censoring.html#bayesian-approach-to-censoring",
    "href": "w10_modelling/topic04_censoring.html#bayesian-approach-to-censoring",
    "title": "Censoring",
    "section": "Bayesian approach to censoring",
    "text": "Bayesian approach to censoring\n\nWhat we have encountered in the Chernobyl example is known as censoring.\nSolution: modelling the censoring process.\n\n\nMathematical description\n\nLet \\(L\\) denote a detection limit (here \\(L = 1.1\\))\nLet \\(X\\) denote the unknown parameter (here, the true mean we try to recover).\nLet \\(i\\) denote the observation index (worker \\(i \\in \\{1, 2, \\dots, 10\\}\\) in our example)\nLet \\(H_i\\) denote the measurements before censoring (i.e., if we had a perfect measurement device immune to saturation effects).\nLet \\(C_i\\) denote a binary indicator on the censoring (i.e., equal to one if the imperfect device has a saturation, zero otherwise).\nLet \\(Y_i = C_i L + (1 - C_i) H_i\\).\n\nIn the Chernobyl example, we will use the following model:\n\\[\\begin{align*}\nX &\\sim {\\mathrm{Exp}}(1/100) \\\\\nH_i &\\sim {\\mathrm{Exp}}(X) \\\\\nC_i &= \\mathbb{1}[H_i \\ge L], \\\\\nY_i &= C_i L + (1 - C_i) H_i.\n\\end{align*}\\]\nThe goal is to compute \\(\\mathbb{E}[X | Y]\\).\n\n\nStan implementation\n\nsuppressPackageStartupMessages(require(rstan))\n\n\ndata {\n1  int&lt;lower=0&gt; n_above_limit;\n2  int&lt;lower=0&gt; n_below_limit;\n  real&lt;lower=0&gt; limit;\n3  vector&lt;upper=limit&gt;[n_below_limit] data_below_limit;\n  \n}\n\nparameters {\n  real&lt;lower=0&gt; rate; \n4  vector&lt;lower=limit&gt;[n_above_limit] data_above_limit;\n}\n\nmodel {\n  // prior\n  rate ~ exponential(1.0/100);\n  \n  // likelihood\n  data_above_limit ~ exponential(rate);\n  data_below_limit ~ exponential(rate); \n}\n\ngenerated quantities {\n  real mean = 1.0/rate;\n}\n\n\n1\n\nThe number of \\(H_i\\)’s above the detection limit.\n\n2\n\nThe number of \\(H_i\\)’s below the detection limit.\n\n3\n\nThe \\(H_i\\)’s below the limit are observed, so they go in the data block.\n\n4\n\nThe \\(H_i\\)’s above the limit are not observed, so they go in the parameters block (a better name would have been “latent block”).\n\n\n\n\n\nfit = sampling(\n  chernobyl_naive,\n  seed = 1,\n  chains = 1,\n  data = list(\n            limit = limit,\n            n_above_limit = n_above_limit, \n            n_below_limit = n_below_limit,\n            data_below_limit = data_below_limit\n          ),       \n  iter = 100000                   \n)\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.6e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:     1 / 100000 [  0%]  (Warmup)\nChain 1: Iteration: 10000 / 100000 [ 10%]  (Warmup)\nChain 1: Iteration: 20000 / 100000 [ 20%]  (Warmup)\nChain 1: Iteration: 30000 / 100000 [ 30%]  (Warmup)\nChain 1: Iteration: 40000 / 100000 [ 40%]  (Warmup)\nChain 1: Iteration: 50000 / 100000 [ 50%]  (Warmup)\nChain 1: Iteration: 50001 / 100000 [ 50%]  (Sampling)\nChain 1: Iteration: 60000 / 100000 [ 60%]  (Sampling)\nChain 1: Iteration: 70000 / 100000 [ 70%]  (Sampling)\nChain 1: Iteration: 80000 / 100000 [ 80%]  (Sampling)\nChain 1: Iteration: 90000 / 100000 [ 90%]  (Sampling)\nChain 1: Iteration: 100000 / 100000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.538 seconds (Warm-up)\nChain 1:                0.72 seconds (Sampling)\nChain 1:                1.258 seconds (Total)\nChain 1: \n\n\n\nfit\n\nInference for Stan model: anon_model.\n1 chains, each with iter=1e+05; warmup=50000; thin=1; \npost-warmup draws per chain=50000, total post-warmup draws=50000.\n\n                      mean se_mean   sd   2.5%    25%    50%    75%  97.5%\nrate                  0.40    0.00 0.20   0.11   0.25   0.36   0.51   0.86\ndata_above_limit[1]   4.44    0.03 4.74   1.17   1.86   3.01   5.23  16.18\ndata_above_limit[2]   4.44    0.03 4.87   1.16   1.85   2.99   5.23  16.30\ndata_above_limit[3]   4.44    0.03 5.01   1.16   1.84   2.96   5.20  16.40\ndata_above_limit[4]   4.42    0.03 4.78   1.17   1.86   3.01   5.21  16.00\ndata_above_limit[5]   4.45    0.03 5.04   1.16   1.86   3.00   5.25  16.24\ndata_above_limit[6]   4.43    0.03 4.60   1.16   1.85   3.00   5.23  16.10\ndata_above_limit[7]   4.45    0.03 4.97   1.16   1.83   2.99   5.29  16.26\nmean                  3.35    0.02 2.43   1.16   1.97   2.74   3.96   9.20\nlp__                -19.27    0.02 2.25 -24.59 -20.55 -18.92 -17.61 -15.96\n                    n_eff Rhat\nrate                29917    1\ndata_above_limit[1] 31929    1\ndata_above_limit[2] 29991    1\ndata_above_limit[3] 30474    1\ndata_above_limit[4] 30375    1\ndata_above_limit[5] 29617    1\ndata_above_limit[6] 29820    1\ndata_above_limit[7] 33738    1\nmean                17755    1\nlp__                16351    1\n\nSamples were drawn using NUTS(diag_e) at Mon Mar 18 23:08:34 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\n\nsuppressPackageStartupMessages(require(bayesplot))\nsuppressPackageStartupMessages(require(ggplot2))\nmcmc_areas_ridges(fit, pars = c(\"mean\")) + \n  theme_minimal() + \n  scale_x_continuous(limits = c(0, 10)) \n\n\n\n\n\n\n\n\nBack to the question of whether the radiation is greater than 1.5 R/s:\n\nsamples = extract(fit)$mean\n\nsum(samples &gt; 1.5) / length(samples)\n\n[1] 0.90094",
    "crumbs": [
      "Modelling techniques",
      "Censoring"
    ]
  },
  {
    "objectID": "w10_modelling/topic02_bricks.html",
    "href": "w10_modelling/topic02_bricks.html",
    "title": "More Bayesian bricks",
    "section": "",
    "text": "More distributions to complement those tested in quiz 1.\nMotivation, realization and parameterization(s) for each.\nReparameterization.\n\n\n\n\nRecall our model building strategy:\n\nstart with observation, and find a distribution that “match its data type” (this creates the likelihood),\n\ni.e. such that support of the distribution \\(=\\) observation data type\n\nthen look at the data types of each of the parameters of the distribution you just picked…\n\n…and search for a distributions that match each the parameters’ data type (this creates the prior),\n\nin the case of hierarchical models, recurse this process.\n\nThere are a few common data types for which we do not have talked much about distributions having realizations of that datatype. We now fill this gap.",
    "crumbs": [
      "Modelling techniques",
      "More Bayesian bricks"
    ]
  },
  {
    "objectID": "w10_modelling/topic02_bricks.html#outline",
    "href": "w10_modelling/topic02_bricks.html#outline",
    "title": "More Bayesian bricks",
    "section": "",
    "text": "More distributions to complement those tested in quiz 1.\nMotivation, realization and parameterization(s) for each.\nReparameterization.\n\n\n\n\nRecall our model building strategy:\n\nstart with observation, and find a distribution that “match its data type” (this creates the likelihood),\n\ni.e. such that support of the distribution \\(=\\) observation data type\n\nthen look at the data types of each of the parameters of the distribution you just picked…\n\n…and search for a distributions that match each the parameters’ data type (this creates the prior),\n\nin the case of hierarchical models, recurse this process.\n\nThere are a few common data types for which we do not have talked much about distributions having realizations of that datatype. We now fill this gap.",
    "crumbs": [
      "Modelling techniques",
      "More Bayesian bricks"
    ]
  },
  {
    "objectID": "w10_modelling/topic02_bricks.html#counts",
    "href": "w10_modelling/topic02_bricks.html#counts",
    "title": "More Bayesian bricks",
    "section": "Counts",
    "text": "Counts\n\nSupport: \\(\\{0, 1, 2, 3, \\dots\\}\\).\n\n\n\n\n\nSimple common choice is the Poisson distribution:\n\n\\({\\mathrm{Poisson}}(\\lambda)\\)\n\nParameter: Mean \\(\\lambda &gt; 0\\).\nMotivation: law of rare events.\nStan doc.\n\nPopular alternative, e.g., in bio-informatics: the negative binomial distribution:\n\n\\({\\mathrm{NegBinom}}(\\mu, \\phi)\\)\nMean parameter \\(\\mu &gt; 0\\) and concentration \\(\\phi &gt; 0\\).\nMotivation:\n\nPoisson’s variance is always the same as its mean.\nConsider \\({\\mathrm{NegBinom}}\\) when empirically the variance is greater than the mean (“over-dispersion”).\n\nStan doc.",
    "crumbs": [
      "Modelling techniques",
      "More Bayesian bricks"
    ]
  },
  {
    "objectID": "w10_modelling/topic02_bricks.html#positive-real-numbers",
    "href": "w10_modelling/topic02_bricks.html#positive-real-numbers",
    "title": "More Bayesian bricks",
    "section": "Positive real numbers",
    "text": "Positive real numbers\n\nSupport: \\(\\{x \\in \\mathbb{R}: x &gt; 0\\} = \\mathbb{R}^+\\)\n\n\n\n\n\nMore common choice is the gamma distribution:\n\n\\({\\mathrm{Gam}}(\\alpha, \\beta)\\)\nParameters: Shape parameters \\(\\alpha &gt; 0\\) and rate \\(\\beta &gt; 0\\).\nStan doc.\n\n\nQuestion: consider the following Stan model:\n\nsuppressPackageStartupMessages(require(rstan))\n\n\ndata {\n  int&lt;lower=0&gt; n_obs;\n  vector&lt;lower=0&gt;[n_obs] observations;\n}\n\nparameters {\n  real&lt;lower=0&gt; shape;\n  real&lt;lower=0&gt; rate;\n}\n\nmodel {\n  // priors\n  shape ~ exponential(1.0/100);\n  rate ~ exponential(1.0/100);\n  \n  // likelihood\n  observations ~ gamma(shape, rate);\n}\n\nNotice that neither of the parameters passed in the likelihood can be interpreted as a mean. However, you are asked to report a mean parameter for the population from which the observations come from. How would you proceed?",
    "crumbs": [
      "Modelling techniques",
      "More Bayesian bricks"
    ]
  },
  {
    "objectID": "w10_modelling/topic02_bricks.html#categories",
    "href": "w10_modelling/topic02_bricks.html#categories",
    "title": "More Bayesian bricks",
    "section": "Categories",
    "text": "Categories\n\n\n\n\n\n\n\n\n\n\nSupport: \\(\\{0, 1, 2, 3, \\dots, K\\}\\), for some number of categories \\(K\\).\nAll such distributions captured by the categorical distribution\nWe first discussed it in Exercise 3.\n\n\\({\\mathrm{Categorical}}(p_1, \\dots, p_K)\\)\nProbabilities \\(p_k &gt; 0\\), \\(\\sum_k p_k = 1\\).\nStan doc.",
    "crumbs": [
      "Modelling techniques",
      "More Bayesian bricks"
    ]
  },
  {
    "objectID": "w10_modelling/topic02_bricks.html#simplex",
    "href": "w10_modelling/topic02_bricks.html#simplex",
    "title": "More Bayesian bricks",
    "section": "Simplex",
    "text": "Simplex\nTerminology for the set of valid parameters for the categorical, \\(\\{(p_1, p_2, \\dots, p_K) : p_k &gt; 0, \\sum_k p_k = 1\\}\\): the \\(K\\)-simplex.\n\n\n\n\nHence, if you need a prior over the parameters of a categorical, you need a distribution over the simplex!\nCommon choice: the Dirichlet distribution:\n\n\\({\\mathrm{Dir}}(\\alpha_1, \\dots, \\alpha_K)\\)\nConcentrations \\(\\alpha_i &gt; 0\\).\n\nStan doc.",
    "crumbs": [
      "Modelling techniques",
      "More Bayesian bricks"
    ]
  },
  {
    "objectID": "w10_modelling/topic02_bricks.html#vectors",
    "href": "w10_modelling/topic02_bricks.html#vectors",
    "title": "More Bayesian bricks",
    "section": "Vectors",
    "text": "Vectors\n\nSupport: \\(\\mathbb{R}^K\\)\n\n\n\n\n\nCommon choice: the multivariate normal.\n\n\\(\\mathcal{N}(\\mu, \\Sigma)\\)\nMean vector \\(\\mu \\in \\mathbb{R}^K\\), covariance matrix \\(\\Sigma \\succ 0\\), \\(\\Sigma\\) symmetric.\nStan doc.",
    "crumbs": [
      "Modelling techniques",
      "More Bayesian bricks"
    ]
  },
  {
    "objectID": "w10_modelling/topic02_bricks.html#many-others",
    "href": "w10_modelling/topic02_bricks.html#many-others",
    "title": "More Bayesian bricks",
    "section": "Many others!",
    "text": "Many others!\n\nReferences\n\nUse wikipedia’s massive distribution list,\nand Stan’s documentation.\n\n\n\nAlternative approach: reparameterization\n\nSuppose you need a distribution with support \\([0, 1]\\).\nWe have seen above that one option is to use a beta prior.\nAn alternative:\n\ndefine \\(X \\sim \\mathcal{N}(0, 1)\\),\nuse a transformation to map it to \\([0, 1]\\), e.g. \\(Y = \\text{logistic}(X)\\).\n\nThis approach is known as “re-parametrization”\n\nFor certain variational approaches, this helps inference.\nMore on that in the last week.",
    "crumbs": [
      "Modelling techniques",
      "More Bayesian bricks"
    ]
  }
]