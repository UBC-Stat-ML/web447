---
title: "Metropolis-Hastings"
editor: 
  mode: source
---

{{< include ../_macros.qmd >}}

## Outline

### Topics

-   Metropolis-Hastings (MH) algorithm with symmetric proposal
-   Trace plots from MH
-   Histograms from MH


### Rationale

The vast majority of MCMC algorithms are based on the Metropolis-Hastings (MH) algorithm.

Learning MH is the first step towards understanding *why* MCMC algorithms such as Stan work. 
Some knowledge of how MCMC works is useful to identify when it fails ("MCMC diagnostic"). 
There are also situations where you will have to write your own MCMC algorithm
(something we will practice more in the last weeks of the course).


## Goal

-   Just as [SNIS](../w03_ppl/topic02_snis.qmd), our goal is to compute $\ex[g(X) | Y = y]$.
-   MH will allow us to approximate this conditional expectation.

How is this different than [the simple Monte Carlo
    method](../w01_discrete_inference/topic11_monte_carlo.qmd) that we
    used in [Ex1.Q.1.3](../exercises/ex01.qmd#sec-q1)?
    
::: {.callout-caution collapse="true"} 
## Click for answer

We used simple Monte Carlo to compute an expectation **without** conditioning on $Y = y$, i.e. $\ex[g(X)]$. 

MH, just like SNIS, allows us to take into account observed data. 
:::

## Problem formulation

MH is used to solve the following type of problems:

-   **Input:** You are given an unnormalized distribution $\gamma(x)$
    -   Example: the numerator in [Bayes
        rule](../w01_discrete_inference/topic09_bayes.qmd).
-   **Output:** You want to approximate an expectation...
    -   ...under the **renormalized** distribution
        $\pi(x) = \gamma(x) / Z$.
    -   Mathematically: MH provides an approximation to
        $\ex_\pi[g(X)] = \sum \pi(x) g(x)$.

**Terminology review:** $g$ is called a **test function**. Think about it as
specifying a **query.**

**Examples:** of test functions

-   To compute $\ex[X | Y = y]$, use $g(x) = x$. 
-   Let's say you want to approximate $\pr(X = 1 | Y = y)$...
    -   ...what function $g(x)$ would you take?
    
{{< include ../../clickers/w07/_c03.qmd >}}    
  

## MH's extra ingredient: the proposal

In addition to the unnormalized distribution $\gamma$ and test function
$f$, MH requires a **proposal** $q(x' | x)$.

**Example:** standard normal centered at the current point, 
  $$q(x' | x) \propto \exp\left(- \frac{1}{2} (x - x')^2 \right).$$

-   The proposal will help MH explore the space of possible values
    that $X$ can take.
-   **Key difference compared to SNIS:** the proposal can depend on the current value $x$
-   Intuition:
      - Once we find a "good neighborhood", i.e., a region where the posterior density is high,
      - having $q(\cdot|x)$ depend on current value allows us to stay for a while in that good neighborhood. 
      
**Today:** 

- we assume $q$ is symmetric, i.e., $q(x | x') = q(x' | x)$. 
- We will relax this later.
      
      
## Symmetric proposal MH algorithm

**Inputs:** unnormalized target $\gamma$, proposal $q$, number of iterations $M$.

**Algorithm:**

- Initialize $X^{(0)}$ arbitrarily
- For $m = 1, 2, \dots, M$ do:
    - Denote the proposal at iteration $m \in \{1, 2, \dots, M\}$ by:
      $$\tilde X^\parm \sim q(\cdot | X^{(m-1)}).$$
    - Compute the **MH ratio**:
        $$R^\parm = \frac{\gamma(\tilde X^\parm)}{\gamma(X^{(m-1)})}.$$
    - Sample an **acceptance Bernoulli**:
      $$A^\parm \sim \distBern(\min(1, R^\parm)).$$
        - If $A^\parm = 1$, we **accept** the proposed sample:
          $$X^\parm = \tilde X^\parm,$$
        - Else, $A^\parm = 0$, and we **reject** the proposed sample and stay at previous position:
          $$X^\parm = \tilde X^{(m-1)}.$$ 
          
**Output:** equally weighted samples $X^{(1)}, X^{(2)}, \dots, X^{(M)}$. 
          

## Intuition

- When proposing to a **higher point** under the posterior density, we have:
    - $\gamma(\tilde X^\parm) > \gamma(X^{(m-1)})$
    - hence $R^\parm > 1$, 
    - so $\min(1, R^\parm) = 1$, 
    - and $A^\parm$ is always one, i.e. always accepted!
- When proposing to a **lower point** under the posterior density, we have:
    - $\gamma(\tilde X^\parm) < \gamma(X^{(m-1)})$
    - hence $R^\parm < 1$, 
    - so $\min(1, R^\parm) = R^\parm$,
    - $A^\parm \sim \distBern(\gamma(\tilde X^\parm)/\gamma(X^{(m-1)}))$,
    - i.e., acceptance probability is a ratio of posterior densities at two points (proposed vs previous). 


## Visualization 

In the exercise this week, you will [implement this simple version MH](../exercises/ex07.qmd#sec-q3) (i.e., symmetric proposal). 

Once your exercise is complete you will be able to call your function as follows:

```{r}
source("../exercises/ex07_scaffold.R")
source("../../solutions/sol07_mh.R")

samples = simple_mh(gamma_beta_binomial, 0.5, 1000)
```


### Trace plot 

Let us visualize the first 100 samples:

```{r}
plot(samples[1:100], type = "o", col = rgb(red = 0, green = 0, blue = 0, alpha = 0.2))
```

**Question:** Notice that many points take exactly the same value, why is that?

{{< include ../../clickers/w07/_c04.qmd >}}


**Terminology:** the above plot (sample as a function of MCMC iteration) is called a **trace plot.**

Full trace plot:

```{r}
plot(samples, type = "o", col = rgb(red = 0, green = 0, blue = 0, alpha = 0.2))
```


### Histogram

As with Stan (another MCMC algorithm we saw earlier this week), we can use a histogram to 
summarize the posterior:

```{r}
hist(samples)
```

To connect the two pictures, it is useful to visualize the histogram rotated as follows:

```{r}
source("../blocks/plot_traces_and_hist.R")

plot_traces_and_hist(samples)
```