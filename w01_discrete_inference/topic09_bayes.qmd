---
title: "Bayes rule"
editor: 
  mode: source
---

{{< include ../_macros.qmd >}}



## Outline

### Topics 

- Bayes rule for discrete models
- Visual intuition

### Rationale

First example of computing a *posterior distribution*, a key concept in Bayesian statistics.


## Running example

{{< include ../blocks/_coinbag.qmd >}}

Consider the [second question in the first exercise](../exercises/ex01.qmd#sec-q2):

Suppose now that you observe the outcome of the 4 coin flips, but not the type 
of coin that was picked. Say you observe: "heads", "heads", "heads", "heads" = `[0, 0, 0, 0]`. 
Given that observation, what is the probability that you picked the standard coin (i.e., the one with $p = 1/2$)?


## Strategy

Denote the observation by $y_{1:4} = (0, 0, 0, 0)$. In the rest of the argument we will always fix $y$ to that value.

1. Attack the more general problem $\pi(x) = \pr(X = x | Y_{1:4} = y_{1:4})$ for **all** hypotheses $x \in \{0, 1, 2\}$ instead of just the requested $x = 1$ (corresponding to the "standard coin").
2. By definition of conditioning:
$$\pi(x) = \frac{\pr(X = x, Y_{1:4} = y_{1:4})}{\pr(Y_{1:4} = y_{1:4})}.$$
Let us call the numerator 
$$\gamma(x) = \pr(X = x, Y_{1:4} = y_{1:4}),$$
and the denominator,
$$Z = \pr(Y_{1:4} = y_{1:4}).$$
3. Start by computing $\gamma(x)$ for all $x$. [(using chain rule)](topic08_chain.qmd#sec-chain-rule)
4. Note $Z = \gamma(0) + \gamma(1) + \gamma(2)$ [(why?)](topic02_axioms.qmd#sec-axioms).

