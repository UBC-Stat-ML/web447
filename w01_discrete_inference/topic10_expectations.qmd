---
title: "Expectations"
editor: 
  mode: source
---

{{< include ../_macros.qmd >}}


## Outline

### Topics 

- Expectation for discrete random models
- Law of the Unconscious Statistician

### Rationale

Expectation si the main tool to translate a posterior distribution into 
the various outputs of Bayesian inference (point estimate, credible intervals, prediction, action). 


## Expectation of a single random variable

Recall:
$$\E[X] = \sum_x x p_X(x),$$
where the sum is over the point masses of $X$, i.e. $\{x : p_X(x) > 0\}$. 

**Example:** compute $\E[X]$ if $X \sim \distBern(p)$, with $p = 0.8$.  

{{< include ../blocks/_bern.qmd >}}


## Law of the Unconscious Statistician

**Proposition:** if $g$ is some function, 
$$\E[g(X)] = \sum_x g(x) p_X(x).$$

**Example:** compute $\E[X^2]$ if $X \sim \distBern(p)$, and hence $\var[X] = \E[X^2] - (\E[X])^2$.  

{{< include ../../clickers/w01/_c04.qmd >}}


## Expectation of a function of several random variables

Let us go back to our running example: 

{{< include ../blocks/_coinbag.qmd >}}

**Example:** computing $\ex[X (Y_1+1)]$ (similar to what you will be doing in the exercise in [question 1.1](../exercises/ex01.qmd#sec-q1))

**Note:** this is of the form $\ex[g(\dots)]$, so we can use the Law of the Unconscious Statistician. 

How to do it: 

- first, identify $g$, here it is $g(x, y_1, \dots, y_4) = x(y_1+1)$ (in the exercise it is slightly different)
- denote by $p$ the joint PMF of **all** the random variables in the model
- compute the expectation using
$$\ex[g(X, Y_1, \dots, Y_4)] = \sum_x \sum_{y_1} \sum_{y_2} \dots \sum_{y_4} g(x, y_1, \dots, y_4)  p(x, y_1, y_2, y_3, y_4).$$
- Each sum runs over the point mass of its PMF as before, e.g. $x \in \{0, 1, 2\}$. 
- Recall: $p(x, y_1, y_2, y_3, y_4)$ can be computed using the chain rule. 

Recall the decision tree, how to visualize the above equation?

{{< include ../blocks/_decisiontree.qmd >}}

{{< include ../../clickers/w01/_c05.qmd >}}