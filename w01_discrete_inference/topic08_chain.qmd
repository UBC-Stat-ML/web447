---
title: "Chain rule"
editor: 
  mode: source
---

{{< include ../_macros.qmd >}}



## Outline

### Topics

- Mathematical statement
- Visual intuition on a decision tree
- Special names for the pieces of chain rule (joint and conditional PMFs)
- Conditional independence

### Rationale

The chain rule allows us to compute a probability that the forward sampling function takes a given path.

The chain rule seem innocent but is used heavily in Bayesian statistics. 
It is also the building block for Bayes rule. 

## Proposition 

If $E_1$ and $E_2$ are any events, $$\pr(E_1, E_2) = \pr(E_1) \pr(E_2 | E_1).$$ 

This is true in any order, i.e. we also have $\pr(E_1, E_2) = \pr(E_2) \pr(E_1 | E_2)$.


## Generalization {#sec-chain-rule}

For any events $E_1, E_2, E_3 \dots$, 

$$\pr(E_1, E_2, E_3 \dots) = \pr(E_1) \pr(E_2 | E_1) \pr(E_3 | E_1, E_2) \dots.$$


## Visual intuition

**Chain rule:** the probability of a node is the product of the edge labels on the path to the root. 

{{< include ../blocks/_decisiontree_node.qmd >}}


## Poll: what is the probability of the node in red?

1. 1/2
1. 1/4
1. 1/8
1. 1/24
1. None of the above


::: {.callout-caution collapse="true"} 
## Click for answer

Multiplying the four edge leading to the node we get: $1/2 \cdot 1/2 \cdot 1/2 \cdot 1/3 = 1/24$.
:::


## Poll: what is the event corresponding to that node?

1. $(Y_3 = 1)$
1. $(Y_3 = 1, Y_2 = 1)$
1. $(Y_3 = 1, Y_2 = 1)$
1. $(Y_3 = 1, Y_2 = 1, Y_1 = 1)$
1. $(Y_3 = 1, Y_2 = 1, Y_1 = 1, X = 1)$

::: {.callout-caution collapse="true"} 
## Click for answer

Recall that the event is the intersection of all node labels to the root, hence 
the event is $(Y_3 = 1, Y_2 = 1, Y_1 = 1, X = 1)$. 

The calculation we did visually in the  previous clicker question is mathematically:
$$\pr(Y_3 = 1, Y_2 = 1, Y_1 = 1, X = 1) = \pr(X = 1) \pr(Y_1 | X = 1) \pr(Y_2 | X = 1, Y_1 = 1) \pr(Y_3 = 1 | X = 1, Y_1 = 1, Y_2 = 1).$$
:::


## Joint PMF

We will often encounter expression of the form of a conjunction (intersection/and) of several variables. 
A handy notation for that is the **joint PMF** 

For example, here is the joint PMF of $(X, Y_1, Y_2, Y_3)$:

$$p(x, y_1, y_2, y_3) = \pr(X = x, Y_1 = y_1, Y_2 = y_2, Y_3 = y_3).$$

Sometimes we put the random variables in question as subscript, for example $p_{X, Y_1}(x, y)$ for the joint 
PMF of $X$ and $Y_1$. 


## Conditional PMF

Similarly, here is an example of a conditional PMF:
$$p_{Y_1|X}(y | x) = \pr(Y_1 = y | X = x).$$


## Conditional independence {#sec-conditional-independence}

The model was specified as: 

$$
\begin{align*}
X &\sim \distUnif\{0, 1, 2\} \\
Y_i | X &\sim \distBern(X/2)
\end{align*}
$$
i.e. with $\pr(X = x)$ and $\pr(Y_i = y | X = x)$ for all $x$ and $y$. 

**Question:** how did we go from $\pr(Y_2 | X = 1, Y_1 = 1)$ (in our chain rule computation) to $\pr(Y_2 | X = 1)$ (model specification)?

**Definition:** $V$ and $W$ are conditionally independence given $Z$ if 
$$\pr(V = v, W = w | Z = z) = \pr(V = v | Z = z) \pr(W = w | Z = z).$$

**Exercise:** show the above definition is equivalent to:

$$\pr(V = v | W = w, Z = z) = \pr(V = v | Z = z).$$



