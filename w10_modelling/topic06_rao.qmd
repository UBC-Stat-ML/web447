---
title: "Rao-Blackwellization"
editor: 
  mode: source
---

{{< include ../_macros.qmd >}}


## Outline

### Topics

- Rao-Blackwellization: what and why
- Implementing Rao-Blackwellization in Stan
- Mathematical underpinnings

### Rationale

Rao-Blackwellization is an important technique for two reasons:

- It can speed-up MCMC considerably. 
- In languages that do not support discrete latent variables ([for example, Stan](https://mc-stan.org/docs/stan-users-guide/latent-discrete.html)), 
  this is they only way to implement certain model (e.g. mixture models, coming next)


## Stan demo

- We revisit the [Chernobyl example](topic04_censoring.qmd).
- This time we implement it in Stan **differently** to demonstrate **Rao-Blackwellization**

{{< include ../blocks/_chernobyl_data_gen.qmd >}}

```{r}
suppressPackageStartupMessages(require(rstan))
```

```{stan output.var = "chernobyl_rao_blackwellized"}
data {
  int<lower=0> n_above_limit;
  int<lower=0> n_below_limit;
  real<lower=0> limit;
  vector<upper=limit>[n_below_limit] data_below_limit;
}

parameters {
  real<lower=0> rate; # <1>
}

model {
  // prior
  rate ~ exponential(1.0/100);
  
  // likelihood
  target += n_above_limit * exponential_lccdf(limit | rate); # <2>
  data_below_limit ~ exponential(rate); 
}

generated quantities {
  real mean = 1.0/rate;
}
```
1. Notice that this time we are not including all these $H_i$ above the detection limit! How is this possible?
2. What is that `exponential_lccdf`?!

**Let us make sure it works empiricially first:**

```{r dependson=knitr::dep_prev()}
fit = sampling(
  chernobyl_rao_blackwellized,
  seed = 1,
  chains = 1,
  data = list(
            limit = limit,
            n_above_limit = n_above_limit, 
            n_below_limit = n_below_limit,
            data_below_limit = data_below_limit
          ),       
  iter = 100000                   
)
```

```{r}
fit
```

**Compare:** this to the result from [the page on censoring](topic04_censoring.qmd), where we got:

```
  ...
1.258 seconds (Total)
  ...
mean  3.35    0.02 2.43   1.16   1.97   2.74   3.96   9.20
  ...
```

**Conclusion:** Essentially the same result (i.e., within [MCSE](../w09_workflow/topic05_mcmc_ess.qmd)), 
but the new version is faster! How is this possible?


## Mathematical underpinnings

- Consider a simplified example where there is only one observation: 

$$\begin{align*} 
X &\sim \distExp(1/100) \\ 
H &\sim \distExp(X) \\ 
C &= \ind[H \ge L], \\
Y &= C L + (1 - C) H.
\end{align*}$${#eq-simplified}

- Suppose our one observation is censored ($C = 1$). 
- The [first Stan model we implemented](topic04_censoring.qmd) targets a distribution over both $X$ and $H$, $\gamma(x, h) = p(x, h, y)$. 
- **Key idea behind Rao-Blackwellization:** 
    - Reduce the problem to a target over $x$ only, $\gamma(x)$. 
    - This is because we do not care much about $h$: it is a *nuisance variable*. 
    - But we would like to do so in such as way that the result of inference on $x$ is the same with $\gamma(x, h)$ and $\gamma(x)$ (just faster with Rao-Blackwellization). 
    
**Question:** how to define $\gamma(x)$ from $\gamma(x, h)$ so that the result on $x$ stay the same?

{{< include ../../clickers/w10/_c03.qmd >}}

**Question:** compute $\gamma(x)$ in our simplified example, @eq-simplified. 

{{< include ../../clickers/w10/_c04.qmd >}}
