---
title: "(Normal) regression"
editor: 
  mode: source
---

{{< include ../_macros.qmd >}}


## Outline

### Topics

- A second example of a Bayesian model based on a linear model, this time with a normal likelihood. 
- More examples of prior: handling the positive real line.

### Rationale

Together with classification (previous page), regression is the other major statistical task frequently encountered. 

We see here that the same approach as we took for classification can be easily modified to do regression. 

This provides us with a second example of Bayesian GLMs. 


## Example

- In the early 20th century, astronomers made the startling observation that pretty much all galaxies are moving away from ours. Why?
- We now know this is because the universe is expanding.
- Here is a metaphor to help understand this:
    - Imagine ants on an inflating balloon. 
    - You are one of the ants...
    - ...and you notice that all the others ants are moving away from you...
        - ...and the further the neighbor ant, the faster it looks like it is moving away from you.
- In 1929 the astronomer Edwin Hubble published a paper[^1] on the relationship between distance and velocity of galaxies relative to us. 
- It is now called [Hubble's law](http://en.wikipedia.org/wiki/Hubble's_law). 
- The estimated slope of the relationship, known as Hubble's constant, leads to an estimate of the age of the universe.

::: column-margin
![](../images/hubble.jpg){width="200"}
:::

[^1]: Proceedings of the National Academy of Sciences, Vol. 15, pp. 168â€“173.


### Data 

- We will estimate Hubble's constant using data from the [original data used by Edwin Hubble](https://quarknet.fnal.gov/fnal-uc/eeu/hubblediagram/hubble-1.xls).
- Here is some EDA on that dataset: 

```{r}
suppressPackageStartupMessages(require("dplyr"))
df = read.csv("../data/hubble-1.csv") %>%
    rename(distance = R..Mpc.) %>%
    rename(velocity = v..km.sec.)
df$velocity = df$velocity/1000
rmarkdown::paged_table(df)
```

- To make the numbers less extreme in the following, I will divide the velocities by 1000

```{r}
plot(df$distance, df$velocity, xlab = "distance", ylab = "velocity")
```



## Building a Bayesian regression model

**Goal:** 

- designing a model containing a "slope" parameter, 
- from which we will compute $\ex[\text{slope} | \text{data}]$. 

To achieve our goal, we will complete the gap in the following code:

```r
source("../../solutions/simple.R")
source("../blocks/simple_utils.R")

regression = function() {
  # priors will be defined here
  # ...
  for (i in 1:nrow(df)) { 
    distance = df[i, "distance"]
    velocity = df[i, "velocity"]
    # likelihood will be defined here
  }
  return(slope)
}

posterior(regression, 1000)
```

- **Recall:** to build a model, start with the observations. 
- Specifically, let us build a model for the observed velocity first. 

**Question:** what would be a reasonable likelihood for the velocity?

{{< include ../../clickers/w04/_c05.qmd >}}

{{< include ../../clickers/w04/_c06.qmd >}}

{{< include ../../clickers/w04/_c07.qmd >}}

{{< include ../../clickers/w04/_c08.qmd >}}

**Question:** complete the code and approximate $\ex[\text{slope} | \text{data}]$. 

{{< include ../../clickers/w04/_c09.qmd >}}


## Visualization of the posterior distribution

```{r}
posterior = posterior_particles(regression, 10000)
weighted_scatter_plot(posterior, plot_options = list(xlab="slope parameter", ylab="sd parameter"))
```

```{r}
plot(df$distance, df$velocity, xlab = "distance", ylab = "velocity")

xs = seq(0, 2, 0.01)
samples = posterior$samples 
norm_weights = posterior$weights / sum(posterior$weights)

for (i in 1:nrow(samples)) {
  slope     = samples[i, 1]
  pr = norm_weights[i]
  lines(xs, slope * xs, col = rgb(red = 0, green = 0, blue = 0, alpha = pr*20))
}
```


## Model criticism

The answer we obtain is close to Edwin Hubble's original estimate. 

The modern estimate based on state-of-the-art measurements (space telescopes, advanced statistical models) gives
an estimate that is about 10x smaller.

**Discussion:** What went wrong?


