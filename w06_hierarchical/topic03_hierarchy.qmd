---
title: "Intro to hierarchical models"
editor: 
  mode: source
---

{{< include ../_macros.qmd >}}


## Outline

### Topics

- Hierarchical models. 

### Rationale

Hierarchical models, a crowning achievement of Bayesian inference, can be used 
to decrease prior sensitivity even when the dataset of interest is of limited size.


## Example

::: column-margin
![](../images/delta.jpeg){width="200"}
:::

**Example:** predicting the probability of failure of the next Delta 7925H launch (so far, observed has been launched 3 times, with
0 failed launches) .

**Key idea:** use "**side data**" to inform the prior...

- For example: success/fail launch data from other **other types** of rockets.
- Can we use the full rocket launch data provided to inform prediction for a single rocket type of interest?


```{r}
suppressPackageStartupMessages(require("dplyr"))
df = read.csv("../data/failure_counts.csv") 
rmarkdown::paged_table(df)
```


## How to use "side data"? Two suboptimal approaches

For pedagogy, we will first cover two simple heuristics to incorporate "side data" and describe their limitations. 

Then we will cover hierarchical models in the next section, showing how they address the limitations of simpler methods.

### First try (do not use this one!)

- Merge all the data?
- I.e. just sum the columns in the data:


```{r}
sum(df$numberOfLaunches)
sum(df$numberOfFailures)
sum(df$numberOfFailures) / sum(df$numberOfLaunches)
```

**Why merging everything is a bad idea?**

::: {.callout-caution collapse="true"} 
## Click for answer

Some of these types of rockets are very different than the one under study (Delta 7925H). 
For example some of them are very old, made by different space agencies, etc. 

We want to consider the different types of rockets as a **population** and model the variability within that population. 
:::


### Towards an improved way to use side data

- Background: "mean--pseudo-sample-size" reparameterization of the [Beta distribution](https://en.wikipedia.org/wiki/Beta_distribution)
    - A reparametrization is a different labelling of a family such that you can go back and forth between the two labellings
    - Consider $$\alpha  = \mu s, \;\; \beta = (1 - \mu) s$$ where $\mu \in (0, 1)$, $s > 0$.
    
- Interpretation:
    - $\mu$: mean of the Beta
    - $s$: measure of "peakiness" of the density, higher $s$ corresponds to more peaked; roughly, $s \sim$ number of data points that would make the posterior peaked like that. 

- Why we did this reparameterization? 
    - Now it should be more intuitive how we could go about using **side data** to inform at least $\mu$. 
    - Ideas?


### Second try (still suboptimal, but not as bad)

- Estimate a failure probability $\hat p_i$ for each type of rocket $i$. 
- Fit a distribution on those $\{\hat p_i\}$. 
- Second try: use this distribution as the prior on $\mu$?


```{r message=F, warning=F}
suppressPackageStartupMessages(require("ggplot2"))
suppressPackageStartupMessages(require("latex2exp"))

ggplot(df, aes(x = numberOfFailures / numberOfLaunches)) +
  geom_histogram() + 
  xlab(TeX("$\\hat{p}_i = \\text{numberOfFailures}_i / \\text{numberOfLaunches}_i$")) +
  geom_rug(alpha = 0.1)
```

**Question:** what are the weaknesses of this method?

::: {.callout-caution collapse="true"} 
## Click for answer

- First, notice the bumps at 1/2 and 1...
    - Most of those are types of rockets with only 1 or 2 launch datapoints (e.g. 1 failure out of 2 launches). 
    - But if you fit a curve to this data, these bumps will have a large undesirable effects. 
    - More broadly, we want to take into account uncertainty in each of these $\hat p_i$'s. 
- Also: less clear how to do this with the pseudo-sample-size $s$. 
:::

## Hierarchical models: a better way to use side data

**Solution:** go fully Bayesian!

**Recall:** [our Bayesian recipe,](../w02_discrete_bayes/topic01_bayes_recipe.qmd)

1. Construct a probability model including
    - random variables for what we will measure/observe
    - **random variables for the unknown quantities**
        - those we are interested in ("parameters", "predictions")
        - **others that just help us formulate the problem** ("nuisance", "random effects"). 
2. Compute the posterior distribution conditionally on the actual data at hand
3. Use the posterior distribution to make a decision

**What it means here:** we model the launcher type's population parameters ($\mu, s$) as random variables. 


### Graphical model

![](../images/hierarchical-gm.png){width="500"}


### Mathematical description

- Share these two "population parameters" across all launch types 
$$p_i | \mu, s \sim \distBeta(\mu s, (1 - \mu) s)$$
- Likelihood same as before: $F_i | p_i \sim \distBern(n_i, p_i).$
- We still need to put prior on $\mu$ and $s$
    - But you should expect this prior choice to be less sensitive. Why? Hint: look at the number of outgoing edges in the graphical model. Compare with number of outgoing edges for the "maiden flight" (see next slide)
    - Example: $\mu \sim \distBeta(1,1) = \distUnif(0, 1)$, $s \sim \distExp(1/10000)$ (why such a small value? Hint: mean vs. rate parameter)
    
    
### Decrease prior choice sensitivity

It seems we have introduced new problems as now we again have hyperparameters, namely those for the priors on $\mu$ and $s$. Here we picked $\mu \sim \distBeta(1,1) = \distUnif(0, 1)$, $s \sim \distExp(1/10000)$

**Key point:** yes, but now we are less sensitive to these choices!

Why? Heuristic: say you have a random variable connected to some hyper-parameters (grey squares) and random variables connected to data (circles)

- If most of the connections are hyper-parameters: will probably be sensitive.
- If there are many more connections to random variables compared to hyper-parameters: will probably be insensitive.

**Before going hierarchical:** for maiden/early flights we had

![](../images/hierarchical-gm-before.jpg){height="200"}

**After going hierarchical:**

![](../images/hierarchical-gm-after.jpg){height="250"}
