---
title: "MCMC diagnostics"
editor: 
  mode: source
---

{{< include ../_macros.qmd >}}

## Outline

### Topics

- Heuristics to detect pathological behaviour. 
- Methods to estimate effective sample size. 

### Rationale

We have seen that [MCMC is consistent](../w08_mcmc1/topic05_mcmc_consistency.qmd), 
however the speed of convergence can vary considerably due to the 
dependence between the successive draws. 

When convergence is too slow it may be necessary to change the inference algorithm, 
either into another MCMC algorithm or to a variational method. 


## Overview

Informally, there are two possible situations to distinguish:

**Fast mixing:**

- The chain is almost like i.i.d. sampling, just a constant time slower.
- This happens when the dependence between time step $i$ and $i+m$ decays exponentially in $m$.[^1]

[^1]: "Fast mixing" is often formalized using [one of several equivalent definitions of geometric ergodicity](https://arxiv.org/pdf/2203.04395.pdf).


**Slow/torpid mixing:**

- Terminology: **slow/torpid mixing**. 
- In this case, changes have to be made to the sampler. 
- We will cover two alternatives to consider for these difficult targets:
    - Tempering methods (week 12),
    - Variational methods (week 13).
    
    
## Heuristics to detect slow mixing chains

- Key idea: run several independent chains from "over-dispersed" initializations. 
- Check for differences between these chains:
    - Trace plots.
    - Rank plots. 
- These are not bullet-proof methods... 
- ... but it is still a good idea to use them unless theory provides guarantees (e.g. [log-concave distributions](https://jmlr.org/papers/volume20/19-306/19-306.pdf)).


### Example of fast and slow mixing chains

```{r}
suppressPackageStartupMessages(require(rstan))
suppressPackageStartupMessages(require(ggplot2))
suppressPackageStartupMessages(require(bayesplot))
```


**Easy problem:**

```{stan output.var = "betabinom"}
data {
  int<lower=0> n_trials;
  int<lower=0> n_successes;
}
parameters {
  real<lower=0, upper=1> p;
}

model {
  p ~ uniform(0, 1);
  n_successes ~ binomial(n_trials, p);
}
``` 

**Challenging problem:** 

- Write $p = p_1 p_2$, where $p_i \sim \distUnif(0, 1)$.
- This creates an **unidentifiability**: for each value $p$ there are several possible $p_1, p_2$ such that $p = p_1 p_2$.
- The posterior looks like a "thin ridge" (see visualization [here](https://julia-tempering.github.io/InferenceReport.jl/stable/generated/toy_turing_unid_model/src/), obtained using tempering, covered in week 12)

::: column-margin
![](../images/ridge.png){width="300"}
:::

```{stan output.var = "unid"}
data {
  int<lower=0> n_trials;
  int<lower=0> n_successes;
}
parameters {
  real<lower=0, upper=1> p1;
  real<lower=0, upper=1> p2;
}

model {
  p1 ~ uniform(0, 1);
  p2 ~ uniform(0, 1);
  n_successes ~ binomial(n_trials, p1 * p2);
}
``` 

```{r message=FALSE, warning=FALSE, results=FALSE, dependson=knitr::dep_prev()}
fit_easy = sampling(
  betabinom,
  seed = 1,
  chains = 2,
  refresh = 0,
  data = list(n_trials=10000000, n_successes=10000000/2),       
  iter = 1000                   
)

fit_hard = sampling(
  unid,
  seed = 1,
  chains = 2,
  refresh = 0,
  data = list(n_trials=10000000, n_successes=10000000/2),       
  iter = 1000                   
)
```

**Trace plots:**

```{r}
mcmc_trace(fit_easy, pars = c("p")) + theme_minimal()
mcmc_trace(fit_hard, pars = c("p1")) + theme_minimal()
```

**Rank histogram:** 

- First look at the chains combined and compute ranks.
- Display for each chain the rank distribution for the samples in that chain. 
- In the fast mixing case, all histograms should be approximately uniform.

```{r}
mcmc_rank_hist(fit_easy, pars = c("p")) + theme_minimal()
mcmc_rank_hist(fit_hard, pars = c("p1")) + theme_minimal()
```
