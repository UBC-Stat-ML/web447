---
title: "Exercise 8: Bayesian Workflow"
editor: 
  mode: source
---
  
{{< include ../_macros.qmd >}}


## Q.1: calibration analysis via cross-validation

We have already discussed the concept of [calibration](../w05_properties/topic05_calibration.qmd) for Bayesian inference, where we reviewed the behavior of both well-specified---the
ideal situation---and mis-specified models---the most common case in practice.
In the latter case, we demonstrated with a simulation experiment that
mis-specification results in credible intervals that are not calibrated.
However in practice, these tests are impossible to carry out because we do
not have access to the true data-generating distribution.

One way to approximate the outcome of such an experiment is via *cross-validation*.
In particular, this exercise will teach you how to use leave-one-out
cross-validation to assess the calibration of the model for the Hubble dataset. 
Let's begin by loading and formatting the data
```{r}
#| eval: false
#| file: ex07_import_hubble.R
```

1. Modify the stan code you wrote for [exercise 7](ex07.qmd) to include

a. In the `data` block: an additional real-valued argument `x_pred` for the
independent variable for the left-out point.
b. In the `generated quantities` block: generate a value `y_pred` given `x_pred`
and the parameters $(\text{slope, sigma})$.

Create a compiled model called `hubble_predict` by passing the modified
stan code to the function `stan_model`.

2. Test your implementation by fitting the version of the model that leaves out
the last row of the dataset. **Hint**: you should pass as data to stan the 
`train_test_dta` list defined below
```r
N_obs = nrow(df)
N_train = N_obs-1
train_test_dta = list(
    N  = N_train,
    xs = df$distance[-N_obs], 
    ys = df$velocity[-N_obs], 
    x_pred = df$distance[N_obs]
)
```
Report the leave-one-out 80% credible interval for the left out observation.

3. Repeat step 2 for all observations. That is, construct a matrix of `nrow(df)`
rows and 2 columns called `ci_limits`, where the `i`-th row contains an 80% credible
interval for the `i`-th observation left out. Report the plot generated by the following code
```r
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(dplyr))

merged_df = df %>% 
  bind_cols(data.frame(CI_L = ci_limits[,1], CI_R = ci_limits[,2])) %>% 
  mutate(Inside_CI = (velocity >= CI_L & velocity <= CI_R)) 
merged_df %>% 
  ggplot(aes(x = 1:N_obs, y = velocity, ymin = CI_L, ymax = CI_R, color=Inside_CI)) +
  geom_point() + 
  geom_errorbar() +
  theme_minimal() +
  labs(x = "Point", y = "Velocity")
```
Is the fraction of left-out observations contained in their respective credible 
intervals similar to the nominal coverage level? 

## Q.2: prior predictive checks

Consider the logistic regression model

$$
\begin{aligned}
\beta_i &\distiid \distNorm(0,1), \quad i\in\{1,\dots,n_\text{pred}\} \\
y_n|\beta &\sim \distBern(\text{logistic}(x_n^T \beta)), \quad n\in\{1,\dots,n_\text{obs}\} 
\end{aligned}
$$ {#eq-log-reg}

In this exercise we will investigate the effect of having fixed independent priors
on the coefficients $\{\beta_i\}_{i=1}^{n_\text{pred}}$ as $n_\text{pred}$ grows.

1. Write a function called `logistic_regression` that takes as input a matrix `X` of
`n_obs` rows and `n_pred` columns. Your function should 
a) simulate $\beta$ according to @eq-log-reg, 
b) simulate the response vector $y$ given $\beta$, and 
c) return the average response $\bar{y}:=n_\text{obs}^{-1}\sum_i y_i$.

2. Let us fix $n_\text{obs}=100$. For each $n_\text{pred}\in\{2,4,15\}$, sample
$X$ according to
$$
X_{n,i} \distiid \distBern(0.9)
$$
Then, run `logistic_regression` 100,000 times, and report the histogram of the
simulated $\bar{y}$ values. To help you, you may use the following code as a
skeleton for your answer
```r
opar = par(mfrow=c(1,3)) # set a layout of 3 plots in a single row
for(n_pred in c(2,4,15)){
  X = # fill this
  simulated_ybars = # fill this
  hist(simulated_ybars, breaks=20, ylim=c(0,17000), 
       main=paste(n_pred, "predictors"), xlab = "Average outcome")
}
par(opar) # reset graphical parameters
```

3. Do you observe any change in the distributions obtained in step 2? Do you think
this is desirable in a model? If yes, why? If not, how would you fix this?

## Q.3: hierarchical models revisited

Modify the [tutorial on tidybayes](../w09_workflow/topic05_grouped_data.qmd) to 
make the model hierarchical, following the same model as in [exercise 6](ex06.qmd).

