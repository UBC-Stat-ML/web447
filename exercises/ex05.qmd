---
title: "Exercise 5: Bayesian theory"
editor: 
  mode: source
---
  
{{< include ../_macros.qmd >}}


## Q.1: sequential updating

Consider a joint probabilistic model given by
$$
\begin{aligned}
\theta &\sim \mu \\
x_i|\theta &\distiid \nu_\theta, \quad i\in\{1,\dots,n\}
\end{aligned}
$$
where $\mu$ is a prior distribution for the unknown parameter $\theta$, and
$\{x_i\}_{i=1}^n$ is a sequence of observations with conditional distribution
$\nu_\theta$.

1. Write down the posterior distribution of $\theta$ given $\{x_i\}_{n=1}^T$.
2. Suppose now we get an additional data point $x_{n+1}$ with the same conditional
distribution $\nu_\theta$. Show that using the posterior from part 1 as *prior* 
and data equal to just $x_{n+1}$ gives *the same* posterior distribution as
redoing part 1 with the $n+1$ data points.



## Q.2: Bayesian inference in the limit of increasing data

We will use the tractability of the coin bag example to explore the behavior of the
posterior distribution as the number of observations goes to infinity. Recall
that its joint distribution is
$$
\begin{aligned}
p &\sim \distDiscrete(\{0,1/K,2/K,\dots,1\},\rho) \\
y_n|p &\distiid \distBern(p), \quad n\in\{1,\dots,n_\text{obs}\}
\end{aligned}
$$ {#eq-posterior-full}
and we use the convention that $y=1$ is "heads".

1. Load the following code to compute the posterior in @eq-posterior-full. 
Nothing to submit for this item.
```{r}
posterior_distribution = function(rho, n_heads, n_observations) {
  K = length(rho) - 1
  gamma = rho * dbinom(n_heads, n_observations, (0:K)/K)
  normalizing_constant = sum(gamma)
  gamma/normalizing_constant
}
```
2. Write a function `posterior_mean` that computes the posterior mean given the
output of `posterior_distribution`.
3. Write another function with signature
```r
simulate_posterior_mean_error = function(rho_true, rho_prior, n_observations){...}
```
that performs the following

a. sample $p_\text{true} \sim \distDiscrete(\{0,1/K,2/K,\dots,1\},\rho_\text{true})$
b. sample the data $y_{1:n_\text{obs}}$ conditional on $p_\text{true}$
c. call `posterior_distribution` using $\rho_\text{prior}$ and the simulated data
d. compute the posterior mean $\ex[p|y_{1:n_\text{obs}}]$
e. return the absolute error between $p_\text{true}$ and the posterior mean

4. Using
```r
rho_true = rho_prior = rep(1, K+1)
```
and $K=20$, run `simulate_posterior_mean_error` for all `n_observations` values in
```r
n_obs_vector <- 2^(0:6)
```
Repeat each case 1000 times. Store the error values in a dataframe that has 
columns `(n_observations, replication, error)`, with `replication` ranging from 
1 to 1000. Show the output of `head` and the output of `tail` for your dataframe.

5. Visualize the above data using a log-log plot using the following code
```r
ggplot(experiment_results, aes(x=n_observations, y=errors+1e-9)) + # avoid log(0)
  stat_summary(fun = mean, geom="line") + # Line averages over 1000 replicates
  scale_x_log10() +  # Show result in log-log scale
  scale_y_log10() +
  coord_cartesian(ylim = c(1e-3, 1)) +
  theme_minimal() +
  geom_point() +
  labs(x = "Number of observations",
       y = "Absolute error of the posterior mean")
```

6. Repeat parts 4 and 5 but now using
```r
rho_true = rep(1, K+1)
rho_prior = 1:(K+1)
```
Compare the plots and comment on their similarities and differences.
