---
title: "Exercise 3: inference on continuous spaces"
editor: 
  mode: source
---
  
{{< include ../_macros.qmd >}}

{{< include ../_construction.qmd >}}


## Goals

- Introduce Monte Carlo integration in continuous spaces.
- Review the concept of importance sampling.


## TODO (delete when finished!)

- 1 - continuous MC on bounded space and uniform proposal✅
- 2 - continuous IS on unbounded space and e.g. exponential proposal✅
	-> bonus: something about 100x more samples needed for 1 more digits using log-log plot (maybe done in class instead?)
	-> bonus/challenge question? or maybe in class instead?: questions on ESS?
	-> bonus? an example where the expectation does not converge due to non-integrability
- 3 - guided steps to universal PPL (providing scaffold code)✅
	- test case: discrete model from last time (e.g. one for parameters posterior)
	- extending to cover predictive?
	- model with two variables? (can wait until regression)
	

## Q.1: functions on the unit interval

1. Write a function `mc_estimate` that takes a function $f:[0,1]\to\reals$ and 
outputs a Monte Carlo estimate of $\int_0^1 f(x)\dee x$ using $n=10000$ 
independent samples from $\distUnif(0,1)$.

2. Consider the function $f:[0,1]\to[0,\infty)$ given by
$$
f(x) = \frac{x^{-1/3}}{\sqrt{1-x}}.
$$
Show that
$$
\int_0^1 f(x)\dee x = \frac{\pi}{\sin\left(\frac{\pi}{3}\right)}.
$${#eq-integral}
Hint: look at the expression for the 
[density](https://en.wikipedia.org/wiki/Beta_distribution#Probability_density_function) 
of a $\distBeta(\alpha,\beta)$ distribution, and remember that it must integrate
to one. Then use the fact that for any $\gamma\in(0,1)$, the 
[gamma function satisfies](https://en.wikipedia.org/wiki/Gamma_function#General) 
$$
\Gamma(\gamma)\Gamma(1-\gamma) = \frac{\pi}{\sin\left(\gamma\pi\right)}.
$$
3. Test your implementation of `mc_estimate` by checking that it produces an
answer close to the value in @eq-integral.
4. In order to quantify the uncertainty inherent in the output of the previous 
step, we will construct a confidence interval[^1] for our Monte Carlo estimate
    a. Approximate the standard error of the estimator by computing the standard
       deviation of $K=30$ i.i.d. replications of `mc_estimate(f)`.
    b. Use the Central Limit Theorem to provide an asymptotically exact
       95% confidence interval.
    
[^1]: Yes, the frequentist concept! Monte Carlo simulation is one instance 
      where the assumptions of frequentist inference hold by design.

5. The following integral, known as the 
[sine integral])(https://en.wikipedia.org/wiki/Trigonometric_integral#Sine_integral),
$$
\int_0^1 \frac{\sin(t)}{t} \dee t.
$$
does not admit a closed-form expression. Provide a 95% confidence interval for
its value using `mc_estimate(f)`.


## Q.2: function on unbounded intervals

1. Write a function `is_exponential` that takes a function 
$f:[0,\infty)\to\reals$ and a number $\lambda>0$, and outputs a Monte Carlo estimate of
$\int_0^\infty f(x)\dee x$ using importance sampling with reference distribution
$\distExp(\lambda)$ and $n=10000$ i.i.d. samples.
2. Show that the function $f:[0,\infty)\to[0,\infty)$ given by
$$
f(x) = \frac{\exp(-\pi^2/x)}{x^3\sqrt{x}}.
$$
satisfies
$$
\int_0^\infty f(x)\dee x = \frac{3}{4\pi^4\sqrt{\pi}}.
$${#eq-integral-unbounded}
Hint: look at the expression for the 
[density of an Inverse Gamma distribution](https://en.wikipedia.org/wiki/Inverse-gamma_distribution#Probability_density_function).
Then use [the fact that](https://en.wikipedia.org/wiki/Gamma_function#General) for $n\in\nats$
$$
\Gamma\left(\frac{1}{2}+n\right) = \frac{(2n)!}{4^nn!}\sqrt{\pi}.
$$
3. Test your implementation of `is_exponential` with $\lambda=1$ by constructing
a 95% confidence interval for your estimate and checking that it contains the 
true value.
4. Re-do part 3 with $\lambda=20$ and compare both confidence intervals. Which
estimator is better?


## Q.3: A minimal universal probabilistic programming language (PPL)

In this question we will write a very short program that will nonetheless be
able to perform Bayesian inference on a wide range of Bayesian models
$$
\pi(x|y) = \frac{\pi(x)L(y|x)}{Z}
$$
The inference engine of our program will be self-normalizing importance sampling
(SNIS).
We will assume that the prior of our model is proper and that we can draw i.i.d.
samples from it.

In the following we will use the package `distr`, which allows us to work with
distributions as objects---a necessary ingredient of every PPL. Load or install
it using
```{r}
#| include: false
if (!require(distr)){
  install.packages("distr")
  require(distr)
}
```

**Example**: evaluating densities
```{r}
distUnif <- Unif(2,3)
p(distUnif)(0.2) # evaluate the pdf of distUnif at 0.2 
```
Note that $0.2\notin[2,3]$ and therefore the answer is $0$. The fact that this
returns $0$ instead of throwing an "out-of-bounds" error is crucial for a 
well-functioning implementation of a PPL.

**Example**: simulation
```{r}
distPoisson <- Pois(lambda = 3)
v = r(distPoisson)(1000) # get 1000 iid samples from distPois
head(v)
```

1. Show that the importance weight of targeting the posterior distribution with
the prior as reference is proportional to the likelihood.
2. One of the most important functions in a PPL is the `observe` statement. It is
the only special statement needs to include in their functions when using the PPL.
Its function is to accumulate likelihood in order to compute the IS weight for
any given sample. To this end, the first step is to define a global variable that
will be used throughout the program to contain the value of the weight:
```{r}
current_log_likelihood = 0.0
```
Now, **write** the `observe(value, dist)` function, which should accept two arguments:
    
    i. a distribution object `dist` from the package `distr`, and
    ii. a value `x` that is in the ambient space of the distribution `dist`.

Your function should add the value of $\log(d(x))$ to the `current_log_likelihood` 
accumulator. **Note**: it is much more numerically robust to use 
`d(dist)(x, log = TRUE)` than to manually compute `log(d(dist)(x))`.
3. Code your first PPL program for the [coin bag example from exercise 1](ex01.qmd).
To achieve this, let us start by storing the data as a global variable
```{r}
coin_flips = rep(0, 4) # observe four heads
```
Now, write a function `my_first_probabilistic_program()` by completing the steps
```{r}
#| eval: false
my_first_probabilistic_program = function(){
  coin_index = FILL_ME        # Step 1: sample a coin index from the prior
  for (i in seq_along(coin_flips)){
    observe(FILL_ME, FILL_ME) # Step 2: accumulate the log-likelihood of each observation given the coin_index
  }
  return(FILL_ME)             # Step 3: return the query: is coin_index a fair coin?
}
```
4. Write a function `posterior(f,n)` that takes a PPL program like
`my_first_probabilistic_program` and a number `n`, and performs posterior
inference via SNIS using `n` samples. Recall that the SNIS estimator is
$$
\widehat{\pi(f)} := \frac{\sum_{i=1}^n w_i f(x_i)}{\sum_{i=1}^n w_i}
$$
Now, fill in the missing steps in the following function skeleton
```{r}
#| eval: false
posterior = function(f, n){
  weights = FILL_ME # a vector of length n to store the weights
  weighted_fs = FILL_ME # a vector of length n to store the summands w_i*f(x_i)
  for (i in 1:n) {
    current_log_likelihood = FILL_ME # reset the global accumulator
    f_i = FILL_ME # run the program `f` and capture its output
    weights[i] = FILL_ME # store the weight
    weighted_fs[i] = FILL_ME # store the summand
  }
  # compute the SNIS estimator and return it
}
```
5. Test your program by checking that you can approximate the posterior probability
of the fair coin obtained in [exercise 1, Q.2](ex01.qmd#sec-q2).
