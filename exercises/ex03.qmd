---
title: "Exercise 3: inference on continuous spaces"
editor: 
  mode: source
---
  
{{< include ../_macros.qmd >}}

{{< include ../_construction.qmd >}}


## Goals

- Introduce Monte Carlo integration in continuous spaces.
- Review the concept of importance sampling.
- Build a universal probabilistic programming language in <30 lines of code.


## TODO (delete when finished!)

- 1 - continuous MC on bounded space and uniform proposal✅
- 2 - continuous IS on unbounded space and e.g. exponential proposal✅
	-> bonus: something about 100x more samples needed for 1 more digits using log-log plot (maybe done in class instead?)
	-> bonus/challenge question? or maybe in class instead?: questions on ESS?
	-> bonus? an example where the expectation does not converge due to non-integrability
- 3 - guided steps to universal PPL (providing scaffold code)✅
	- test case: discrete model from last time (e.g. one for parameters posterior)
	- extending to cover predictive?
	- model with two variables? (can wait until regression)
	

## Q.1: functions on the unit interval

1. Write a function `mc_estimate` that takes a function $f:[0,1]\to\reals$ and 
outputs a Monte Carlo estimate of $\int_0^1 f(x)\dee x$ using $n=10000$ 
independent samples from $\distUnif(0,1)$.

2. Consider the function $f:[0,1]\to[0,\infty)$ given by
$$
f(x) = \frac{1}{\sqrt[3]{x^2(1-x)}}.
$$
It is possible to show that
$$
\int_0^1 f(x)\dee x = \frac{\pi}{\sin\left(\frac{\pi}{3}\right)}.
$${#eq-integral}
Test your implementation of `mc_estimate` by checking that it produces an
answer close to the value in @eq-integral.
4. The following integral, known as the 
[sine integral])(https://en.wikipedia.org/wiki/Trigonometric_integral#Sine_integral),
$$
\int_0^1 \frac{\sin(t)}{t} \dee t.
$$
does not admit a closed-form expression. Estimate its value using `mc_estimate(f)`.


## Q.2: function on unbounded intervals

In unbounded intervals, the uniform measure is not a probability measure, and
therefore cannot be used to estimate integrals as in the previous section.
Thus, we must rely on alternative computational tricks to achieve this task.

Self-normalizing importance sampling is a Monte Carlo scheme that allows us to
estimate expectations of real-valued functions $f$ with respect to intractable
distributions $\pi$ by using samples from a tractable reference distribution
$q$. It is "self-normalizing" because it works even when normalzing constants are
missing in either density function; i.e., if only we have access to $\gamma$ and
$\nu$ such that
$$
\pi(x) = \frac{\gamma(x)}{Z_\pi} \qquad q(x) = \frac{\nu(x)}{Z_q}
$$
Let $\ex_\pi$ represent expectations with respect to $\pi$, and $\ex_\nu$ expectations
w.r.t. $\nu$. The fundamental SNIS identity is
$$
\ex_\pi[f] = \frac{\ex_\nu\left[f(x) w(x) \right]}{\ex_\nu\left[ w(x) \right]},
$$
where
$$
w(x) := \frac{\gamma(x)}{\nu(x)}.
$$
If $\{x_i\}_{i=1}^n\distiid q$, then
$$
\widehat{\pi(f)} := \frac{\sum_{i=1}^n w_i f(x_i)}{\sum_{i=1}^n w_i}
$$
is a consistent estimator of $\ex_\pi[f]$, with $w_i=w(x_i)$.


1. Write a function `snis_exponential` that takes a function 
$f:[0,\infty)\to\reals$ and a number $\lambda>0$, and outputs a Monte Carlo estimate of
$\int_0^\infty f(x)\dee x$ using importance sampling with reference distribution
$\distExp(\lambda)$ and $n=10000$ i.i.d. samples.
2. The function $f:[0,\infty)\to[0,\infty)$ given by
$$
f(x) = \frac{\exp(-\pi^2/x)}{x^3\sqrt{x}}.
$$
satisfies
$$
\int_0^\infty f(x)\dee x = \frac{3}{4\pi^4\sqrt{\pi}}.
$${#eq-integral-unbounded}
Test your implementation of `is_exponential` with $\lambda=1$ by constructing
a 95% confidence interval for your estimate and checking that it contains the 
true value.
3. Re-do part 3 with $\lambda=20$ and compare both confidence intervals. Which
estimator is better?


## Q.3: A minimal universal probabilistic programming language (PPL)

In this question we will write a very short program that will nonetheless be
able to perform Bayesian inference on a wide range of Bayesian models
$$
\pi(x|y) = \frac{\pi(x)L(y|x)}{Z}
$$
The inference engine of our PPL will be SNIS, where the target is the posterior
and the reference is the prior. Thus, we will restrict our PPL to models in 
which the prior is proper and it is possible to draw i.i.d. samples from it.

In the following we will use the package `distr`, which allows us to work with
distributions as objects---a necessary ingredient of every PPL. Load or install
it using
```{r}
#| include: false
if (!require(distr)){
  install.packages("distr")
  require(distr)
}
```

**Example**: evaluating a distribution function
```{r}
distUnif <- Unif(2,3)
p(distUnif)(0.2) # evaluate the cdf of distUnif at 0.2 
```
Note that $0.2\notin[2,3]$ and therefore the answer is $0$. The fact that this
returns $0$ instead of throwing an "out-of-bounds" error is crucial for a 
well-functioning implementation of a PPL.

**Example**: simulation
```{r}
distPoisson <- Pois(lambda = 3)
v = r(distPoisson)(1000) # get 1000 iid samples from distPoisson
head(v)
```

The $\distBern(p)$ distribution is not available inside `distr`. Nevertheless,
we can leverage the generic `DiscreteDistribution` object to create it
```{r}
Bern = function(prob){
  DiscreteDistribution(supp = 0:1, prob = c(1-prob, prob))
}
```
**Example**: evaluating mass/density functions
```{r}
d(Bern(0.5))(1) == 0.5
```

1. Show that the SNIS weight when the target is the posterior and the reference
is the prior is (proportional to) the likelihood.
2. One of the most important functions in a PPL is the `observe` statement. It is
the only special statement needs to include in their functions when using the PPL.
Its function is to accumulate likelihood in order to compute the IS weight for
any given sample. To this end, the first step is to define a global variable that
will be used throughout the program to contain the value of the weight:
```{r}
current_log_likelihood = 0.0
```
Now, **write** the `observe(x, dist)` function, which should accept two arguments:
    
    i. a distribution object `dist` from the package `distr`, and
    ii. a value `x` that is in the ambient space of the distribution `dist`.

An example call would be `observe(1.3, Norm(mean=-4, sd=3.5))`.
Your function should add the logpdf of `x` under `dist` to the 
`current_log_likelihood` accumulator.

::: {.callout-note}
The operator `<<-` in `R` lets us modify variables that live in the global scope
from inside a function.
:::

::: {.callout-tip}
It is *much* more numerically robust to use `d(dist)(x, log = TRUE)` than to 
manually compute `log(d(dist)(x))`.
:::

3. Code your first PPL program for the [coin bag example from exercise 1](ex01.qmd).
To achieve this, let us start by storing the data as a global variable
```{r}
coin_flips = rep(0, 4) # observe four heads
```
Now, write a function `my_first_probabilistic_program()` by completing the steps
```{r}
#| eval: false
my_first_probabilistic_program = function(){
  coin_index = FIX_ME        # sample a coin index from the prior
  for (i in seq_along(coin_flips)){
    observe(FIX_ME, FIX_ME)  # accumulate the log-likelihood of each observation given the coin_index
  }
  return(FIX_ME)             # return the query: is coin_index a fair coin?
}
```
4. Write a function `posterior(f,n)` that takes a PPL program like
`my_first_probabilistic_program` and a number `n`, and performs posterior
inference via SNIS using `n` samples. Use the following template
```{r}
#| eval: false
posterior = function(f, n){
  numerator = 0.0
  denominator = 0.0
  for (i in 1:n) {
    current_log_likelihood <<- FIX_ME # reset the global accumulator
    f_i = FIX_ME # run the program `f` and capture its output
    numerator = numerator + FIX_ME # update the accumulator for numerator
    denominator = denominator + FIX_ME # update the accumulator for denominator
  }
  return(numerator/denominator)
}
```



5. Test your program by checking that you can approximate the posterior probability
of the fair coin obtained in [exercise 1, Q.2](ex01.qmd#sec-q2).
