---
title: "Exercise 3: inference on continuous spaces"
editor: 
  mode: source
---
  
{{< include ../_macros.qmd >}}


## Goals

- Introduce Monte Carlo integration in continuous spaces.
- Review the concept of importance sampling.
- Build a universal probabilistic programming language in <30 lines of code.


## Q.1: functions on the unit interval

1. Write a function `mc_estimate` that takes a function $f:[0,1]\to\reals$ and 
outputs a Monte Carlo estimate of $\int_0^1 f(x)\dee x$ using $n=10000$ 
independent samples from $\distUnif(0,1)$.

2. Consider the function $f:[0,1]\to[0,\infty)$ given by
$$
f(x) = \frac{1}{\sqrt[3]{x^2(1-x)}}.
$$
It is possible to show that
$$
\int_0^1 f(x)\dee x = \frac{\pi}{\sin\left(\frac{\pi}{3}\right)}.
$${#eq-integral}
Test your implementation of `mc_estimate` by checking that it produces an
answer close to the value in @eq-integral.
4. The following integral, known as the 
[sine integral])(https://en.wikipedia.org/wiki/Trigonometric_integral#Sine_integral),
$$
\int_0^1 \frac{\sin(t)}{t} \dee t.
$$
does not admit a closed-form expression. Estimate its value using `mc_estimate(f)`.


## Q.2: function on unbounded intervals

In unbounded intervals, the uniform measure is not a probability measure, and
therefore cannot be used to estimate integrals as in the previous section.
Thus, we must rely on alternative computational tricks to achieve this task.

Self-normalizing importance sampling is a Monte Carlo scheme that allows us to
estimate expectations of real-valued functions $f$ with respect to intractable
distributions $\pi$ by using samples from a tractable reference distribution
$q$. It is "self-normalizing" because it works even when normalzing constants are
missing in either density function; i.e., if only we have access to $\gamma$ and
$\nu$ such that
$$
\pi(x) = \frac{\gamma(x)}{Z_\pi} \qquad q(x) = \frac{\nu(x)}{Z_q}
$$
Let $\ex_\pi$ represent expectations with respect to $\pi$, and $\ex_\nu$ expectations
w.r.t. $\nu$. The fundamental SNIS identity is
$$
\ex_\pi[f] = \frac{\ex_\nu\left[f(x) w(x) \right]}{\ex_\nu\left[ w(x) \right]},
$$
where
$$
w(x) := \frac{\gamma(x)}{\nu(x)}.
$$
If $\{x_i\}_{i=1}^n\distiid q$ and we set $w_i=w(x_i)$, then
$$
\widehat{\pi(f)} := \frac{\sum_{i=1}^n w_i f(x_i)}{\sum_{i=1}^n w_i}
$${#eq-SNIS-estimator}
is a consistent estimator of $\ex_\pi[f]$. Moreover,
$$
\frac{1}{n}\sum_{i=1}^n w_i
$${#eq-SNIS-norm-const}
is a consistent estimator of $\frac{Z_\pi}{Z_q}$.


### Exercises

1. The pdf of a $\pi = \distNorm(0,\sigma^2)$ distribution is proportional to
$$
\gamma(x) = \exp\left(-\frac{x^2}{2\sigma^2}\right)
$$
We know that its normalizing constant is
$$
\int_{-\infty}^\infty \gamma(x)\dee x = \sigma\sqrt{2\pi}.
$$
Also, $\ex_\pi[x^2]=\sigma^2$. We will use SNIS to estimate both of these quantities,
for $\sigma = 3$. As reference, use the 
[Laplace distribution](https://en.wikipedia.org/wiki/Laplace_distribution) 
$\distLap(0,3)$. It is implemented in the `extraDistr` package
```{r}
library(extraDistr)

dlaplace(8, sigma = 3) # pdf at x=8 of Lap(0,3)
rlaplace(1, sigma = 3) # random sample from Lap(0,3)
```
Fill in the blanks below to implement a function `snis(n)` that uses SNIS to
estimate the above quantities (normalizing constant and $\ex_\pi[x^2]$)
```{r}
#| eval: false
snis = function(n){
  numerator = 0.0
  denominator = 0.0
  for (i in 1:n) {
    xi = FIX_ME # sample from Laplace(0, 3)
    f_xi = FIX_ME # compute target function at the sampled point xi
    w_i = FIX_ME / dlaplace(xi, sigma = 3) # compute the weight
    numerator = numerator + FIX_ME # update the accumulator for numerator
    denominator = denominator + FIX_ME # update the accumulator for denominator
  }
  return(list(expectation = numerator/denominator, norm_const = denominator/n))
}
```
2. Run `snis(10000)` and report the results comparing to the true values.


## Q.3: A minimal universal probabilistic programming language (PPL)

In this question we will write a very short program that will nonetheless be
able to perform Bayesian inference on a wide range of Bayesian models
$$
\pi(x|y) = \frac{\pi(x)L(y|x)}{Z}
$$
The inference engine of our PPL will be SNIS, where the target is the posterior
and the reference is the prior. Thus, we will restrict our PPL to models in 
which the prior is proper and it is possible to draw i.i.d. samples from it.

### Detour: the `distr` package

In the following we will use the package `distr`, which allows us to work with
distributions as objects---a necessary ingredient of every PPL. Load or install
it using
```{r}
#| eval: false
if (!require(distr)){
  install.packages("distr")
  require(distr)
}
```
<!-- avoid the startup message using include: false -->
```{r}
#| include: false
library(distr)
```

**Example**: evaluating a distribution function
```{r}
distUnif <- Unif(2,3)
p(distUnif)(0.2)
```

**Example**: simulation
```{r}
distPoisson <- Pois(lambda = 3)
v = r(distPoisson)(1000) # get 1000 iid samples from distPoisson
head(v)
```

The $\distBern(p)$ distribution is not available inside `distr`. Nevertheless,
we can leverage the generic `DiscreteDistribution` object to create it
```{r}
Bern = function(prob){
  DiscreteDistribution(supp = 0:1, prob = c(1-prob, prob))
}
```
**Example**: evaluating mass/density functions
```{r}
d(Bern(0.5))(1) == 0.5
```



### Exercises

1. Show that the SNIS weight when the target is the posterior and the reference is
$$
w(x) = L(y|x)
$$
2. One of the most important functions in a PPL is the `observe` statement. It is
the only special statement needs to include in their functions when using the PPL.
Its function is to accumulate likelihood in order to compute the SNIS weight for
any given sample. To this end, the first step is to define a global variable that
will be used throughout the program to contain the value of the weight:
```{r}
current_log_likelihood = 0.0
```
Now, **write** a function `observe(x, dist)` where `dist` is a distribution from
the package `distr`, and `x` is a value in the ambient space of `dist`. Fill in
the blank below
```{r}
#| eval: false
observe = function(x, dist){
  current_log_likelihood <<- current_log_likelihood + FIX_ME
}
```

::: {.callout-note}
The operator `<<-` in `R` lets us modify variables that live in the global scope
from inside a function.
:::

::: {.callout-tip}
It is *much* more numerically robust to use `d(dist)(x, log = TRUE)` than to 
manually compute `log(d(dist)(x))`.
:::

3. Code your first PPL program for the [coin bag example from exercise 1](ex01.qmd).
To achieve this, let us start by storing the data as a global variable
```{r}
coin_flips = rep(0, 4) # observe four heads
```
Now, write a function `my_first_probabilistic_program()` by completing the steps
```{r}
#| eval: false
my_first_probabilistic_program = function(){
  coin_index = FIX_ME        # sample a coin index from the prior
  for (i in seq_along(coin_flips)){
    observe(FIX_ME, FIX_ME)  # accumulate the log-likelihood of each observation given the coin_index
  }
  return(FIX_ME)             # return the query: is coin_index a fair coin?
}
```
4. Write a function `posterior(f,n)` that takes a PPL program like
`my_first_probabilistic_program` and a number `n`, and performs posterior
inference via SNIS using `n` samples. Use the following template
```{r}
#| eval: false
posterior = function(f, n){
  numerator = 0.0
  denominator = 0.0
  for (i in 1:n) {
    current_log_likelihood <<- FIX_ME # reset the global accumulator
    f_i = FIX_ME # run the program `f` and capture its output
    numerator = numerator + FIX_ME # update the accumulator for numerator
    denominator = denominator + FIX_ME # update the accumulator for denominator
  }
  return(numerator/denominator)
}
```

5. Test your program by checking that you can approximate the posterior probability
of the fair coin obtained in [exercise 1, Q.2](ex01.qmd#sec-q2).
